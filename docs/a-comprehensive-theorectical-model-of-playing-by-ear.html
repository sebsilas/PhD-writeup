<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4 A Comprehensive Theorectical Model of Playing By Ear | Playing By Ear: A Computational Approach</title>
  <meta name="description" content="4 A Comprehensive Theorectical Model of Playing By Ear | Playing By Ear: A Computational Approach" />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="4 A Comprehensive Theorectical Model of Playing By Ear | Playing By Ear: A Computational Approach" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4 A Comprehensive Theorectical Model of Playing By Ear | Playing By Ear: A Computational Approach" />
  
  
  

<meta name="author" content="Seb Silas" />


<meta name="date" content="2025-06-08" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="backgrounds.html"/>
<link rel="next" href="theoretical-model.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Front Matter</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#foreword"><i class="fa fa-check"></i><b>1.1</b> Foreword</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i><b>1.2</b> Acknowledgements</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#toc"><i class="fa fa-check"></i><b>1.3</b> Table of Contents</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>2</b> Introduction</a>
<ul>
<li class="chapter" data-level="2.1" data-path="introduction.html"><a href="introduction.html#what-is-playing-by-ear"><i class="fa fa-check"></i><b>2.1</b> What Is Playing By Ear?</a></li>
<li class="chapter" data-level="2.2" data-path="introduction.html"><a href="introduction.html#learning-to-play-by-ear"><i class="fa fa-check"></i><b>2.2</b> Learning to Play By Ear</a></li>
<li class="chapter" data-level="2.3" data-path="introduction.html"><a href="introduction.html#the-problem-domain-and-approach"><i class="fa fa-check"></i><b>2.3</b> The Problem Domain and Approach</a></li>
<li class="chapter" data-level="2.4" data-path="introduction.html"><a href="introduction.html#motivations"><i class="fa fa-check"></i><b>2.4</b> Motivations</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="introduction.html"><a href="introduction.html#generalising-beyond-jazz-and-playing-by-ear"><i class="fa fa-check"></i><b>2.4.1</b> Generalising Beyond Jazz and Playing By Ear</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="introduction.html"><a href="introduction.html#thesis-statement"><i class="fa fa-check"></i><b>2.5</b> Thesis Statement</a></li>
<li class="chapter" data-level="2.6" data-path="introduction.html"><a href="introduction.html#original-contributions"><i class="fa fa-check"></i><b>2.6</b> Original Contributions</a></li>
<li class="chapter" data-level="2.7" data-path="introduction.html"><a href="introduction.html#research-questions"><i class="fa fa-check"></i><b>2.7</b> Research Questions</a></li>
<li class="chapter" data-level="2.8" data-path="introduction.html"><a href="introduction.html#objectives-and-scope"><i class="fa fa-check"></i><b>2.8</b> Objectives and Scope</a>
<ul>
<li class="chapter" data-level="2.8.1" data-path="introduction.html"><a href="introduction.html#scope"><i class="fa fa-check"></i><b>2.8.1</b> Scope</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="introduction.html"><a href="introduction.html#front-facing-tools"><i class="fa fa-check"></i><b>2.9</b> Front-Facing Tools</a>
<ul>
<li class="chapter" data-level="2.9.1" data-path="introduction.html"><a href="introduction.html#slonimsky.app"><i class="fa fa-check"></i><b>2.9.1</b> Slonimsky.app</a></li>
<li class="chapter" data-level="2.9.2" data-path="introduction.html"><a href="introduction.html#songbird"><i class="fa fa-check"></i><b>2.9.2</b> Songbird</a></li>
</ul></li>
<li class="chapter" data-level="2.10" data-path="introduction.html"><a href="introduction.html#dissertation-outline"><i class="fa fa-check"></i><b>2.10</b> Dissertation Outline</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="backgrounds.html"><a href="backgrounds.html"><i class="fa fa-check"></i><b>3</b> Backgrounds</a>
<ul>
<li class="chapter" data-level="3.1" data-path="backgrounds.html"><a href="backgrounds.html#historical_background"><i class="fa fa-check"></i><b>3.1</b> Historical Background: Approaches to Improving Playing by Ear in Jazz Music</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="backgrounds.html"><a href="backgrounds.html#the-aural-origins-of-jazz-education"><i class="fa fa-check"></i><b>3.1.1</b> The Aural Origins of Jazz Education</a></li>
<li class="chapter" data-level="3.1.2" data-path="backgrounds.html"><a href="backgrounds.html#record-players-as-pedagogical-tools"><i class="fa fa-check"></i><b>3.1.2</b> Record Players as Pedagogical Tools</a></li>
<li class="chapter" data-level="3.1.3" data-path="backgrounds.html"><a href="backgrounds.html#codification-of-jazz-education"><i class="fa fa-check"></i><b>3.1.3</b> Codification of Jazz Education</a></li>
<li class="chapter" data-level="3.1.4" data-path="backgrounds.html"><a href="backgrounds.html#the-chord-scale-approach-pattern-books-and-melodic-vocabulary-acquisition"><i class="fa fa-check"></i><b>3.1.4</b> The Chord-Scale Approach, Pattern Books, and Melodic Vocabulary Acquisition</a></li>
<li class="chapter" data-level="3.1.5" data-path="backgrounds.html"><a href="backgrounds.html#john-coltrane-and-slonimskys-thesaurus-aspirational-melodic-vocabulary-acquisition"><i class="fa fa-check"></i><b>3.1.5</b> John Coltrane and Slonimsky’s Thesaurus: Aspirational Melodic Vocabulary Acquisition</a></li>
<li class="chapter" data-level="3.1.6" data-path="backgrounds.html"><a href="backgrounds.html#historical-background-conclusion"><i class="fa fa-check"></i><b>3.1.6</b> Historical Background: Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="backgrounds.html"><a href="backgrounds.html#modern_technological_approaches"><i class="fa fa-check"></i><b>3.2</b> Modern Technological Approaches to Ear Training</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="backgrounds.html"><a href="backgrounds.html#historical-development-of-ear-training-technology"><i class="fa fa-check"></i><b>3.2.1</b> Historical Development of Ear-Training Technology</a></li>
<li class="chapter" data-level="3.2.2" data-path="backgrounds.html"><a href="backgrounds.html#non-commercial-and-research-initiatives"><i class="fa fa-check"></i><b>3.2.2</b> Non-Commercial and Research Initiatives</a></li>
<li class="chapter" data-level="3.2.3" data-path="backgrounds.html"><a href="backgrounds.html#pedagogical-strategies-and-practical-use"><i class="fa fa-check"></i><b>3.2.3</b> Pedagogical Strategies and Practical Use</a></li>
<li class="chapter" data-level="3.2.4" data-path="backgrounds.html"><a href="backgrounds.html#benefits-and-limitations"><i class="fa fa-check"></i><b>3.2.4</b> Benefits and Limitations</a></li>
<li class="chapter" data-level="3.2.5" data-path="backgrounds.html"><a href="backgrounds.html#modern-technological-approaches-to-ear-training-conclusion"><i class="fa fa-check"></i><b>3.2.5</b> Modern Technological Approaches to Ear Training: Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="backgrounds.html"><a href="backgrounds.html#playing_by_ear_literature_review"><i class="fa fa-check"></i><b>3.3</b> Playing by Ear Research: A Literature Review</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="backgrounds.html"><a href="backgrounds.html#historical-overview"><i class="fa fa-check"></i><b>3.3.1</b> Historical Overview</a></li>
<li class="chapter" data-level="3.3.2" data-path="backgrounds.html"><a href="backgrounds.html#developmental-and-pedagogical-aspects-of-playing-by-ear-skill-acquisition"><i class="fa fa-check"></i><b>3.3.2</b> Developmental and Pedagogical Aspects of Playing By Ear Skill Acquisition</a></li>
<li class="chapter" data-level="3.3.3" data-path="backgrounds.html"><a href="backgrounds.html#mcphersons-integrated-model-of-musical-skill-development"><i class="fa fa-check"></i><b>3.3.3</b> McPherson’s Integrated Model of Musical Skill Development</a></li>
<li class="chapter" data-level="3.3.4" data-path="backgrounds.html"><a href="backgrounds.html#cognitive-mechanisms-underlying-playing-by-ear"><i class="fa fa-check"></i><b>3.3.4</b> Cognitive Mechanisms Underlying Playing by Ear</a></li>
<li class="chapter" data-level="3.3.5" data-path="backgrounds.html"><a href="backgrounds.html#auditory-perception"><i class="fa fa-check"></i><b>3.3.5</b> Auditory Perception</a></li>
<li class="chapter" data-level="3.3.6" data-path="backgrounds.html"><a href="backgrounds.html#working-memory"><i class="fa fa-check"></i><b>3.3.6</b> Working Memory</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="backgrounds.html"><a href="backgrounds.html#interim-summary-1"><i class="fa fa-check"></i><b>3.4</b> Interim Summary</a></li>
<li class="chapter" data-level="3.5" data-path="backgrounds.html"><a href="backgrounds.html#pfordreshers-cognitive-model-of-singing-accuracy-and-h-pac-models"><i class="fa fa-check"></i><b>3.5</b> Pfordresher’s Cognitive Model of Singing Accuracy and H-PAC Models</a></li>
<li class="chapter" data-level="3.6" data-path="backgrounds.html"><a href="backgrounds.html#bakers-computational-model-of-melodic-dictation"><i class="fa fa-check"></i><b>3.6</b> Baker’s Computational Model of Melodic Dictation</a></li>
<li class="chapter" data-level="3.7" data-path="backgrounds.html"><a href="backgrounds.html#models-of-sung-recall"><i class="fa fa-check"></i><b>3.7</b> Models of Sung Recall</a>
<ul>
<li class="chapter" data-level="3.7.1" data-path="backgrounds.html"><a href="backgrounds.html#playing-by-ear-research-conclusion"><i class="fa fa-check"></i><b>3.7.1</b> Playing by Ear Research: Conclusion</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="a-comprehensive-theorectical-model-of-playing-by-ear.html"><a href="a-comprehensive-theorectical-model-of-playing-by-ear.html"><i class="fa fa-check"></i><b>4</b> A Comprehensive Theorectical Model of Playing By Ear</a>
<ul>
<li class="chapter" data-level="4.1" data-path="a-comprehensive-theorectical-model-of-playing-by-ear.html"><a href="a-comprehensive-theorectical-model-of-playing-by-ear.html#musicological_background"><i class="fa fa-check"></i><b>4.1</b> Computational Musicological Perspectives</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="a-comprehensive-theorectical-model-of-playing-by-ear.html"><a href="a-comprehensive-theorectical-model-of-playing-by-ear.html#music-and-combinatorics"><i class="fa fa-check"></i><b>4.1.1</b> Music and Combinatorics</a></li>
<li class="chapter" data-level="4.1.2" data-path="a-comprehensive-theorectical-model-of-playing-by-ear.html"><a href="a-comprehensive-theorectical-model-of-playing-by-ear.html#music-corpora-analysis"><i class="fa fa-check"></i><b>4.1.2</b> Music Corpora Analysis</a></li>
<li class="chapter" data-level="4.1.3" data-path="a-comprehensive-theorectical-model-of-playing-by-ear.html"><a href="a-comprehensive-theorectical-model-of-playing-by-ear.html#melodic-represention-and-computational-melodic-features"><i class="fa fa-check"></i><b>4.1.3</b> Melodic Represention and Computational Melodic Features</a></li>
<li class="chapter" data-level="4.1.4" data-path="a-comprehensive-theorectical-model-of-playing-by-ear.html"><a href="a-comprehensive-theorectical-model-of-playing-by-ear.html#computed-melodic-features-as-a-musicological-basis-for-selecting-items"><i class="fa fa-check"></i><b>4.1.4</b> Computed Melodic Features as a Musicological Basis for Selecting Items</a></li>
<li class="chapter" data-level="4.1.5" data-path="a-comprehensive-theorectical-model-of-playing-by-ear.html"><a href="a-comprehensive-theorectical-model-of-playing-by-ear.html#pattern-books-as-item-banks"><i class="fa fa-check"></i><b>4.1.5</b> Pattern Books as Item Banks</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="a-comprehensive-theorectical-model-of-playing-by-ear.html"><a href="a-comprehensive-theorectical-model-of-playing-by-ear.html#melodic-similarity"><i class="fa fa-check"></i><b>4.2</b> Melodic Similarity</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="a-comprehensive-theorectical-model-of-playing-by-ear.html"><a href="a-comprehensive-theorectical-model-of-playing-by-ear.html#methodological-background"><i class="fa fa-check"></i><b>4.2.1</b> Methodological Background</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="a-comprehensive-theorectical-model-of-playing-by-ear.html"><a href="a-comprehensive-theorectical-model-of-playing-by-ear.html#cognitive_psychological_background"><i class="fa fa-check"></i><b>4.3</b> Cognitive Psychology</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="a-comprehensive-theorectical-model-of-playing-by-ear.html"><a href="a-comprehensive-theorectical-model-of-playing-by-ear.html#conclusion-cognitive-psychological-background"><i class="fa fa-check"></i><b>4.3.1</b> Conclusion: Cognitive Psychological Background</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="a-comprehensive-theorectical-model-of-playing-by-ear.html"><a href="a-comprehensive-theorectical-model-of-playing-by-ear.html#statistical_modelling_frameworks"><i class="fa fa-check"></i><b>4.4</b> Statistical Modelling Frameworks</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="a-comprehensive-theorectical-model-of-playing-by-ear.html"><a href="a-comprehensive-theorectical-model-of-playing-by-ear.html#cognitive-modelling-item-feature-modelling-via-item-response-theory"><i class="fa fa-check"></i><b>4.4.1</b> Cognitive Modelling Item Feature Modelling via Item Response Theory</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="theoretical-model.html"><a href="theoretical-model.html"><i class="fa fa-check"></i><b>5</b> Playing By Ear: A Theoretical Model</a>
<ul>
<li class="chapter" data-level="5.1" data-path="theoretical-model.html"><a href="theoretical-model.html#bringing-it-together"><i class="fa fa-check"></i><b>5.1</b> Bringing It Together</a></li>
<li class="chapter" data-level="5.2" data-path="theoretical-model.html"><a href="theoretical-model.html#moving-from-theoretical-to-computational"><i class="fa fa-check"></i><b>5.2</b> Moving from Theoretical to Computational</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="theoretical-model.html"><a href="theoretical-model.html#inputs"><i class="fa fa-check"></i><b>5.2.1</b> Inputs</a></li>
<li class="chapter" data-level="5.2.2" data-path="theoretical-model.html"><a href="theoretical-model.html#sequential-construction"><i class="fa fa-check"></i><b>5.2.2</b> Sequential Construction</a></li>
<li class="chapter" data-level="5.2.3" data-path="theoretical-model.html"><a href="theoretical-model.html#function-application"><i class="fa fa-check"></i><b>5.2.3</b> Function Application</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="theoretical-model.html"><a href="theoretical-model.html#mathematical-representation"><i class="fa fa-check"></i><b>5.3</b> Mathematical Representation</a></li>
<li class="chapter" data-level="5.4" data-path="theoretical-model.html"><a href="theoretical-model.html#algorithm-flow"><i class="fa fa-check"></i><b>5.4</b> Algorithm Flow</a></li>
<li class="chapter" data-level="5.5" data-path="theoretical-model.html"><a href="theoretical-model.html#final-equation"><i class="fa fa-check"></i><b>5.5</b> Final Equation</a></li>
<li class="chapter" data-level="5.6" data-path="theoretical-model.html"><a href="theoretical-model.html#strategies"><i class="fa fa-check"></i><b>5.6</b> Strategies</a></li>
<li class="chapter" data-level="5.7" data-path="theoretical-model.html"><a href="theoretical-model.html#the-melodic-mind-as-item-bank"><i class="fa fa-check"></i><b>5.7</b> The Melodic Mind as Item Bank</a></li>
<li class="chapter" data-level="5.8" data-path="theoretical-model.html"><a href="theoretical-model.html#predictions"><i class="fa fa-check"></i><b>5.8</b> Predictions</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="computational-ecosystem.html"><a href="computational-ecosystem.html"><i class="fa fa-check"></i><b>6</b> A Computational Ecosystem</a>
<ul>
<li class="chapter" data-level="6.1" data-path="computational-ecosystem.html"><a href="computational-ecosystem.html#transcription-of-melodic-production-data-pyin"><i class="fa fa-check"></i><b>6.1</b> Transcription of Melodic Production Data: pyin</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="pyin-r-package.html"><a href="pyin-r-package.html"><i class="fa fa-check"></i><b>7</b> pYIN R package</a>
<ul>
<li class="chapter" data-level="7.1" data-path="pyin-r-package.html"><a href="pyin-r-package.html#installation"><i class="fa fa-check"></i><b>7.1</b> Installation</a></li>
<li class="chapter" data-level="7.2" data-path="pyin-r-package.html"><a href="pyin-r-package.html#usage"><i class="fa fa-check"></i><b>7.2</b> Usage</a></li>
<li class="chapter" data-level="7.3" data-path="pyin-r-package.html"><a href="pyin-r-package.html#notes"><i class="fa fa-check"></i><b>7.3</b> Notes</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="pyin-r-package.html"><a href="pyin-r-package.html#compatability"><i class="fa fa-check"></i><b>7.3.1</b> Compatability</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="pyin-r-package.html"><a href="pyin-r-package.html#references"><i class="fa fa-check"></i><b>7.4</b> References</a></li>
<li class="chapter" data-level="7.5" data-path="pyin-r-package.html"><a href="pyin-r-package.html#assessment-of-melodic-similarity-melsim"><i class="fa fa-check"></i><b>7.5</b> Assessment of Melodic Similarity: melsim</a></li>
<li class="chapter" data-level="7.6" data-path="pyin-r-package.html"><a href="pyin-r-package.html#psychologically-meaningful-musical-item-banks-itembankr"><i class="fa fa-check"></i><b>7.6</b> Psychologically Meaningful Musical Item Banks: itembankr</a></li>
<li class="chapter" data-level="7.7" data-path="pyin-r-package.html"><a href="pyin-r-package.html#references-1"><i class="fa fa-check"></i><b>7.7</b> References</a></li>
<li class="chapter" data-level="7.8" data-path="pyin-r-package.html"><a href="pyin-r-package.html#musicassessrdb"><i class="fa fa-check"></i><b>7.8</b> musicassessrdb</a></li>
<li class="chapter" data-level="7.9" data-path="pyin-r-package.html"><a href="pyin-r-package.html#datasets-created-with-itembankr"><i class="fa fa-check"></i><b>7.9</b> Datasets (created with itembankr)</a>
<ul>
<li class="chapter" data-level="7.9.1" data-path="pyin-r-package.html"><a href="pyin-r-package.html#wjd"><i class="fa fa-check"></i><b>7.9.1</b> WJD</a></li>
</ul></li>
<li class="chapter" data-level="7.10" data-path="pyin-r-package.html"><a href="pyin-r-package.html#references-2"><i class="fa fa-check"></i><b>7.10</b> References</a>
<ul>
<li class="chapter" data-level="7.10.1" data-path="pyin-r-package.html"><a href="pyin-r-package.html#berkowitz"><i class="fa fa-check"></i><b>7.10.1</b> Berkowitz</a></li>
</ul></li>
<li class="chapter" data-level="7.11" data-path="pyin-r-package.html"><a href="pyin-r-package.html#references-3"><i class="fa fa-check"></i><b>7.11</b> References</a>
<ul>
<li class="chapter" data-level="7.11.1" data-path="pyin-r-package.html"><a href="pyin-r-package.html#slonimsky"><i class="fa fa-check"></i><b>7.11.1</b> Slonimsky</a></li>
</ul></li>
<li class="chapter" data-level="7.12" data-path="pyin-r-package.html"><a href="pyin-r-package.html#other-useful-functions"><i class="fa fa-check"></i><b>7.12</b> Other useful functions</a>
<ul>
<li class="chapter" data-level="7.12.1" data-path="pyin-r-package.html"><a href="pyin-r-package.html#assessing-musical-behaviours-musicassessr"><i class="fa fa-check"></i><b>7.12.1</b> Assessing musical behaviours: musicassessr</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="musicassessr.html"><a href="musicassessr.html"><i class="fa fa-check"></i><b>8</b> musicassessr</a>
<ul>
<li class="chapter" data-level="8.1" data-path="musicassessr.html"><a href="musicassessr.html#the-musicassessr-ecosystem"><i class="fa fa-check"></i><b>8.1</b> The musicassessr ecosystem</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="musicassessr.html"><a href="musicassessr.html#musical-ability-tests"><i class="fa fa-check"></i><b>8.1.1</b> Musical ability tests</a></li>
<li class="chapter" data-level="8.1.2" data-path="musicassessr.html"><a href="musicassessr.html#cheat-sheet"><i class="fa fa-check"></i><b>8.1.2</b> Cheat Sheet</a></li>
<li class="chapter" data-level="8.1.3" data-path="musicassessr.html"><a href="musicassessr.html#analysis-pipeline"><i class="fa fa-check"></i><b>8.1.3</b> Analysis pipeline</a></li>
<li class="chapter" data-level="8.1.4" data-path="musicassessr.html"><a href="musicassessr.html#research-and-documentation"><i class="fa fa-check"></i><b>8.1.4</b> Research and Documentation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="setup.html"><a href="setup.html"><i class="fa fa-check"></i><b>9</b> Setup</a>
<ul>
<li class="chapter" data-level="9.1" data-path="setup.html"><a href="setup.html#references-4"><i class="fa fa-check"></i><b>9.1</b> References</a></li>
<li class="chapter" data-level="9.2" data-path="setup.html"><a href="setup.html#ability-tests"><i class="fa fa-check"></i><b>9.2</b> Ability tests</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="setup.html"><a href="setup.html#play-by-ear-test-pbet"><i class="fa fa-check"></i><b>9.2.1</b> Play By Ear Test (PBET)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="play-by-ear-test-pbet-1.html"><a href="play-by-ear-test-pbet-1.html"><i class="fa fa-check"></i><b>10</b> Play By Ear Test (PBET)</a>
<ul>
<li class="chapter" data-level="10.1" data-path="play-by-ear-test-pbet-1.html"><a href="play-by-ear-test-pbet-1.html#author"><i class="fa fa-check"></i><b>10.1</b> Author</a></li>
<li class="chapter" data-level="10.2" data-path="play-by-ear-test-pbet-1.html"><a href="play-by-ear-test-pbet-1.html#demos"><i class="fa fa-check"></i><b>10.2</b> Demos</a></li>
<li class="chapter" data-level="10.3" data-path="play-by-ear-test-pbet-1.html"><a href="play-by-ear-test-pbet-1.html#installation-1"><i class="fa fa-check"></i><b>10.3</b> Installation</a></li>
<li class="chapter" data-level="10.4" data-path="play-by-ear-test-pbet-1.html"><a href="play-by-ear-test-pbet-1.html#usage-1"><i class="fa fa-check"></i><b>10.4</b> Usage</a></li>
<li class="chapter" data-level="10.5" data-path="play-by-ear-test-pbet-1.html"><a href="play-by-ear-test-pbet-1.html#usage-notes"><i class="fa fa-check"></i><b>10.5</b> Usage notes</a></li>
<li class="chapter" data-level="10.6" data-path="play-by-ear-test-pbet-1.html"><a href="play-by-ear-test-pbet-1.html#citation"><i class="fa fa-check"></i><b>10.6</b> Citation</a></li>
<li class="chapter" data-level="10.7" data-path="play-by-ear-test-pbet-1.html"><a href="play-by-ear-test-pbet-1.html#references-5"><i class="fa fa-check"></i><b>10.7</b> References</a>
<ul>
<li class="chapter" data-level="10.7.1" data-path="play-by-ear-test-pbet-1.html"><a href="play-by-ear-test-pbet-1.html#singing-ability-assessment-saa"><i class="fa fa-check"></i><b>10.7.1</b> Singing Ability Assessment (SAA)</a></li>
</ul></li>
<li class="chapter" data-level="10.8" data-path="play-by-ear-test-pbet-1.html"><a href="play-by-ear-test-pbet-1.html#author-1"><i class="fa fa-check"></i><b>10.8</b> Author</a></li>
<li class="chapter" data-level="10.9" data-path="play-by-ear-test-pbet-1.html"><a href="play-by-ear-test-pbet-1.html#demos-1"><i class="fa fa-check"></i><b>10.9</b> Demos</a></li>
<li class="chapter" data-level="10.10" data-path="play-by-ear-test-pbet-1.html"><a href="play-by-ear-test-pbet-1.html#installation-2"><i class="fa fa-check"></i><b>10.10</b> Installation</a></li>
<li class="chapter" data-level="10.11" data-path="play-by-ear-test-pbet-1.html"><a href="play-by-ear-test-pbet-1.html#usage-2"><i class="fa fa-check"></i><b>10.11</b> Usage</a></li>
<li class="chapter" data-level="10.12" data-path="play-by-ear-test-pbet-1.html"><a href="play-by-ear-test-pbet-1.html#usage-notes-1"><i class="fa fa-check"></i><b>10.12</b> Usage notes</a></li>
<li class="chapter" data-level="10.13" data-path="play-by-ear-test-pbet-1.html"><a href="play-by-ear-test-pbet-1.html#citation-1"><i class="fa fa-check"></i><b>10.13</b> Citation</a></li>
<li class="chapter" data-level="10.14" data-path="play-by-ear-test-pbet-1.html"><a href="play-by-ear-test-pbet-1.html#references-6"><i class="fa fa-check"></i><b>10.14</b> References</a></li>
<li class="chapter" data-level="10.15" data-path="play-by-ear-test-pbet-1.html"><a href="play-by-ear-test-pbet-1.html#conclusion"><i class="fa fa-check"></i><b>10.15</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="melsim_development.html"><a href="melsim_development.html"><i class="fa fa-check"></i><b>11</b> Experiment 1: Development of a Melodic Similarity Algorithm for Short Recalled Melodies</a></li>
<li class="chapter" data-level="12" data-path="pbet_lab_study.html"><a href="pbet_lab_study.html"><i class="fa fa-check"></i><b>12</b> Experiment 2: An Explanatory Model Of Playing By Ear</a>
<ul>
<li class="chapter" data-level="12.1" data-path="pbet_lab_study.html"><a href="pbet_lab_study.html#methods"><i class="fa fa-check"></i><b>12.1</b> Methods</a>
<ul>
<li class="chapter" data-level="12.1.1" data-path="pbet_lab_study.html"><a href="pbet_lab_study.html#participants-and-preliminary-questionnaire"><i class="fa fa-check"></i><b>12.1.1</b> Participants and Preliminary Questionnaire</a></li>
<li class="chapter" data-level="12.1.2" data-path="pbet_lab_study.html"><a href="pbet_lab_study.html#apparatus-and-general-procedure"><i class="fa fa-check"></i><b>12.1.2</b> Apparatus and General Procedure</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="pbet_online_study.html"><a href="pbet_online_study.html"><i class="fa fa-check"></i><b>13</b> Experiment 3: Modelling Melodic Difficulty in Playing by Ear: An Item Response Analysis of Online Performance Data</a></li>
<li class="chapter" data-level="14" data-path="study_history_study.html"><a href="study_history_study.html"><i class="fa fa-check"></i><b>14</b> Experiment 4: A Longitudinal Study of Playing By Ear Skills</a></li>
<li class="chapter" data-level="15" data-path="similarity_study.html"><a href="similarity_study.html"><i class="fa fa-check"></i><b>15</b> Experiment 5: A Similarity-Based Approach to Item Selection</a>
<ul>
<li class="chapter" data-level="15.1" data-path="similarity_study.html"><a href="similarity_study.html#collaborative-filtering"><i class="fa fa-check"></i><b>15.1</b> Collaborative Filtering</a></li>
<li class="chapter" data-level="15.2" data-path="similarity_study.html"><a href="similarity_study.html#approaches-to-similarity-for-large-scale-corpora"><i class="fa fa-check"></i><b>15.2</b> Approaches to Similarity for Large-Scale Corpora</a></li>
<li class="chapter" data-level="15.3" data-path="similarity_study.html"><a href="similarity_study.html#melodic-corpora-as-networks"><i class="fa fa-check"></i><b>15.3</b> Melodic corpora as networks</a>
<ul>
<li class="chapter" data-level="15.3.1" data-path="similarity_study.html"><a href="similarity_study.html#approach-1-similarity-of-features-as-a-proxy-for-melodic-similarity"><i class="fa fa-check"></i><b>15.3.1</b> Approach #1: Similarity of Features as a Proxy for Melodic Similarity</a></li>
<li class="chapter" data-level="15.3.2" data-path="similarity_study.html"><a href="similarity_study.html#approach-2-generative-similarity"><i class="fa fa-check"></i><b>15.3.2</b> Approach #2: Generative Similarity</a></li>
<li class="chapter" data-level="15.3.3" data-path="similarity_study.html"><a href="similarity_study.html#approach-3-parent-melody-representation"><i class="fa fa-check"></i><b>15.3.3</b> Approach #3: Parent Melody Representation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="16" data-path="additional-learning-paradigms.html"><a href="additional-learning-paradigms.html"><i class="fa fa-check"></i><b>16</b> Additional Learning Paradigms</a>
<ul>
<li class="chapter" data-level="16.1" data-path="additional-learning-paradigms.html"><a href="additional-learning-paradigms.html#generative-similarity"><i class="fa fa-check"></i><b>16.1</b> Generative Similarity</a></li>
<li class="chapter" data-level="16.2" data-path="additional-learning-paradigms.html"><a href="additional-learning-paradigms.html#learn-chunks"><i class="fa fa-check"></i><b>16.2</b> Learn Chunks</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="experiment-6-model-synthesis-development-of-a-musical-dashsim-model.html"><a href="experiment-6-model-synthesis-development-of-a-musical-dashsim-model.html"><i class="fa fa-check"></i><b>17</b> Experiment 6: Model synthesis: Development of a musical DASH+SIM model</a></li>
<li class="chapter" data-level="18" data-path="interventional_Study.html"><a href="interventional_Study.html"><i class="fa fa-check"></i><b>18</b> Experiment 7: An Interventional Study</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Playing By Ear: A Computational Approach</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="a-comprehensive-theorectical-model-of-playing-by-ear" class="section level1 hasAnchor" number="4">
<h1><span class="header-section-number">4</span> A Comprehensive Theorectical Model of Playing By Ear<a href="a-comprehensive-theorectical-model-of-playing-by-ear.html#a-comprehensive-theorectical-model-of-playing-by-ear" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Consequently, synthesising the results above, we can characterise playing by ear as involving the following processes. Where I mark the text grey, I highlight an additional suggestion beyond the reviewed literature, as a contribution from my own theory development. I will refer to the person playing by ear as the “player” in the following description.</p>
<p>Playing by ear first requires a target melody to be perceived. Due to the nature of serial presentation, a presented auditory melody unfolds note-by-note. From the acoustic signal, the player decodes the fundamental frequencies into a melodic object based on their structural knowledge, based on a lifetime of listening (enculturation). The ability to do this also depends on the acoustic features themselves, especially its timbre. The closer the timbre to the player’s on instrument, the easier it is to decode (and later recall) because the player can draw not only on melodic memory, memory for absolute pitch, including multimodal representations that might activate kinaesthetic/tactile traces (CITE). This overall ability to image the correct melodic object is what we call “audiation”. More experienced listeners and better musical training are better able to audiate this image, segmenting/chunking the melody into relevant chunks. Each time a new note is heard from the melody, its strain on working memory increases based on the information content in each new note, but also, which ultimate represents the emergent tonal, rhythmic, metric and other structural features. In long term working memory for melodies, N-Gram chunks are distinct entities, each with their own properties - in particular, the amount that the player has experienced them in their listening experiences (including practising an instrument). This statistical sensibility should approximate the real frequency of occurrence in music. If there are visual cues (e.g., visual notation or a model - like a teacher to immitate) accompanying the audiated melody, these will be decoded in their respective working memory slave systems and potentially integrated in the episodic buffer, to form a multimodal representaiton that could inform overall recall.</p>
<p>Once the target melody has completed, it will be stored in a short term memory buffer and flagged as a point of reference. The longer this representation remains in this buffer without mental or physical rehearsal, the more it will decay in memory in a non-linear fashion.</p>
<p>The extent of the target melody’s burden on working memory load dictates if how the player will attempt to recall the melody, if at all. If working memory capacity is greatly surpased, with the player able to retrieve no reliable representations, they may not attempt to recall the melody at all. If they have some vague representations, they might attempt to improve a sketch of what they heard. In a performance/high pressure situation (e.g., an aural exam), maybe they will have some fallback approach to attempt to respond musically or “style it out”. Depending on the context, maybe the player (e.g., in the practising/learning) will take a strategy to help reinforce the melodic image because attempting to reproduce it, for example, by singing it.</p>
<p>Eventually, if at least some - if not all - of the melody of the is represented reliably in melodic long term working memory, the player will attempt to produce it in the recall and an action plan will be setup to reproduce the melody. Potentially, clear motor action plans will have already been activated in respond to the stimulus in the form of imagined actions to reproduce the sequence. The player will direct their attention to the instrument and the tactile sensations and motoric actions required to reproduce the sound. As they produce each note, these will be perceived by the ear and parsed again, note-by-note into first basic auditory representations in the phonological loop, but then higher order melodic structural ones in melodic working memory. Because the recalled melody is being physically produced by the player, it will have richer cues: e.g., visual ones, for a pianist directing their fingers to different keys, tactile ones for all instruments, motoric cues (i.e., the pattern tactile sensations). Again, they will be parsed eventually into an unfolding multimodal melodic representation.</p>
<p>Every time a new note is recalled, a melodic similarity algorithm will compute on the latest version of the recalled melody in relation to the target melody. Generally, as each new note is recalled, the similarity should go up. If the similarity goes down, this will be flagged as a prediction error, feeding back to motor plans and potentially causing the player to make judgements. The player will presumably attempt to make the rest of the recall maximise the similarity, even if certain notes or rhythms have clearly violated the possibility of perfect similarity. Typically, target melodies that were not held strongly enough in working memory or where there were errors in the recall will lead to shorter recalls, whereby the player may stop, perhaps to re-imagine the melodic object, or, if possible, to take another attempt at hearing it and trying again. If there are multiple attempts, which is usually the case in learning a melody, each attempt will increase in length and similarity, until the length of target and recall match and there is perfect similarity, or, if this is not possible in a given (learning) session, working on that melody will eventually be terminated once fatigue or motivation is sufficiently low. Returning to replay that melody at a later time, especially after a night’s sleep, due to consolidation effect, should typically make the process easier next time, so long as the melody in question was too difficult - and therefore, the feasibility of this is a function of a melody’s difficulty, which is also a function of a player’s ability (which is in turn a function of their musical training and general working memory capacity).</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="a-comprehensive-theorectical-model-of-playing-by-ear.html#cb4-1" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">include_graphics</span>(<span class="st">&#39;../img/PBE Computational Model.png&#39;</span>)</span></code></pre></div>
<p><img src="../img/PBE%20Computational%20Model.png" /><!-- --></p>
<div id="musicological_background" class="section level2 hasAnchor" number="4.1">
<h2><span class="header-section-number">4.1</span> Computational Musicological Perspectives<a href="a-comprehensive-theorectical-model-of-playing-by-ear.html#musicological_background" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Computational musicological approaches help motivate or solve three general problems in the present thesis: 1) some basic mathematics illustrate and motivate the problem of <em>melodic combinatorics</em> and I discuss how <em>corpus analysis</em> can help deal with combinatoric problems, 2) <em>computational melodic feature analysis</em> helps solve the problem of <em>item selection</em> and 3) <em>melodic similarity</em> research helps solve the problem of <em>score computation</em> regarding the accuracy of participants recalling melodies by ear in relation to a target melody. In this section, I will explore these areas and their relevance to the present thesis.</p>
<div id="music-and-combinatorics" class="section level3 hasAnchor" number="4.1.1">
<h3><span class="header-section-number">4.1.1</span> Music and Combinatorics<a href="a-comprehensive-theorectical-model-of-playing-by-ear.html#music-and-combinatorics" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As discussed in the section on historical context, Nicolas Slonimsky derived his <em>Thesaurus of Scales and Melodic Patterns</em>. This was essentially an attempt to document and systematise the possibilities within the twelve-tone system to unlock new melodic vocabulary, which, was subseqently picked up by jazz improvisers such as John Coltrane to inspire melodic invention. Slonimsky undertook this task manually via a system of dividing the octave into parts and interpolating intervening notes, among other various algorithmic procedures.</p>
<p>This process of enumeration leads to a more general issue around combinatorics and musical, melodic, material. This problem of enumeration is much easier to solve computationally, up to a point, where computational tractability diminishes.</p>
<p>In general, the number of melodic possibilities, given a</p>
<p>Let:</p>
<ul>
<li><span class="math inline">\(n\)</span>: number of intervals (length of the n-gram)</li>
<li><span class="math inline">\(I = \{1, 2, \ldots, h\}\)</span>: the set of possible intervals, where <span class="math inline">\(h\)</span> is the highest interval (e.g., 12, for an octave)</li>
</ul>
<p>Then the set of all melodic n-grams is:</p>
<p><span class="math display">\[
\text{Combos} = I^n = \{ (i_1, i_2, \ldots, i_n) \mid i_j \in I \text{ for } j = 1, \ldots, n \}
\]</span></p>
<p>and the number of combinations is:</p>
<p><span class="math display">\[
\text{Number of combinations} = |I^n| = h^n
\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(|I^n|\)</span> is the number of elements in the Cartesian product <span class="math inline">\(I^n\)</span></li>
<li><span class="math inline">\(h^n\)</span> means you are choosing one of <span class="math inline">\(h\)</span> intervals at each of the <span class="math inline">\(n\)</span> positions</li>
</ul>
<p>Already for even the relatively short melody length of nine notes (eight intervals), with a maximum interval semitone size of 12 (i.e., an octave), 429,981,696 melodies are possible and the computational feasibility of computing all such melodies is relatively limited on an ordinary modern day laptop.</p>
<p>This suggests a few points: 1) Slonimsky’s thesaurus of fewer than 1,500 was by no means exhaustive; 2) up until a point, such a task is much easier with a computer, with a few lines of code; 3) this “melodyverse” ensures that composers and improvisers will never cease to find new melodic sounds, which is great from a creative point of view.</p>
<p>From a cognitive point of view, this may lead the musician learning to play by ear to be nervous: how can one select from the melodyverse, or indeed, a small, but still overwhelming version of it, like Slonimsky’s Thesaurus. Other than the general approach explicated in this thesis, there are several reasons to reserve dauntedness. First, in practice, the combinations of twelve tones are usually constrained in various ways: the first level of constraint in Western music are usually to bound note choices within the seven-note major or minor scales, or subsets of these “tonal” scales, like the pentatonic scale. However, the use of chromaticism within Western harmony re-opens the melodic space again. Additionally, it may not be necessary to have encountered and practised a specific melodic combination in one’s life study history to successfully play it by ear. It is more likely that some level of abstraction develops such that previous learning (the training set) can still generalise to a wider test set; notions important to (artifical) intelligence. Therefore, it is conceivable, that if one chose training data intelligently, one’s playing by ear abilities could project to a much wider space, eventually. This is the goal of this thesis: to constantly find the parts of the melodic space that can open a learner’s ear to possibilities they could not previously hear, be that at a very beginner, or the level of Coltrane. There must be a general solution to this problem.</p>
<!-- 

-->
</div>
<div id="music-corpora-analysis" class="section level3 hasAnchor" number="4.1.2">
<h3><span class="header-section-number">4.1.2</span> Music Corpora Analysis<a href="a-comprehensive-theorectical-model-of-playing-by-ear.html#music-corpora-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><span class="citation">(Müllensiefen and Frieler n.d.)</span></p>
</div>
<div id="melodic-represention-and-computational-melodic-features" class="section level3 hasAnchor" number="4.1.3">
<h3><span class="header-section-number">4.1.3</span> Melodic Represention and Computational Melodic Features<a href="a-comprehensive-theorectical-model-of-playing-by-ear.html#melodic-represention-and-computational-melodic-features" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A melody can be represented symbolically, in a way that makes computational analysis, in a number of ways. A simple convention, is to use <em>MIDI</em> notation <span class="citation">(<strong>rothsteinMIDIComprehensiveIntroduction1996?</strong>)</span>, whereby pitches are represented as integers, with e.g., C4 being represented by 60, Db4 by 61 etc. For durations, millsecond or second values can be used. This is a basic low-level representation, which may be suitable for simple playback. However, this precludes many other possible higher-level musical representations, such as those to do with phrases, meter, not to mention articulation etc.</p>
<p><span class="citation">Conklin and Witten (1995)</span> proposed the Multiple Viewpoint System (MVS) as a cognitively motivated framework for modelling musical expectation and structure. Acknowledging that human listeners attend to a range of simultaneous musical cues—such as pitch, rhythm, contour, and phrasing—the MVS represents each musical event through a set of distinct viewpoints, each capturing a different musically salient abstraction (e.g., melodic interval, position within the bar, or phrase boundaries).</p>
<p>The system incorporates both long-term statistical knowledge (learned from a corpus of musical works) and short-term contextual information (specific to the piece in progress). This reflects the dual influence of stylistic familiarity and local pattern recognition in human musical cognition. Each viewpoint is modelled as an independent probabilistic context model, and their predictions are combined to estimate the probability of upcoming events. Predictive performance is evaluated using entropy, with lower entropy interpreted as greater predictability.</p>
<p>The authors argue that low-entropy predictions align with cognitive plausibility, positioning entropy as a proxy for musical expectation. In empirical tests on Bach chorale melodies, the model achieved prediction accuracy close to that of human listeners. The multiple viewpoint system thus offers a compelling computational account of how listeners may internally represent and anticipate musical structure by integrating multiple concurrent sources of musical information.</p>
<p>In general, See <span class="citation">Frieler (2018)</span> for a thorough description of monophonic melodic representation in music.</p>
<div id="melodic-feature-analysis" class="section level4 hasAnchor" number="4.1.3.1">
<h4><span class="header-section-number">4.1.3.1</span> Melodic Feature Analysis<a href="a-comprehensive-theorectical-model-of-playing-by-ear.html#melodic-feature-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Conversely, for melodic items with multiple notes, musical features emerge (e.g., tonality, contour, rhythm). Such emergent features clearly rely on high-level mental representations and templates (i.e., musical knowledge). Consequently, there can be significant variance in complexity when a melody is the item of testing, and these kind of item difficulties are important to model. Important melodic representations can be quantified for each melodic item across important dimensions <span class="citation">(<strong>mullensiefenFANTASTICFeatureANalysis2009?</strong>)</span>. As suggested by previous literature <span class="citation">(Baker 2019; <strong>dreyfusRecognitionLeitmotivesRichard2016?</strong>; <strong>harrisonModellingMelodicDiscrimination2016?</strong>)</span>, there are several melodic features which could indicate an item’s complexity and predict singing performance (e.g., tonality, interval contour, a melody’s frequency in occurrence).</p>
</div>
</div>
<div id="computed-melodic-features-as-a-musicological-basis-for-selecting-items" class="section level3 hasAnchor" number="4.1.4">
<h3><span class="header-section-number">4.1.4</span> Computed Melodic Features as a Musicological Basis for Selecting Items<a href="a-comprehensive-theorectical-model-of-playing-by-ear.html#computed-melodic-features-as-a-musicological-basis-for-selecting-items" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Highlighting the intrinsic link between melodic features and human cognition, <span class="citation">Herborn (2022)</span> suggested the notion of “composed melodies <em>as</em> applied music psychology”.</p>
</div>
<div id="pattern-books-as-item-banks" class="section level3 hasAnchor" number="4.1.5">
<h3><span class="header-section-number">4.1.5</span> Pattern Books as Item Banks<a href="a-comprehensive-theorectical-model-of-playing-by-ear.html#pattern-books-as-item-banks" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The book has at least 1,400 discrete items and some of the patterns are terrifically long and complicated. Additionally, nearly all of the melodies are presented from a single note, C, and could be transposed to start on the other 11 chromatic tones. This means there are at least 16,800 (12 * 1400) items to be learned. The author’s saxophone tutor, who was a virtuoso, once said, “Oh, don’t bother trying to learn [Slonimsky’s Thesaurus of Scales and Melodic Patterns] systematically”. This was a wise comment since, as there is so much information, it is almost impossible for a human to comprehend and track their learning of it systematically. However, with advances in computation, this is a relatively easy challenge to solve once the data is in digital form. The author entered the Slonimsky’s Thesaurus corpus into his computer to provide a digital representation. This corpus will remain the test case for description in this document later on.</p>
</div>
</div>
<div id="melodic-similarity" class="section level2 hasAnchor" number="4.2">
<h2><span class="header-section-number">4.2</span> Melodic Similarity<a href="a-comprehensive-theorectical-model-of-playing-by-ear.html#melodic-similarity" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the scientific area that has been termed <em>Music Information Retrieval</em> <span class="citation">(Downie 2003)</span>, and that has seen a large boost in recent years, several approaches to <em>similarity</em> measurement for melodies and other musical objects have been developed <span class="citation">(e.g., Müllensiefen and Frieler 2004a; Pearce and Müllensiefen 2017; Typke, Wiering, and Veltkamp 2007; Yuan et al. 2020)</span>. The similarity measures employed in the present dissertation are favoured because they proved their effectiveness and ecological validity (or rather comparability) with the notion of melodic similarity of musically experienced participants in separate studies <span class="citation">(Müllensiefen and Pendzich 2009; Müllensiefen and Frieler 2004b, 2007)</span>.</p>
<p>Therefore, while there still might not be an undisputed theory of melodic identity, as Sloboda and Parker claimed in 1985, this study will use some algorithms that at least came quite close in emulating musically experienced participants’ similarity judgements. However, whilst measures of similarity have been validated in the context of human perceptual judgements (e.g., to predict court case outcomes), we are not aware that they have been profiled in the context of sung recall, as we intend to do here.</p>
<!--
### Set Theory

### Fuzzification

-->
<div id="methodological-background" class="section level3 hasAnchor" number="4.2.1">
<h3><span class="header-section-number">4.2.1</span> Methodological Background<a href="a-comprehensive-theorectical-model-of-playing-by-ear.html#methodological-background" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Having obtained numerical representations of both sung recalls and the target melodies, the similarities between a target melody and sung recall of that melody, for each attempt of each participant, can be calculated using the algorithmic similarity measures described in <span class="citation">Müllensiefen and Frieler (2004b)</span>. The similarity measures employed here comply with the two main points already raised by <span class="citation">Sloboda and Parker (1985)</span> in their discussion of their methodology of melodic comparison: in most cases - that is especially true for the earlier trials - participants only recall a smaller part of the original melody, which may not even start with the beginning of the original. Thus, a similarity measure (or algorithm) must be chosen that automatically looks for the best alignment of the (short) melodic sequence of the sung recall with the original melodic sequence. <span class="citation">Sloboda and Parker (1985)</span> manually attempted a form of alignment, making it a cumbersome task, but additionally, their method is not precisely described. To advance on this point, we detail a precise computational method.</p>
<p>An algorithm for the optimal alignment of two symbol sequences that has been widely used in domains such as text retrieval or bio-computing, as well as music information retrieval, is the so-called Edit Distance or Levenshtein distance <span class="citation">(e.g., <strong>mongeauComparisonMusicalSequences1990?</strong>)</span>. The Edit Distance is the minimum number of operations it takes to transform one symbol string into another: the possible operations being insertion, deletion, and substitution. The actual calculation of the Edit Distance is carried out using dynamic programming and is not explained here. For a general reference regarding the algorithm see e.g., <span class="citation">(<strong>gusfieldAlgorithmsStringsTrees1997?</strong>)</span>. In this case, the maximal Edit Distance of two strings is equal to the length of the longer string. To convert the Edit Distance into a similarity measure with a range of values <span class="math inline">\([0,1]\)</span> we use the following:</p>
<p><span class="math display" id="eq:editdist">\[\begin{equation}
\sigma(s,t)=1-\frac{d_e (s,t)}{\max(|s|,|t|)}
\tag{4.1}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(|s|\)</span> and <span class="math inline">\(|t|\)</span> denote the element counts of strings <span class="math inline">\(s\)</span> and <span class="math inline">\(t\)</span> respectively, and <span class="math inline">\(d_e(s,t)\)</span> stands for the Edit Distance between strings <span class="math inline">\(s\)</span> and <span class="math inline">\(t\)</span>.</p>
<p>Just like the manual scoring techniques employed by <span class="citation">Sloboda and Parker (1985)</span>, the edit distance calculates the similarity between two symbolic sequences by taking the number of edits (i.e. additions, deletions or substitutions) that are necessary to transform one of the sequences into the other, and dividing this figure by the number of symbols in the longer sequence. It thus could be argued that Sloboda and Parker intuitively used a version of the edit distance, evaluating the similarity between the recalls of their participants on the original melody, keeping the order of notes in mind.</p>
<p>However, importantly, instead of applying the edit distance to raw note values, here the edit distance is computed on various symbolic representations of musical dimensions (i.e., relative pitch sequences - intervals - as opposed to single pitches; rhythm sequences; and sequences of implied harmonies; <span class="citation">Müllensiefen and Frieler (2004b)</span>). Specifically, we employ the <em>opti3</em> measure of melodic similarity <span class="citation">(Müllensiefen and Frieler 2004b)</span> as our main dependent variable. <em>opti3</em> is a hybrid measure derived from the weighted sum of three individual measures which represent different aspects of melodic similarity. The similarity in interval content is captured by the <em>ngrukkon</em> measure is based on the Ukkonen Measure that measures the difference of the occurrence frequencies of interval trigrams (<span class="math inline">\(\tau\)</span>) contained within the target melody (<span class="math inline">\(f_s (\tau)\)</span>) and the comparison melody (<span class="math inline">\(f_s (\tau)\)</span>) (see <span class="citation">(<strong>uitdenbogerdMusicInformationRetrieval2002?</strong>)</span> Formally:</p>
<p><span class="math display" id="eq:ngrukkon">\[\begin{equation}
u(s,t)=\sum_{\tau \in s_n \cup t_n} |f_s (\tau)-f_t(\tau)|
\tag{4.2}
\end{equation}\]</span></p>
<p>As the Ukkonen Measure is a distance measure in its original definition, we normalise by the maximum possible number of <span class="math inline">\(n\)</span>-grams and subtract the result from 1:</p>
<p><span class="math display" id="eq:ngrukkon2">\[\begin{equation}
\sigma (s,t) = 1 - \frac{u(s,t)}{|s|+|t|-2(n-1)}
\tag{4.3}
\end{equation}\]</span></p>
<p>Note that the Ukkonen measure is not based on the edit distance but still takes order of notes into account at a local level by comparing trigrams of pitch intervals.</p>
<p>Harmonic similarity is measured by the <em>harmcore</em> measure. This measure is based on the chords implied by a melodic sequence, taking pitches and durations (i.e., segmentation) into account. Implied harmonies are computed using the Krumhansl-Schmuckler algorithm <span class="citation">(Krumhansl 1990)</span> and the harmonic progression of the two melodies are compared by computing the number of operations necessary to transform one harmonic progression into the other sequence via the edit distance. Finally, likewise, rhythmic similarity is computed by first categorizing the durations of the notes of both melodies (known as “fuzzification”) and then applying the edit distance to measure the distance between the two sequences of categorised durations. The resulting measure of rhythmic similarity is called <em>rhythfuzz</em> <span class="citation">(Müllensiefen and Frieler 2004b)</span>. Note that <em>rhythfuzz</em> does not take metric information into account and works solely based on (relative) note durations. Similarly, <em>ngrukkon</em> works with interval information and is hence invariant to transposition.</p>
<p>Based on the perceptual data collected by <span class="citation">Müllensiefen and Frieler (2004b)</span>, the three individual measures are weighted and combined to form a single aggregate measure of melodic similarity, <em>opti3</em>. Hence, <em>opti3</em> is sensitive to similarities and differences in three important aspects of melodic perception (pitch intervals, harmony, rhythm). We note that all three individual measures (<em>ngrukkon</em>, <em>harmcore</em>, <em>rhythfuzz</em>) can take values between 0 (= no similarity) and 1 (= identity) and are length-normalized by considering the number of elements of the longer melody. <em>opti3</em> then comprises <span class="citation">(Müllensiefen and Frieler 2004b)</span>:</p>
<p><span class="math display" id="eq:opti3">\[\begin{equation}
{opti3}= 0.505 \cdot \mathtt{ngrukkon} + 0.417 \cdot \mathtt{rhythfuzz} + 0.24 \cdot \mathtt{harmcore} -0.146
\tag{4.4}
\end{equation}\]</span></p>
<p>where we here present the normalised weights, which constrain the values to be [0,1].</p>
<p>Beyond the target or comparison melody lengths being used to normalise the <em>opti3</em> score, we note that <em>opti3</em> is dependent on the length of the two comparison melodies further in only a soft sense, which is particularly relevant to Experiment 2 of this paper, where we use the sung recall attempt length as an auxiliary dependent variable. If one melody is shorter than the other, at least some of the melodic identity is destroyed: necessarily, the rhythmic (<em>rhythfuzz</em>) and intervallic (<em>ngrukkon</em>) components, but not necessarily the harmonic (<em>harmcore</em>) component. It should be clear that <em>opti3</em> captures far more (musical) information than melody length(s) alone and/or accuracy-style measures. The ecological validity of the aggregate similarity measure has been established in several perceptual experiments <span class="citation">(Müllensiefen and Pendzich 2009; Yuan et al. 2020)</span>. See Appendix B for concise descriptions comparing the similarity measures. Moreover, to build an intuition on how similarity measures change in relation to attempt, see Appendix C for notated examples of development in sung recall performance and a qualitative description of their change in similarity.</p>
<p>In summary, similarity measures pay attention to musical features that arise from the relationships between pitch and rhythmic values and could be considered more “global” in nature. Conversely, accuracy measures which count notes or even intervals (bigrams), do not respect the higher order emergent properties of musical features. Consequently, aggregate similarity measures have a greater ability to represent perceptual properties relevant in human cognition and represent a robust step towards computationally representing a notion of melodic identity. In this way, similarity algorithms have been used to predict subjective similarity judgements, for example, in musical plagiarism court cases, with excellent success <span class="citation">(Yuan et al. 2020; Müllensiefen and Pendzich 2009)</span>. As an aid in developing an intuitive understanding of the different properties arising from scoring melodic recall data on accuracy-style vs. similarity measures, see Appendix A2 for simple example comparisons.</p>
<p>The usage of melodic similarity metrics to score sung recall data has been previously documented by the present author <span class="citation">(Silas and Müllensiefen 2023; Silas, Müllensiefen, and Kopiez 2023)</span>.</p>
<div id="selecting-melodic-material-as-an-optimisation-problem" class="section level4 hasAnchor" number="4.2.1.1">
<h4><span class="header-section-number">4.2.1.1</span> Selecting Melodic Material as an Optimisation Problem<a href="a-comprehensive-theorectical-model-of-playing-by-ear.html#selecting-melodic-material-as-an-optimisation-problem" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Having motivated this work in terms of historical context up to contemporary technological approaches to improving playing by ear skills, I will now turn to motivating it in terms of the more specific epistemological approaches taken to assess this question in this dissertation. These approaches are: systematic musicological, cognitive psychological, and computational. In general, these approaches come from the <em>scientific approach</em>, whereas the content reviewed in the last chapter was largely not based on such an approach.</p>
</div>
</div>
</div>
<div id="cognitive_psychological_background" class="section level2 hasAnchor" number="4.3">
<h2><span class="header-section-number">4.3</span> Cognitive Psychology<a href="a-comprehensive-theorectical-model-of-playing-by-ear.html#cognitive_psychological_background" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>To support the second main motivation of this project: to build an item selection mechanism for playing by ear items, it is necessary to review literature regarding learning and memory more generally, and where more extensive research has been undertaken into finding items for people to learn as well as review.</p>
<div id="theoretical-background" class="section level4 hasAnchor" number="4.3.0.1">
<h4><span class="header-section-number">4.3.0.1</span> Theoretical Background<a href="a-comprehensive-theorectical-model-of-playing-by-ear.html#theoretical-background" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Learning and memory are fundamental cognitive processes that allow us to acquire new knowledge and retain it over time. Psychologists have long sought to understand how we learn and why we forget. Early experimental work established basic patterns of memory retention and loss that remain influential today. For example, pioneering studies by Hermann Ebbinghaus in the 1880s laid the groundwork for formal memory research by charting how quickly information is forgotten <span class="citation">(Cepeda et al. 2008; Ebbinghaus 1888)</span>. Since then, cognitive psychology has built a rich literature on the principles underlying learning and forgetting. This review provides an overview of general principles of learning and memory from a cognitive perspective, with an emphasis on how these principles manifest in both short-term and long-term memory. We will discuss historical findings (such as Ebbinghaus’s forgetting curve and Jenkins &amp; Dallenbach’s work on memory and sleep), examine the phenomena of learning and forgetting curves in different time scales, and explore the effectiveness of spaced repetition as a learning technique. We also highlight a recent computational model of memory – the DASH model (Difficulty, Ability, and Study History) by <span class="citation">Mozer and Lindsey (2016)</span> – and discuss how these cognitive principles are applied in modern language learning approaches and apps. Throughout, we ground the review in peer-reviewed findings to ensure academic rigor while keeping the explanations accessible to a general audience.</p>
<div id="learning-and-forgetting-curves" class="section level5 hasAnchor" number="4.3.0.1.1">
<h5><span class="header-section-number">4.3.0.1.1</span> Learning and Forgetting Curves<a href="a-comprehensive-theorectical-model-of-playing-by-ear.html#learning-and-forgetting-curves" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>One of the core findings in cognitive psychology is that both learning and forgetting tend to follow systematic, nonlinear patterns over time. A learning curve describes how performance or recall improves with repeated practice. Typically, learning is rapid initially, then the rate of improvement slows as one approaches mastery – a phenomenon often described as a negatively accelerated curve (early gains followed by plateau). Conversely, a forgetting curve describes how retained information is lost over time when no further practice occurs. Hermann Ebbinghaus was the first to quantitatively measure a forgetting curve. Using himself as a subject, Ebbinghaus memorized lists of nonsense syllables and tested his memory at various delays, from 20 minutes to 31 days. He found that forgetting occurs rapidly at first – a large proportion of the information is lost soon after learning – and then the rate of forgetting gradually slows down <span class="citation">(Cepeda et al. 2008)</span>. In other words, memory retention drops steeply in the first hours or day, then levels off, never quite reaching zero. This characteristic shape has been replicated in modern studies. For instance, a 2015 replication of Ebbinghaus’s experiment confirmed the same pattern of a sharp initial decline followed by a more gradual loss, validating that the “forgetting curve” is a robust phenomenon <span class="citation">(Murre and Dros 2015)</span>. Theoretical analyses of forgetting curves suggest that memory decay is not linear but follows a curve that can often be described by a mathematical function (historically debated as exponential or power-law in form) <span class="citation">(Cepeda et al. 2008)</span>. The key practical implication is that most forgetting happens early, and what remains eventually stabilizes (information that survives the initial rapid forgetting may stay in memory for a long time).</p>
</div>
<div id="short-term-vs.-long-term-forgetting" class="section level5 hasAnchor" number="4.3.0.1.2">
<h5><span class="header-section-number">4.3.0.1.2</span> Short-Term vs. Long-Term Forgetting<a href="a-comprehensive-theorectical-model-of-playing-by-ear.html#short-term-vs.-long-term-forgetting" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>The time scale of retention is crucial. In short-term memory, forgetting can occur on the order of seconds when rehearsal is prevented. Classic experiments by <span class="citation">Peterson and Peterson (1959)</span> demonstrated that without active rehearsal, even a simple piece of information (such as a trigram of letters) fades quickly from short-term memory. In their study, participants were asked to remember a three-letter sequence, then were prevented from rehearsing it by performing an intervening task (counting backwards) for a variable duration. The results showed a dramatic drop in recall within half a minute – after just 15–18 seconds of distraction, memory for the letters was almost completely gone (on the order of only ~5–10% of trigrams recalled correctly). This rapid loss highlighted the very limited duration of short-term memory when maintenance via rehearsal is blocked. Long-term memory, on the other hand, operates over much longer intervals and is subject to different influences. Forgetting in long-term memory occurs more slowly and is heavily affected by factors such as meaningfulness of the material, repeated exposure, and interference from other learning. While time itself does contribute to forgetting, the role of interference is particularly evident in long-term retention. A famous study by <span class="citation">Jenkins and Dallenbach (1924)</span> provided early evidence that forgetting over several hours is not due solely to the passive decay of memory traces with time, but is exacerbated by waking activity (i.e. interference from new experiences). In this study, people learned nonsense syllables and were tested after a retention interval spent either sleeping or staying awake. The researchers found memory performance was significantly better after a period of sleep than after an equivalent period of wakefulness <span class="citation">(Fenn and Hambrick 2013)</span>. During sleep, when little new information was encountered, participants forgot much less. The condition with wakefulness produced more forgetting, presumably because ongoing mental activities interfered with the previously learned syllables. Jenkins and Dallenbach concluded that interference plays a major role in forgetting: our memories fade faster when we are bombarded with new information (or other cognitive activity) in the interim. Later research on sleep and memory consolidation has reaffirmed that sleep can stabilise memories by protecting them against retroactive interference from new learning <span class="citation">(Fenn and Hambrick 2013)</span>. In summary, short-term memory is fleeting unless actively maintained, and long-term memory, while more durable, is vulnerable to disruption by other experiences. These foundational insights set the stage for strategies to combat forgetting, such as managing interference and optimizing review timing.</p>
</div>
</div>
<div id="key-findings-from-cognitive-psychology" class="section level4 hasAnchor" number="4.3.0.2">
<h4><span class="header-section-number">4.3.0.2</span> Key Findings from Cognitive Psychology<a href="a-comprehensive-theorectical-model-of-playing-by-ear.html#key-findings-from-cognitive-psychology" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div id="ebbinghauss-forgetting-curve" class="section level5 hasAnchor" number="4.3.0.2.1">
<h5><span class="header-section-number">4.3.0.2.1</span> Ebbinghaus’s Forgetting Curve<a href="a-comprehensive-theorectical-model-of-playing-by-ear.html#ebbinghauss-forgetting-curve" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Hermann Ebbinghaus’s work in the 1880s was groundbreaking as the first systematic, quantitative study of human memory. Using the method of savings (measuring how many fewer trials were needed to relearn material after a delay), Ebbinghaus plotted the decline in memory retention over time. His forgetting curve showed that memory loss is steep soon after learning – he famously noted that more than half of learned syllables could be forgotten within hours – but then the curve flattens out. For example, if 100% of information is remembered immediately after learning, a day later a much smaller fraction might be retained, and further losses after that point are slower. Ebbinghaus also observed that even when something seems forgotten, not all is necessarily lost; savings in relearning demonstrate that some residual memory can facilitate faster re-learning later. The broader significance of Ebbinghaus’s finding is that forgetting is systematic rather than random. The initial rapid drop suggests a need for review soon after learning, while the leveling off indicates a core of memory that can persist. Ebbinghaus’s methods were primitive by today’s standards (he was his own sole test subject), yet remarkably, his results and general conclusions have stood the test of time. Modern quantitative models still aim to describe the exact shape of the forgetting curve, with many agreeing that a negatively accelerated function (such as a power-law decay) best fits empirical data. For learners, an intuitive takeaway from the forgetting curve is the importance of timely reviews: without reinforcement, memories will decay fastest early on.</p>
</div>
<div id="interference-and-consolidation" class="section level5 hasAnchor" number="4.3.0.2.2">
<h5><span class="header-section-number">4.3.0.2.2</span> Interference and Consolidation<a href="a-comprehensive-theorectical-model-of-playing-by-ear.html#interference-and-consolidation" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Building on Ebbinghaus’s insights, later researchers explored why memories fail. The concept of interference became central to theories of forgetting. Proactive interference (earlier learning disrupting new learning) and retroactive interference (new learning disrupting old memories) were identified as key processes that cause forgetting beyond the mere passage of time. Jenkins and Dallenbach’s 1924 experiment, discussed above, is an elegant demonstration of retroactive interference: when awake and taking in new input, the mind overwrites or obscures existing memories, whereas during sleep (with minimal new input) memory traces are better preserved <span class="citation">(Fenn and Hambrick 2013)</span>. This finding hinted at the process of memory consolidation – the idea that recently formed memories gradually strengthen and become more stable (often thought to occur during sleep). Later research in physiological psychology confirmed that sleep has beneficial effects on memory consolidation at the neural level (e.g. through replay of learning during slow-wave sleep), complementing the cognitive account that sleep mainly shields against interference. Overall, a key principle is that not all forgetting is due to time; much forgetting is due to interactions between memories. This has practical implications: spacing out learning (as we will see) can reduce interference between items, and ensuring periods of rest (or sleep) after intense learning can improve retention.</p>
</div>
<div id="spacing-effect-and-distributed-practice" class="section level5 hasAnchor" number="4.3.0.2.3">
<h5><span class="header-section-number">4.3.0.2.3</span> Spacing Effect and Distributed Practice<a href="a-comprehensive-theorectical-model-of-playing-by-ear.html#spacing-effect-and-distributed-practice" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>One of the most robust findings in cognitive psychology of learning is the spacing effect – the benefit of distributing study or practice sessions over time, rather than cramming in a single session. Interestingly, Ebbinghaus himself observed a precursor of the spacing effect. In one anecdotal report, Ebbinghaus noted that after “cramming” repetitions of a list in one day he could recall it, but achieving similar recall with practice spread over three days required only about half as many total repetitions <span class="citation">(Settles and Meeder 2016)</span>. In other words, spaced practice was more efficient than massed practice. Subsequent research over the past century has thoroughly documented that spaced repetition leads to better long-term retention compared to massed learning (i.e. studying the same total amount in one lump). A comprehensive review by <span class="citation">Cepeda et al. (2008)</span> examined hundreds of studies and confirmed that spacing effects are reliable across a wide range of materials and contexts. Spacing improves memory for simple verbal facts, textbook material, motor skills, and more – it appears to be a general principle of learning. The size of the benefit is often substantial. For example, one experiment found that properly spacing study sessions nearly doubled the amount of information retained after a one-year delay, compared to a massed practice condition (same total study time) <span class="citation">(Khajah, Lindsey, and Mozer 2014)</span>. This dramatic improvement occurs because spaced sessions allow some forgetting to happen between sessions, which actually forces more powerful relearning. Each time you retrieve or restudy the material after a delay, the memory trace is strengthened more than if you had restudied it immediately. In effect, spaced practice exploits the forgetting curve by scheduling reviews at the point when some forgetting has occurred but not too much – a strategy often summarized as reviewing “right before you would have forgotten”. Psychologists also describe a related finding, the lag effect, which indicates that spacing is even more effective when the intervals between study sessions gradually increase (for instance, first review after 1 day, next after 3 days, then 1 week, etc.) <span class="citation">(Settles and Meeder 2016)</span>. This helps match the schedule to the slowing pace of forgetting as retention improves. Importantly, spacing benefits are not limited to rote memorization. Studies have shown improved learning of concepts and problem-solving skills under spaced practice, and field experiments have demonstrated that spacing can boost learning in real classroom settings <span class="citation">(Cepeda et al. 2008)</span>. In one classroom study, spaced review led to higher test scores on a semester-end exam than a massed review schedule <span class="citation">(Sense et al. 2019)</span>. The universality of the spacing effect has led researchers to conclude it is a “desirable difficulty” – a learning strategy that feels more challenging (because some forgetting happens between sessions) but yields superior long-term results <span class="citation">(Bjork 1994)</span>.</p>
</div>
<div id="retrieval-practice" class="section level5 hasAnchor" number="4.3.0.2.4">
<h5><span class="header-section-number">4.3.0.2.4</span> Retrieval Practice<a href="a-comprehensive-theorectical-model-of-playing-by-ear.html#retrieval-practice" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Another principle often intertwined with spacing is the benefit of retrieval practice – actively recalling information (testing oneself) is more effective for memory than simply re-reading the material. While not the central focus of this review, it’s worth noting that much of the power of spaced repetition comes from the combination of spacing and retrieval. When learners practice retrieving information at spaced intervals, they harness two potent effects: the spacing effect and the testing effect. Research in cognitive psychology has articulated that a successful recall from memory solidifies knowledge more than passive exposure does <span class="citation">(Chukharev-Hudilainen and Klepikova 2016)</span>. Moreover, recalling an item after some delay (when it’s a bit difficult to remember) has a bigger impact on retention than recalling it immediately after study. These findings underpin modern spaced repetition techniques, which typically involve attempting to retrieve the answer (as in flashcard quizzes) rather than simply reviewing the information. In summary, the key findings from over a century of research can be distilled into a few general principles: memory retention improves with appropriately spaced reviews; forgetting is mitigated when learning is spaced and when interference is minimized; and active retrieval of knowledge strongly reinforces memory. These insights have paved the way for practical methods to enhance learning.</p>
</div>
<div id="cognitive-models-of-memory-and-optimal-scheduling" class="section level5 hasAnchor" number="4.3.0.2.5">
<h5><span class="header-section-number">4.3.0.2.5</span> Cognitive Models of Memory and Optimal Scheduling<a href="a-comprehensive-theorectical-model-of-playing-by-ear.html#cognitive-models-of-memory-and-optimal-scheduling" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Beyond empirical findings, cognitive psychologists have developed theoretical models to explain learning and forgetting, and to predict the optimal timing of review. For instance, the <em>ACT-R</em> cognitive architecture (<em>Adaptive Control of Thought—Rational</em>) includes a mathematical formula for memory decay where each exposure adds to an item’s “activation” but that activation diminishes as a power function of time. Such models can capture phenomena like the spacing effect by adjusting activation based on practice history. A more recent advancement is the development of data-driven models that personalize spacing schedules for individual learners. <span class="citation">Mozer and Lindsey (2016)</span>’s DASH model is one prominent example. “DASH” stands for Difficulty, Ability, and Study History, reflecting the key factors the model takes into account <span class="citation">(Sense et al. 2019)</span>. In a 2014 study, <span class="citation">Lindsey et al. (2014)</span> tested a personalized spacing algorithm based on a memory model in a real classroom (a semester-long Spanish course). Students used a computer-based flashcard system for vocabulary practice throughout the term. The researchers compared different scheduling algorithms and found that the personalized, model-driven spacing schedule (DASH) produced the highest end-of-semester exam scores, especially for material learned early in the course. The DASH algorithm works by estimating how well each student knows each item and how quickly that item will be forgotten, based on past study history and item difficulty. In essence, it uses a cognitive model of the forgetting curve for each item and each learner, then schedules reviews at the optimal time (just as the item is predicted to be on the verge of being forgotten). <span class="citation">Mozer and Lindsey (2016)</span> further elaborated this approach, emphasizing that integrating cognitive theory with “big data” from educational technology can greatly improve learning outcomes. They demonstrated that a theory-informed model like DASH could outperform generic one-size-fits-all spacing strategies. Simulations and experiments showed that taking into account individual differences in learning and forgetting led to more effective review schedules. For example, one student might need to see a particular vocabulary word five times with increasing intervals (1 day, 3 days, 1 week, 3 weeks, etc.), whereas another student might need fewer or more repetitions at different intervals, depending on how quickly they tend to forget that word. The model’s job is to tailor the schedule optimally. The success of DASH and similar models illustrates a general principle: optimal learning schedules should be adaptive. By using cognitive principles (like the forgetting curve and spacing effect) in a quantitative way, these models guide the timing of study to maximize retention. This line of research merges cognitive psychology with computational methods and forms the bridge to real-world applications in educational technology, such as intelligent tutoring systems and language learning apps.</p>
</div>
</div>
<div id="applications-in-language-learning" class="section level4 hasAnchor" number="4.3.0.3">
<h4><span class="header-section-number">4.3.0.3</span> Applications in Language Learning<a href="a-comprehensive-theorectical-model-of-playing-by-ear.html#applications-in-language-learning" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The principles of learning and memory described above have found practical application in many domains, but perhaps one of the most visible in everyday life is language learning. Mastering a new language requires acquiring a large amount of vocabulary and grammatical knowledge – a clear case where effective memory retention is crucial. It is no surprise that many language learning methods and apps explicitly incorporate cognitive psychology findings like spaced repetition to help users remember foreign words and phrases. Here we review how these principles are applied in language learning and assess their effectiveness from an academic standpoint.</p>
<p><strong>Spaced Repetition in Vocabulary Acquisition:</strong> Vocabulary learning has benefited greatly from spaced repetition techniques. Traditional flashcards have long been used by learners to drill word meanings; with the advent of computers and smartphones, flashcard programs can schedule reviews optimally using algorithms based on the spacing effect. One fundamental idea is that each word should be reviewed at increasing intervals – shortly after first learning it, then a bit later, then perhaps a day or two after, then a week, and so on. This schedule aligns with the psychological finding that an “expanding” review interval helps solidify memory (the lag effect) <span class="citation">(Settles and Meeder 2016)</span>. Modern spaced repetition software, such as the popular Anki (CITE) or Memrise (CITE) platforms, implement this by tracking each item’s review history and success, and dynamically adjusting when it will next appear. For example, if you easily recall a word, the system might schedule the next review farther out; if you struggle or forget, the system will show it again sooner. This approach is a direct translation of the difficulty-sensitive spacing principle discussed earlier (and indeed echoes the DASH model’s focus on item difficulty and individual performance). Academic research strongly supports the effectiveness of such approaches for language learning. In a controlled study, <span class="citation">Chukharev-Hudilainen and Klepikova (2016)</span> tested a computer-based spaced repetition tool for English vocabulary training among foreign language learners. Over the course of a semester, students who used the tool for just a few minutes each day achieved significantly better long-term retention of the vocabulary than those who did not. In fact, the spaced repetition group’s vocabulary retention after a delay was roughly three times higher than that of the control group, despite the minimal daily time investment. This three-fold improvement is striking evidence that incorporating spacing into study can greatly enhance learning efficiency. Other studies have similarly found that spaced presentation of new words yields better memory than massed presentation. Classic research by <span class="citation">Bahrick et al. (1993)</span> showed that spreading foreign language word practice sessions across intervals as long as 56 days produced superior retention over a 5-year period, compared to shorter interval practice or cramming. The benefits of spacing are thus seen not only in short laboratory tests but in longitudinal retention of language knowledge.</p>
<p><strong>Language Learning Apps and Cognitive Principles:</strong> Popular language-learning applications like <em>Duolingo</em>, <em>Babbel</em>, and <em>Memrise</em> have brought these cognitive principles to millions of users. <em>Duolingo</em>, for instance, uses a gamified approach to teach languages, but behind the scenes it employs algorithms influenced by spaced repetition and learning curve models. The <em>Duolingo</em> research team developed a statistical model called “Half-Life Regression” to predict how long a learner will remember a word or grammar concept and when they will need to review it <span class="citation">(Settles and Meeder 2016)</span>. The model’s name is inspired by the idea of a memory “half-life” – the time it takes for the probability of recalling an item to fall to 50%. By analysing the huge amount of data from its user base, Duolingo’s system can continuously adjust practice schedules for each learner. Empirical evaluation of this system showed tangible improvements: the half-life regression model was able to predict users’ recall accuracy much better (reducing prediction error by over 45% compared to simpler baselines) and, when deployed, it led to a 12% increase in user engagement with daily practice <span class="citation">(Settles and Meeder 2016)</span>. The increased engagement likely results from users experiencing a manageable level of difficulty – not too easy (which can be boring) and not too hard (which can be discouraging) – by reviewing items at the right time. From an effectiveness standpoint, more engagement means more practice and thus better learning outcomes, a contention supported by the finding that those on optimized schedules retain more and stick with the program longer. Another app, <em>Memrise</em>, uses a similar spaced repetition strategy and often incorporates mnemonic aids. <em>Babbel</em>’s curriculum explicitly staggers review sessions for previously learned vocabulary at expanding intervals to reinforce retention. Overall, these apps are effectively applied cognitive psychology tools. They implement the spacing effect, interleaved practice, and sometimes retrieval practice (since users must actively recall translations) to maximize learning per unit time.</p>
<p><strong>Effectiveness and Academic Assessments:</strong> Do these approaches actually work in practice? Research so far indicates yes – when used properly, spaced repetition-based language learning can be highly effective. Learners can achieve substantial vocabulary gains through self-guided app practice. Studies comparing app-based learning to classroom instruction have found that dedicated app users can learn a volume of material in a few months that is comparable to one or two semesters of language class instruction. For example, an early independent study on Duolingo by <span class="citation">Vesselinov and Grego (2012)</span> estimated that 34 hours of <em>Duolingo</em> practice were equivalent to a semester of college language education in terms of reading and writing outcomes (though speaking skills were not as directly taught). From a cognitive perspective, this efficiency makes sense: the apps force learners to practice frequently, recall actively, and review content right as they are about to forget it – all conditions known to solidify memory.</p>
<p>Another empirical evaluation of Babbel conducted by <span class="citation">Loewen, Isbell, and Sporn (2020)</span> found that users who engaged with the app over a 12-week period demonstrated significant gains in receptive linguistic knowledge, including vocabulary and grammar, as well as improvements in oral communicative ability in Spanish. Similarly, a 2019 study by <span class="citation">Van Deusen-Scholl, Lubrano, and Sporn (2021)</span> in collaboration with Babbel reported that learners who completed a structured set of lessons showed measurable enhancements in conversational skills, particularly in speaking and comprehension. These findings suggest that well-structured language learning apps, when used consistently, can effectively support the acquisition of core language competencies.</p>
<p>While user motivation and engagement levels vary (and dropout rates can be high), these studies indicate that learners who adhere to the recommended schedules benefit from the apps’ built-in instructional design. Importantly, such platforms typically integrate spaced repetition with a variety of pedagogical strategies, including multimedia input, contextual learning, and retrieval-based exercises. This integration can make it difficult to isolate the unique contribution of spacing to learning outcomes. Nevertheless, given the substantial body of laboratory research demonstrating the robust benefits of spaced repetition, it is reasonable to conclude that its implementation plays a major role in the observed learning gains.</p>
<p>Furthermore, language learning apps like Duolingo and Babbel now serve not only as educational tools but also as research platforms. Large-scale analyses of user data have yielded novel insights into learning processes—for instance, identifying which word types tend to be forgotten more easily, or how factors such as native language and lexical similarity influence learning trajectories. This iterative feedback loop—where cognitive science informs app design, and app usage informs cognitive science—represents a compelling example of applied psychological research in action.</p>
<p>In summary, modern language learning applications leverage foundational cognitive principles such as spaced practice, repeated retrieval, and interference management to support durable knowledge acquisition. Spaced repetition, in particular, emerges as a common and effective mechanism across successful programs. Empirical assessments confirm that these strategies improve long-term retention more reliably than massed learning or unsystematic study. By embedding these methods in engaging, user-friendly formats, educational apps have made research-backed learning strategies accessible to a broad public. Learners today benefit from sophisticated, adaptive scheduling algorithms that embody over a century of cognitive psychology research—turning the science of memory into everyday practice.</p>
</div>
<div id="conclusion-cognitive-psychological-background" class="section level3 hasAnchor" number="4.3.1">
<h3><span class="header-section-number">4.3.1</span> Conclusion: Cognitive Psychological Background<a href="a-comprehensive-theorectical-model-of-playing-by-ear.html#conclusion-cognitive-psychological-background" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Understanding the general principles of learning and memory has been a central quest in cognitive psychology, from the early experiments of Ebbinghaus to the sophisticated computational models of today. This literature review has highlighted several key concepts: the predictable shape of forgetting and learning curves, the crucial influence of factors like interference and repetition on memory retention, and the immense benefits of spaced practice for durable learning. Foundational studies provided simple but powerful lessons – memory fades quickly without reinforcement, and the context of learning (such as being asleep or awake, or spacing out practice) can dramatically change how much we retain. Building on those insights, modern researchers have developed strategies and models (e.g., the DASH model of Mozer &amp; Lindsey) to optimize learning by tailoring review schedules to the learner’s needs. These advances are not only theoretical; they have been translated into practice in educational technology. Language learning, a field heavily dependent on memorization, has been a prime beneficiary of this translational research. Techniques like spaced repetition and adaptive recall practice, grounded in cognitive theory, demonstrably improve language learners’ outcomes, making the process more efficient and effective <span class="citation">(Chukharev-Hudilainen and Klepikova 2016; Settles and Meeder 2016)</span>. In essence, cognitive psychology has revealed that learning is not just about how much time we spend or how motivated we are, but also when and how we study. Timing matters: reviewing information at strategically spaced intervals can cement knowledge in a way that marathon cramming never can. Method matters: actively recalling and engaging with the material strengthens memory more than passive exposure. These general principles apply broadly – whether one is trying to remember a list of words, understand a scientific concept, or practice a musical instrument. By applying these principles, educators and learners can significantly boost memory performance. The success of spaced repetition in both lab and real-world settings underscores a hopeful message: forgetting is natural, but it is not inevitable – with the right techniques, we can greatly extend the longevity of our memories. Looking ahead, the synergy between cognitive research and educational application is likely to grow. As we collect more data from learning apps and classroom interventions, our models of memory will become more refined, and in turn those models will power more effective learning tools. This ongoing dialogue between theory and practice was anticipated by pioneers like Ebbinghaus, who sought general laws of memory, and it continues in contemporary efforts to personalize and optimize learning. For the general population, the takeaway is clear: strategies based on cognitive psychology (like spacing your study, self-testing, and giving yourself time to forget and relearn) can make a tremendous difference in how well you learn and remember. Harnessing these principles can turn the ebbing tides of forgetting into lasting knowledge. In conclusion, the marriage of time-tested cognitive principles with modern technology offers an exciting path forward to help people learn better and remember longer – fulfilling, in a very practical way, the early psychologists’ quest to understand and improve human memory.</p>
</div>
</div>
<div id="statistical_modelling_frameworks" class="section level2 hasAnchor" number="4.4">
<h2><span class="header-section-number">4.4</span> Statistical Modelling Frameworks<a href="a-comprehensive-theorectical-model-of-playing-by-ear.html#statistical_modelling_frameworks" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="cognitive-modelling-item-feature-modelling-via-item-response-theory" class="section level3 hasAnchor" number="4.4.1">
<h3><span class="header-section-number">4.4.1</span> Cognitive Modelling Item Feature Modelling via Item Response Theory<a href="a-comprehensive-theorectical-model-of-playing-by-ear.html#cognitive-modelling-item-feature-modelling-via-item-response-theory" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Performance on an ability test can vary as a function of individual differences (i.e., some participants have a higher ability than others), but also as a function of items themselves (i.e., some items may be more difficult than others). As previously discussed, quantitative features can be computed for melodies across various domains (e.g., tonality, contour, rhythm). Such emergent features clearly rely on high-level mental representations and templates (i.e., musical knowledge) and thus should predict performance. Consequently, there can be significant variance in complexity when a melody is the item of testing, and these kind of item difficulties are important to model. We here compute melodic representations can be quantified for each melodic item across important dimensions <span class="citation">(<strong>mullensiefenFANTASTICFeatureANalysis2009?</strong>)</span>.</p>
<p>In order to formally relate structural features of melodies to the cognitive difficulty of melody processing, the main methodological approach we utilise here is <em>explanatory item-response theory</em> (<em>IRT</em>; <span class="citation">De Boeck, Cho, and Wilson (2016)</span>). In this paper, <em>IRT</em> can be considered our first level of modelling, where melodic features become predictors of the <em>opti3</em> similarity score, which we take as representing variance in both singing accuracy and melodic memory. <em>IRT</em> is useful for our enquiry since it allows the simultaneous modelling of item difficulties and individual differences together via mixed effects modelling, whilst compartmentalising the variance into fixed item effects (melodic features), random item effects (unexplained effects due to melodic items) and participant effects (effects due to individual participants’ abilities). Additionally, an <em>IRT</em> model can be the basis of creating an adaptive test, which is highly efficient and can be variable in test length, since encoding relationships between item features and performance can be used to generate or select items based on modeled difficulties <span class="citation">(for similar approaches see <strong>geldingEfficientAdaptiveTest2021?</strong>; <strong>harrisonApplyingModernPsychometric2017?</strong>; <strong>harrisonDevelopmentValidationComputerised2018?</strong>; <strong>harrisonModellingMelodicDiscrimination2016?</strong>; <strong>tsigemanJackJillAdaptive2022?</strong>)</span>. Such an adaptive test can hence be employed flexibly, with potential applications in education.</p>
<p>In this paper, our strategy to relate singing accuracy to melodic memory is to extract participant and item level scores from our <em>IRT</em> mixed effects models and use these outputs in further modelling. For instance, we use participant-level scores to represent individual differences in overall melodic memory and singing ability, and participant level indicators of singing accuracy alone (comprising e.g., single long note singing, singing accuracy, precision), to predict such outputs. This allows us to evaluate the potential extent that low-level singing abilities are responsible for the overall variance in singing performance, leaving the rest to do with variance in melodic memory, or being unexplained.</p>
<p>As suggested by previous literature in <span class="citation">(Baker 2019; <strong>dreyfusRecognitionLeitmotivesRichard2016?</strong>; <strong>harrisonModellingMelodicDiscrimination2016?</strong>; Silas and Müllensiefen 2023; Silas, Müllensiefen, and Kopiez 2023)</span>, there are several melodic features which could indicate an item’s complexity and predict playing by ear performance performance (e.g., tonality, interval contour, a melody’s frequency in occurrence).</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="backgrounds.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="theoretical-model.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": null,
    "text": null
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
