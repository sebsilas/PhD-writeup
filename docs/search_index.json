[["index.html", "Playing By Ear: A Computational Approach 1 Front Matter 1.1 Foreword 1.2 Acknowledgements 1.3 Table of Contents", " Playing By Ear: A Computational Approach Seb Silas 2025-06-08 1 Front Matter “I love science, and it pains me to think that so many are terrified of the subject or feel that choosing science means you cannot also choose compassion, or the arts, or be awed by nature. Science is not meant to cure us of mystery, but to reinvent and reinvigorate it.” — Robert Sapolsky (Sapolsky 2004) 1.1 Foreword The topic of this dissertation is “playing by ear” and whilst I generalise the results, I motivate this mainly in terms of jazz education. Jazz is an oral tradition. Cognitive psychological tool If you want to learn the tradition, transcribe the solos, go and play with new solos. Less a means of learning the tradition orally (best go straight to the source, transcribe real solos) - more about developing personal vocabulary based on your current knowledge. Exploring new spaces The computer as the “heavy-lifter” so the musician can focus more on the sounds themselves; unburdening the musician 1.2 Acknowledgements For Soraya. Sylvia, Daniel Müllensiefen, Reinhard Kopiez. Klaus Frieler, Mohamed Etteyeb Peter Harrison, Kilian Sander, Antoine Sachet, James Petticrew, Annabel Cohen 1.3 Table of Contents 1. Front Matter Foreword Acknowledgements Table of Contents 2. Introduction The Problem Domain and Approach: A Computational Approach Motivations Generalising beyond jazz and playing by ear Thesis Statement Original Contributions Research Questions Objectives and Scope Scope Front-Facing Tools Slonimsky.app Songbird Dissertation Outline 3. Foundations What is playing by ear? Learning to play by ear 4. Historical Background: Approaches to Improving Playing by Ear in Jazz Pedagogy The Aural Origins of Jazz Education Record Players as Pedagogical Tools Codification of Jazz Education The Chord-Scale Approach, Pattern Books, and Melodic Vocabulary Acquisition John Coltrane and Slonimsky’s Thesaurus: Aspirational Melodic Vocabulary Acquisition Summary Modern Technological Approaches to Ear Training Historical Development of Ear-Training Technology Commercial Ear-Training and Transcription Tools Chord/Harmonic Recognition Tools. Rhythm Training Tools. Non-Commercial and Research Initiatives Pedagogical Strategies and Practical Use Benefits and Limitations Conclusion 5. Empirical Backgrounds: Systematic Musicological and Cognitive Psychological Playing by Ear: Cognitive Foundations, Developmental Trajectories, and Practical Strategies in Musical Ear-Playing Introduction Cognitive and Neural Mechanisms Underlying Playing by Ear Developmental and Pedagogical Aspects of Playing By Ear Skill Acquisition Practical Strategies and Individual Differences in Playing By Ear Conclusion Musicological Perspectives Music and combinatorics Melody as Cognitive Psychology / Melodic represention Computed melodic features as a solution to selecting items Pattern books as item banks Music Psychology/Music Education Cognitive Psychology General Principles of Learning and Memory: A Cognitive Psychology Perspective Introduction Theoretical Background Key Findings from Cognitive Psychology Applications in Language Learning Conclusion Statistical Modelling Frameworks Item Response Theory 6. Playing By Ear: A Theoretical Model Pfordresher’s H-PAC model Pfordresher’s cognitive model of singing accuracy Baker’s computational model of melodic dictation Models of sung recall Non-musical models of serial recall WM bits Bringing it together Moving from Theoretical to Computational Inputs Sequential Construction Function Application Mathematical Representation Algorithm Flow Final Equation Strategies Learning, forgetting The Melodic Mind As Item Bank Predictions: 7. A computational ecosystem pyin: Psychologically meaningful musical item banks: itembankr Datasets WJD Berkowitz Slonimsky Other useful functions Assessing musical behaviours: musicassessr Ability tests Conclusion 8. Experiment 1: Development of a melodic similarity algorithm for short recalled melodies 9. Experiment 2: An Explanatory Model Of Playing By Ear Methods Participants and Preliminary Questionnaire Apparatus and General Procedure 10. Experiment 3: Modelling Melodic Difficulty in Playing by Ear: An Item Response Analysis of Online Performance Data 11. Experiment 4: A Longitudinal Study of Playing By Ear Skills Production perception paradigm 12. Experiment 5: A similarity-based approach to item selection Collaborative filtering Approaches to similarity for large scale corpora Melodic corpora as networks Approach #1: similarity of features as proxy for melodic similarity Approach #2: Generative similarity Approach #3: on breaking up items into ngrams, store representations/similarity. 13. Experiment 6: Model synthesis: Development of a musical DASH+SIM model 14. Additional Learning Paradigms Generative similarity identify correct vs incorrect ngrams in a recall: add incorrect ngrams to beginning of session buffer 15. Experiment 7: An Interventional Study "],["introduction.html", "2 Introduction 2.1 What Is Playing By Ear? 2.2 Learning to Play By Ear 2.3 The Problem Domain and Approach 2.4 Motivations 2.5 Thesis Statement 2.6 Original Contributions 2.7 Research Questions 2.8 Objectives and Scope 2.9 Front-Facing Tools 2.10 Dissertation Outline", " 2 Introduction “I don’t read music. I just need to hear it once and then I can play it.” — Derek Paravicini (Ockelford 2007) “You learned to speak before you could read. Why shouldn’t music be the same?” — Victor Wooten (Wooten 2008) In 1920s Ohio, a partially blind boy named Art Tatum sat at a player piano and listened to a roll of stride piano music. Without reading a single note of sheet music, and with vision so limited that he could barely see the keys, he listened - just once or twice - then placed his fingers on the keyboard and reproduced the piece note for note. Then, with a mischievous glint, he added flourishes and harmonic substitutions that surpassed what the original composer had written. This self-taught musical prodigy would go on to become one of the most revered jazz pianists of the 20th century—universally recognised for his astonishing ear, rapid improvisation, and harmonic imagination (Lester 1995). However, Tatum’s genius was not rooted in formal conservatoire training. Instead, he learned almost exclusively by ear: from radio broadcasts, piano rolls, records, and live performances. This mode of learning - aural, intuitive, improvisational - stands in stark contrast to the notation-based pedagogy that dominates Western classical music education today. Yet, it is not unique to Tatum. From 19th-century savants like Blind Tom Wiggins, who reportedly memorised and reproduced full-length concert works by ear at the age of five (Southall 2009), to contemporary musicians such as Derek Paravicini and Leslie Lemke—who, despite profound disabilities, are able to play complex pieces after a single hearing — playing by ear continues to reveal deep, and often mysterious, dimensions of musical cognition (Ockelford 2007; Treffert 2009). What does it mean to play by ear? How do musicians mentally encode and reproduce sound without the aid of notation? How can this skill be enhanced effectively and efficiently? These questions bridge disciplines - encompassing general music education, jazz pedagogy, cognitive psychology, systematic musicology, and cultural history. While accounts of prodigies and savants capture the public imagination, they also illuminate fundamental cognitive skills: auditory memory, pattern recognition, improvisational logic, and emotional sensitivity. Moreover, many “ordinary” musicians - particularly in jazz and popular music - also learn and create by ear, suggesting that this way of engaging with music is not rare, but rather underappreciated within formal discourse, despite the fact that playing by ear skills also supports other musical skills, (e.g., sight-reading) because they help create useful representations which generalise to other musical skills (G. E. McPherson, Bailey, and Sinclair 1997) and the suggestion in the pedagogical literature that the paradigm of teaching music notation first, should be reversed and instead music should be taught from “sound to sign” (temporary-citekey-1803?; Mainwaring 1951). 2.1 What Is Playing By Ear? In the playing by ear literature, there is some critical discussion as to what constitutes “playing by ear”. Priest (1989) regards that “all musical playing [should] be viewed as ‘by ear’, including when notation is involved, so that the aural basis for musicianship is maintained” (although, later, they seem to be more restrictive “all playing that takes place without notation being used at the time”). Conversely, G. E. McPherson (2005) maintains that playing by ear is “reproducing a pre-existing piece of music that was learned aurally without the aid of notation”. Musco (2010) rejects the Priest (1989) definition and instead adopts the G. E. McPherson (2005) definition because “it differentiates playing by ear from improvisation and playing from memory”. They also add some more detail to this, arguing that playing by ear does not involve something that was learned through rote. As I will detail later in my historical review of learning to play by ear in jazz music, in view of the multifaceted ways that jazz musicians learn to play by ear (including incorporating support from visual notation) - a perspective which may be missing from the formation of the previous definitions - the G. E. McPherson (2005) seems not quite adequate. Musco (2010) is correct that playing by ear should be delineated from improvisation - which is a generative process involving spontaneous composition (i.e., the generation of novel ideas). However, to delineate playing from memory from playing by ear is, I believe, a mistake. Indeed, playing by ear is playing from memory. The distinction which appears to be made by G. E. McPherson (2005) and Musco (2010) is based on the method (or modality) of learning. If music is learned/memorised with visual notation (or rote repetition), then later recalled, this is “playing from memory”; if it is learned through an exclusively auditory stimulus, then it is playing by ear. However, this definition is very limiting and creates an artificial line between visual and auditory learning, suggesting that no melody learned with visual notation, could be later recalled by ear, even though auditory feedback generally accompanies visual notation in learning. What if a musician writes down a few notes they are recalling, as a momentary aid, is this now playing from memory? Such a distinction seems unnecessary, especially as in practice, musicians may use a combination of visual and auditory methods to help learn melodies by ear - especially in the case of learning from “pattern books” as will be discussed later. Instead, I more in line with Priest (1989)’s more restrictive definition that playing by ear is “all playing that takes place without notation being used at the time”, in this dissertation, I define playing by ear as the ability to reproduce an aurally presented melody on an instrument (including the voice). I make no prescription about how the melody was learned. If someone can hear a melody and reproduce it, they are playing by ear - no matter how they learned it. I thus presume that, if you sung a classical musician a phrase from a piece that they “learned from memory” and they were able to play it back from your auditory model, this would constitute playing by ear. Necessarily, if such a person can perform from such an auditory model, they must have an auditory “by ear” representation of the melody - and that is playing by ear; even if it is accompanied by mental visual imagery, and even if that imagery involves the image of a musical stave. In reality, visual and kinaesthetic imagery forms part of representing a melody for many learners (CITE) and there must be a wide spectrum of possibilities. Trying to draw a line somewhere here seems fruitless. As Priest (1989) suggests that possible mechanism underlying playing by ear could be based on “two kinds of images of sounds - notational and aural - in varying strengths”. There may, however, be two different cognitive process underlying the ability to “play by ear”: recall and recognition. Simply recognising a pre-learned phrase and being able to reproduce it may involve a less deep auditory image than hearing a novel melody, representing it fully and reproducing it. However, both processes are both “playing by ear” and “playing from memory” and have nothing to do with the modality of learning. Thus, I here define playing by ear as simply the ability to reproduce an aurally presented melody on an instrument (including the voice). To play well by ear means to reproduce the musical properties of the heard melody accurately across relevant domains, such as pitch, interval and rhythmic structures. I note, that, precisely what these important musical structures to assess accuracy by are consist of is non-trivial, and something partly explored on this thesis in Chapter 7. I thus leave the definition relatively general at this point. To play badly by ear means to struggle to (accurately) reproduce the musical structures contained in the melody. Note that my definition separates playing by ear from “improvisation”, which is about spontaneously generating (new) ideas; not a faithful replication of that heard before. 2.2 Learning to Play By Ear I separate the general process of “playing by ear” from “learning to play by ear”, even though the two are intrinsically linked. “Playing by ear” presumes that a melody is sufficiently “pre-learned” to be recalled practically perfectly. Conversely, learning to play by ear is a separate process, whereby the melody to be recalled is not yet stable enough in memory to be reproduced faithfully. It will take specifically more than one attempt to be recalled accurately (i.e., potentially “revised” on each subsequent attempt) or may not be recallable in a given session. Thus, learning requires multiple attempts and assumes that improvements are made over attempts, in the relative short term (Silas and Müllensiefen 2023), but also over longer time periods. I believe this separation between playing by ear as an ability and a learning process, which I frame this dissertation around, helps reconcile the discussion around definitions above. Learning to play by ear could take many forms, including potential assistance from visual notation - but always involving the development of auditory representations alongside any such other support. After learning, anything heard and reproduced has been played by ear. 2.3 The Problem Domain and Approach “Data are profoundly dumb.” — Judea Pearl and Dana Mackenzie (Pearl and Mackenzie 2018) This research focuses on how musicians - particularly those learning in traditions like jazz - acquire the ability to play melodies by ear. At the heart of this ability lies a complex set of cognitive, perceptual, and motor processes that transform a heard melody into a performed one, without reliance on written notation. While musicians have long developed this skill through immersion and repetition, this project asks: can we systematise and support this learning process through adaptive, computer-assisted methods? The central problem addressed here is how to select and deliver the most useful melodic material for learners to practise at any given time. More concretely, how can we build a system that responds to individual learning needs—tracking what a musician finds easy or difficult, what they are likely to forget, and what would help them progress? Instead of relying on fixed curricula or static pattern books, this research explores the potential of computational models to make such decisions in a dynamic, data-driven way, whilst also shedding light on the cognitive processes underpinning playing by ear abilities. The proposed solution adopts a practical approach: rather than solely relying on a fixed theory of musical difficulty or melodic structure, the system is designed to learn from new data—specifically, from how real learners interact with musical material. This reflects a broader empiricist tradition in music cognition and psychology, where the goal is to observe patterns and build models based on actual behaviour. Drawing from psychometric frameworks like item response theory to model what musical features make certain melodies easier or harder to play by ear and synthesising this with models of learning trajectories inspired by broader cognitive psychology, information is leveraged to tailor learning for each user. In contrast to more abstract approaches, this project is grounded in musical practice. The models are trained not on idealised musical forms but on melodies more likely to be used by real learners—such as transcribed jazz solos or pedagogical patterns from exercise books. The system is built with a strong focus on usability and feedback, aiming to be both a research tool and a practical aid for musicians. Lastly, as the title suggests, a “computational” approach is taken to tackle this question. In general, this refers to using computational methods to solve problem. But it also has the more specific meaning in cognitive science - to describe methods and models in a sufficiently precise way that someone else could reproduce it. Ultimately, this research brings together insights from music education, music psychology, cognitive science, and computer-assisted learning. It uses these to ask both empirical and applied questions, such as: What makes a melody hard to reproduce by ear - or learn to reproduce by ear? How does the mode of learning (by sight or sound) affect retention and performance? And how can we use this knowledge to build better learning environments for contemporary musicians? 2.4 Motivations Learning to play melodies by ear is often described as an essential musicianship skill (CITE) because it touches almost every domain of music-making: improvisation, memory, listening, interpretation, and expression. Despite its importance, however, playing by ear remains poorly understood in terms of the specific processes involved, and it is still largely taught through informal or experience-based methods. This project is motivated by a desire to bring clarity, structure, and modern learning tools to a domain that has traditionally relied on tacit knowledge. At the same time, we live in a moment where computational tools have become central to how we study music. Musicologists, psychologists, and educators alike are increasingly turning to data-driven approaches to understand how musical knowledge is formed, transmitted, and used. This project aligns with that shift. It explores how methods from psychometrics and computational learning can help unpack what’s happening when someone learns to play a melody by ear—and how we might support that learning more effectively. From a cognitive science perspective, playing by ear is a fascinating - and sophisticated - domain to study. It requires the integration of auditory perception, short- and long-term memory, motor control, and pattern recognition, all operating in a feedback loop (CITE PFORDRESHER). The ability to take a heard melody, internalise its structure, and translate it into action on an instrument is not only musically rich—it’s cognitively complex. Understanding this process gives us insights into broader questions about how musicians represent, store, and retrieve musical information. From a computational standpoint, the project draws inspiration from language learning technologies that personalise the delivery of vocabulary items to match the learner’s memory and skill state. Melodies, like words, can be seen as structured sequences with varying complexity, familiarity, and usefulness. This opens the door to modelling musical learning in ways that are already well-established in educational psychology but have rarely been less often applied to music. Finally, this work is motivated by a practical and pedagogical concern: many musicians—especially those in jazz and oral traditions—struggle to find tools that support their aural learning processes which are based on robust scientific principles. Existing “ear training” apps tend to focus on recognition rather than production, and they often lack adaptivity, theoretical grounding, or openness for academic use. This project aims to fill that gap by building an open-source system that is not only informed by theory but is also flexible, responsive, and grounded in real musical materials. In short, the motivation for this research lies at the intersection of four fields: cognitive psychology, systematic musicology, computational modelling, and educational practice. It is driven by both theoretical questions about how we learn music and practical concerns about how we can learn it better. The two main problems this thesis attempts to solve is: A Computational Model of Playing By Ear Abilities: Given an aurally presented melody as input and a reproduction of that melody by a performer as output, what are the cognitive processes underlying performance? Can input characteristics about a given performer and the melody to be performed predict subsequent output accurately? This problem is one of developing a) a cognitive, computational model of playing by ear skills. Extending such a model, how do b) people learn to play by ear? Based on such knowledge, how can people learn to play by ear more effectively? Selecting melodic material for people to effectively enhance their playing by ear skills: Based on the computational model devised above, the second problem is that of selecting new melodic material to learn to improve playing by ear skills. How can we curate melodic material, select from it, and potentially generate new optimum material for learners to enhance their playing by ear skills more effectively than the use of manual methods alone. 2.4.1 Generalising Beyond Jazz and Playing By Ear Whilst this thesis is initially motivated historically and pedagogically in terms of jazz education, it is assumed that many of the outcomes will be relevant to the playing by ear of melodies in general. In other words, it assumed that, whilst there will be domain-specific findings, much of the work should shed light on the cognitive processes underlying melodic memory and learning in general. In this way, much of the approach and findings should logically extend to the realm of singing (by ear) too, which is far more ubiquitous. Since virtually everyone can sing (CITE) but far fewer people have access to an instrument beyond the voice (CITE), I will often discuss singing by ear and borrow from this literature. In general, much of the cognitive processes behind singing and playing by ear should be very similar. I thus suggest that the outcomes of this dissertation are not only relevant to jazz musicians, but can have a much larger impact, for musicians and musics of all kinds - at least in principle. In practice, model specifics may need adapting for utmost relevance to other domains. 2.5 Thesis Statement The thesis proposed in this dissertation is that computer-assisted learning systems - designed in alignment with cognitive psychology, psychometric modelling, and computational music analysis - can substantially improve the acquisition of melodic playing by ear skills in music learners. This research is grounded in the belief that automated, adaptive systems informed by statistical and psychological principles can serve as effective tools for developing musicianship skills that traditionally rely on subjective teaching and rote learning. Specifically, the present research investigates the following key questions: What psychometric and computational features of melodic stimuli best predict difficulty in playing by ear tasks? How can a computerised adaptive system deliver melodies in a way that maximally supports durable learning and personalised progression? What are the empirical effects of such a system compared to traditional, non-adaptive methods for learning melodies? To answer these questions, the research adopts a multi-part approach involving the development of new psychometrically valid tests, statistical modelling of melodic corpora, and the design and evaluation of an adaptive training system. The methodologies draw on the fields of music psychology, psychometrics and statistical modelling/machine learning and the system is developed using open-source technologies to ensure accessibility and reproducibility. This research also leverages insights from language learning and item response theory to address the cognitive dimensions of musical learning and proposes novel mechanisms for real-time assessment and personalised item delivery. 2.6 Original Contributions This dissertation makes several original contributions across the domains of music psychology, psychometrics, computational music analysis, and music education technology. Redefining Playing By Ear This dissertation redefines playing by ear to be more inclusive, in line with older, less restrictive definitions. It does so mainly in the light of a review of alternative approaches to playing by ear training that might have been left out. Development of an Automated Playing by Ear Test A key contribution is the creation of a fully automated, computer-scored playing by ear ability test. Unlike previous assessments, this test allows for real-time behavioural scoring, enabling scalable and objective evaluation of melodic ear-to-hand coordination. This test lays the empirical foundation for later adaptive training applications and contributes a novel psychometric tool for music research and education. Psychometric Modelling of Melodic Item Difficulty Through item response theory and explanatory modelling, this research identifies and quantifies the features (e.g., tonality, melodic contour, interval structure) that make melodic items more or less difficult for learners. These findings enhance theoretical understanding of melodic cognition and provide empirical guidelines for designing effective learning materials. Creation of an Adaptive Training System for Melodic Learning The dissertation proposes and implements an adaptive computer-assisted learning (CAL) system that dynamically selects and delivers melodic stimuli based on real-time learner performance. By incorporating spaced repetition and similarity-based item delivery algorithms, the system optimises both the efficiency and longevity of learning. Digitisation and Optimisation of Melodic Corpora This research includes the development of software tools (e.g., itembankr, musicassessr) that process and structure melodic corpora (e.g., jazz transcriptions, pedagogical pattern books) into statistically characterisable item banks. This facilitates both learning and research, enabling a data-driven approach to musical skill acquisition. Empirical Evaluation through Controlled Interventions The final component of the project involves a controlled intervention study that compares the adaptive CAL system against traditional learning methods. The study evaluates long-term retention, learning efficiency, and user satisfaction, offering concrete evidence for the effectiveness of the proposed system. Together, these contributions represent a significant advancement in how musical skills—particularly those related to improvisation and aural learning—can be taught and understood through technology. The research also delivers openly available software and data, providing a platform for future studies and educational applications. 2.7 Research Questions At its core, this dissertation seeks to answer the primary question: PRQ1: How can one effectively learn to play by ear (PBE)? This is explored through four intersecting lenses—psychological, musicological, computational, and pedagogical—each of which shapes how the skill of playing by ear can be understood, modelled, and improved. To address this overarching problem, several more specific research questions are posed: SRQ1: What makes playing by ear difficult? (Exploring psychological and music-theoretical factors such as memory, melodic complexity, and contour.) SRQ2: Can a computer application that dynamically selects practice material help learners improve faster? (Assessing the role of computational adaptivity in musical skill acquisition.) SRQ3: What needs to be taken into account by such a model of adaptive learning? (Investigating features like difficulty, adaptivity, spacing, and history—the DASH model—as key inputs.) SRQ4: Does the mode of presentation (visual notation vs. auditory listening) influence how well a melody is learned? (Examining modality effects on encoding, retention, and recall.) SRQ5: Does singing a melody before playing it aid the learning process? (Exploring embodied cognition and internal audiation as pedagogical tools.) SRQ6: To what extent is PBE driven by absolute pitch abilities versus relative pitch strategies? SRQ7: What is the relationship between general cognitive capacities—like working memory—and PBE, and how do they interact with musical training? In addition, the project explores the connection between sight-reading and PBE skills, asking whether the two are complementary, independent, or perhaps grounded in shared underlying mechanisms. A second overarching question guides the later stages of the project: PRQ2: Can an intervention—specifically, an adaptive, computational learning system—lead to measurable improvements in PBE performance? This question shifts from theory to application, testing whether a system built on the insights above actually works in practice. The broader aim is not only to build such a system but also to evaluate its effectiveness in real learning contexts through controlled experimental studies. Finally, the project engages with what might be called the “Coltrane-Slonimsky Problem”: Given a vast landscape of melodic material, how can one select items that are both suitably challenging and musically engaging for the learner? Solving this requires balancing psychological difficulty with artistic relevance—a challenge that lies at the intersection of computation and musicianship. 2.8 Objectives and Scope In response to the research questions above, the dissertation pursues the following concrete objectives: To empirically examine what makes melodies difficult to learn by ear, using item response theory and performance data to model melodic difficulty based on psychological and structural features. To develop and validate a computerised PBE ability test, capable of recording, scoring, and tracking melodic performance in real time, forming the foundation for adaptive learning. To design and implement an adaptive learning system—the Songbird platform—that delivers personalised melodic training based on individual user profiles, learning history, and cognitive principles. To digitise, analyse, and optimise melodic corpora from both real jazz transcriptions and pedagogical pattern books—converting them into structured item banks for learning (slonimsky.app). To investigate pedagogical strategies, including the impact of presentation modality (visual vs. auditory) and the use of singing as a preparatory step in melodic reproduction. To run an interventional study evaluating the system’s effectiveness, measuring whether the adaptive system leads to improved learning outcomes compared to traditional or non-adaptive methods. To make all tools and datasets freely available as open-source software, supporting further research and application in music cognition, education, and human-computer interaction. 2.8.1 Scope The scope of this research is intentionally delimited in several ways: Musical focus: The primary context is jazz and related genres where playing by ear is central. However, the underlying cognitive and computational models may generalise to other traditions. Task type: The project focuses specifically on melodic ear-to-hand coordination—reproducing heard melodies on an instrument without notation—rather than broader definitions of improvisation or harmonic ear training. User level: While designed with advanced students and self-directed learners in mind, the systems developed may also support novices or be used in classroom settings as a pedagogical supplement. Musical representation: Melodies are treated symbolically as discrete event sequences (pitches, intervals), allowing for clean computational modelling. Audio processing is not the focus, though real-world audio is considered in some experiments. Methodological scope: The project blends controlled experiments, computational modelling, and tool development. It does not aim to exhaustively model all aspects of musical improvisation or ear training but rather to provide a focused and rigorous foundation for future research. 2.9 Front-Facing Tools 2.9.1 Slonimsky.app Slonimsky.app is a digital interface and training tool aimed at advanced learners, particularly within jazz and improvisation-rich traditions. Named in homage to Nicolas Slonimsky’s Thesaurus of Scales and Melodic Patterns, this application converts melodic content from jazz solos, pattern books, and pedagogical sources into a searchable, adaptive item bank. The goal is to solve the “Coltrane-Slonimsky Problem”: given a wide field of musical possibilities, how can we find the next melody that’s optimally challenging and musically rewarding? Slonimsky.app incorporates models of difficulty and melodic similarity, allowing learners to explore and practice material personalised to their evolving skill level. It supports improvisers and ear-trained musicians in expanding their vocabulary in a structured yet musically sensitive way. 2.9.2 Songbird Songbird (songbird.training) is designed as a psychometrically informed tool to support the development of singing and melodic memory in younger learners and educational settings. It provides a streamlined, browser-based interface that presents melodic stimuli, records vocal reproductions, and assesses performance in real time. Built to function both as a research platform and educational tool, Songbird tracks individual progress and adapts the difficulty of stimuli over time. It also includes pedagogical modules that integrate singing as a foundational step in melodic learning—supporting the hypothesis that vocal practice aids both memory and instrumental imitation. It is particularly well-suited for use in schools, choirs, and youth music programmes. 2.10 Dissertation Outline First, I will motivate my discussion of learning to play by ear with some historical context around the traditional methods that have been used, particularly with in the context of learning to play jazz music in the 20th and then 21st century. Then, I will largely leave this historical context behind and motivate the question from systematic musicological and cognitive psychological points of view, where the remainder of this manuscript will largely remain. As discussed, there are two (related) principal problems that this thesis aims to tackle. First, since, as of yet, there exists no comprehensive computational model of playing by ear skills (in terms of both acquisition and utilisation), I intend to first propose a theoretical model based on a synthesis of related literature. Once described, I seek to investigate and validate parts of this proposed model in light of empirical data. The second principal problem is to, given such a computational model, design a computer application which helps playing by ear learners more efficiently improve their skills. This part of the thesis has a practical deliverable component that is intended to be used in real-life learning situations, whilst being situated in a scientific framework - both theoretical and statistical. Such an application would be proposed in an open source framework that can be extended by future developers to add new features and improve existing components of the application. The main problem the application intends to solve is one of item selection i.e., finding “optimum” playing by ear items to learn for a given learner, at a given timepoint. "],["backgrounds.html", "3 Backgrounds 3.1 Historical Background: Approaches to Improving Playing by Ear in Jazz Music 3.2 Modern Technological Approaches to Ear Training 3.3 Playing by Ear Research: A Literature Review 3.4 Interim Summary 3.5 Pfordresher’s Cognitive Model of Singing Accuracy and H-PAC Models 3.6 Baker’s Computational Model of Melodic Dictation 3.7 Models of Sung Recall", " 3 Backgrounds TODO: Webster and Greg Surber dissertations should appear here! 3.1 Historical Background: Approaches to Improving Playing by Ear in Jazz Music Jazz music began as an oral tradition where skills were developed through listening and imitation rather than formal instruction. The development of aural skills to support improvisation (i.e., playing by ear”) — has been a central feature of learning jazz music throughout the 20th and 21st centuries. In this section, I describe the historical evolution of approaches to developing aural skills in jazz music. I survey how record players, transcription books, and pattern books have supported the ear training process in jazz music and the evolving interplay between informal, aural traditions and formalised, institutional methods. This suggests a gradual shift from purely aural learning to increasingly systematised approaches, reflecting broader trends in the institutionalisation of jazz education. Despite this evolution, the fundamental importance of developing acute listening skills remains central to effective jazz pedagogy and discourse. 3.1.1 The Aural Origins of Jazz Education The early history of jazz was characterised by learning that occurred primarily through aural means involving “hours of listening, transcribing, and participating in jam sessions” (Herzig 2019) before formal educational approaches emerged. In this pre-institutional era, the transmission of jazz knowledge occurred organically within communities of practice, with experienced musicians serving as mentors to aspiring players. This master-apprentice relationship formed the foundation of early jazz pedagogy, where direct demonstration and immediate repetition were the primary teaching methods. The absence of formal jazz education in the early period necessitated the aural approach, as musicians needed to develop their skills through direct engagement with the music itself. While some things were written down, such as Downbeat magasine transcriptions in the 1940’s (Koger et al. 1985), the vast majority of what people learned and played was done solely “by ear”. This established a precedent that would continue to influence jazz pedagogy, even as more formalised approaches developed. The emphasis on learning by ear ensured that early jazz musicians developed strong aural skills as a natural consequence of their training process. 3.1.2 Record Players as Pedagogical Tools The development of recording technology dramatically transformed jazz education by making the music of master performers accessible to aspiring musicians regardless of geographic location. Recordings became essential educational resources, allowing students to study the repertoire and improvisations of established artists (Butterfield 2002; Re 2004). This technological advancement democratised access to jazz education, extending learning opportunities beyond those who could physically attend performances or study with established musicians. Technologies such as variable-speed turntables and digital audio software enabled closer analysis, allowing students to slow down recordings for detailed ear training (Re 2004). Listening became active engagement, with learners looping short segments, replicating phrases, and attempting to match phrasing and articulation. The ability to repeatedly listen to recorded performances proved particularly valuable, especially since records could be “endlessly repeated” (Surber 2009). This repetition allowed students to internalise not only melodies and chord progressions but also nuances of style, tone, and expression that are difficult to communicate through written notation. The practice of slowing down recordings or lifting the needle to replay difficult passages became standard pedagogical techniques that allowed for deep analysis of complex musical passages. Berliner (1994) emphasises that imitation from recordings helped players build a mental library of harmonic and melodic ideas, forming a foundation for improvisation. The phonograph (and later recording techologies e.g., CDs, MP3s etc.) thus become an indispensable tool for jazz learners. Listening to records enabled musicians to internalise jazz language and style. Even as jazz moved into universities, listening retained primacy. As Wilf (2012) describes, jazz education features “rituals” that centre around repeated listening to and playing along with canonical recordings. These practices continue to uphold the oral traditions at the heart of jazz pedagogy. 3.1.3 Codification of Jazz Education The transition from purely aural learning to more systematised approaches began in the mid-20th century. Herzig (2019) identifies the 1930s as a pivotal period, with early attempts to codify jazz instruction including Beihoff (1934)’s “Modern Arranging and Orchestration” and Lee Bowden’s training program for Afro-American Service Musicians at the Great Lakes Naval Base (1942-45). These initial efforts to systematise jazz education represented the beginning of a shift toward more formalised and codified instructional approaches, though they remained relatively limited in scope. However, the rise of printed transcription books - like the Charlie Parker Omnibook in the 1970’s (Parker 2009) - introduced shifts in practice. While these books provided accessible models for study, Witmer and Robbins (1988) argued that many early transcription collections failed to support active listening, with students relying more on notation rather than using their ears. Generally, transcription is thought most effective when used in tandem with active listening: students should first attempt to transcribe by ear and then use notated versions for verification or analysis (Berliner 1994; Re 2004) 3.1.4 The Chord-Scale Approach, Pattern Books, and Melodic Vocabulary Acquisition A watershed moment in jazz education came in the late 1960s, with what Herzig (2019) terms “the ABCs of jazz” - referring to the influential work of Jamey Aebersold, David Baker, and Jerry Coker. Their development of comprehensive instructional materials, including Baker (1969)’s “Jazz Improvisation: a Comprehensive Method for all Players” and Aebersold’s play-along recordings, established a framework for jazz education that continues to influence pedagogical approaches today. These materials helped standardise jazz instruction and made it more accessible to students without direct access to master performers, contributing to what Prouty (2005) describes pejoratively as the “wholesale growth” of jazz education during the 1960s and 70s. The chord-scale approach pioneered by Baker, Aebersold, and Coker became a dominant paradigm in jazz education. These influential educators “focus on scales and modes to examine improvisation” (Spice 2010). This approach provided students with systematic frameworks for navigating harmonic structures, offering concrete tools for developing improvisational facility. The emphasis on scales and patterns created a more accessible entry point for students transitioning from classical training to jazz improvisation. Aebersold’s own description of his pedagogical evolution reveals the gradual systematisation of his approach: “I published my first jazz play-a-long in 1967 and the [accompanying] booklet included concert [key] chords for each track. Subsequent printings added transposed chord symbols [for Bb and Eb instruments] and, eventually, I added the needed transposed scales and chords for each track” (Herzig 2019). This approach represented a compromise between purely aural learning and more visually-oriented instructional methods, coupling “the eye with the ear” in ways that provided students with multiple pathways for internalising jazz vocabulary. The development of pattern books and play-along recordings created standardised resources that facilitated the institutional teaching of jazz improvisation. Pattern books, such as Coker’s Patterns for Jazz and Baker’s Improvisational Patterns, systematised jazz vocabulary through short, transposable licks and exercises. While Witmer and Robbins (1988) recognises their role in helping students internalise chord-scale relationships and idiomatic phrases, these tools also sparked criticism. Benward and Wildman (1984) caution that over-reliance on such methods can yield formulaic improvisation disconnected from the spontaneous, reactive nature of jazz. Wilf (2012) views pattern books as part of a broader institutional shift from an oral tradition to a codified curriculum. Proponents of such methods like Coker (1964) emphasised that such resources should be integrated with listening and improvisation, rather than standing in as a replacement. In this way, practising melodic patterns can be used as an ear-training aid by helping to reinforce melodic shapes and harmonic contexts, provided that students also engage with recordings and creative application. It is for this reason that many pattern books are also presented with isorhythmic melodies; as noted in the introduction to Coker et al. (1999): Most of the patterns contained herein are presented in eighth notes (the rhythmic level of most jazz improvisation), in a continuing fashion, without rhythmic variation, and without rhythmic phrase-endings. This was an arbitrary approach, so as not to dictate what the rhythms should be. nor to restrict them to a single rhythmic approach. When the practiced patterns are applied to an improvisation. it is expected that the rhythms would be loosened, so that the idea takes on a more lyrical, natural, and less mechanical feeling. 3.1.5 John Coltrane and Slonimsky’s Thesaurus: Aspirational Melodic Vocabulary Acquisition John Coltrane’s approach to improvisation, characterised by systematic exploration of harmonic possibilities, had a profound impact on jazz pedagogy. Musicological research examining Coltrane’s melodic vocabulary suggests his methodological approach to developing improvisational frameworks, including his systematic exploration of harmonic structures through patterns and exercises (e.g., Bertholf 2014; Martin 2012; O’Gallagher 2020). Coltrane’s disciplined practice methods and innovative harmonic concepts established new paradigms for jazz improvisation that would later be incorporated into educational approaches, showing how his influence extended beyond his specific musical innovations to encompass his overall approach to musical development. Coltrane’s systematic exploration of musical possibilities demonstrated the value of structured practice and theoretical understanding alongside intuitive improvisation. Scholars have analysed Coltrane’s use of pitch-class sets and other patterns (O’Gallagher 2020), noting the probable influence of Slonimsky’s Thesaurus of Scales and Melodic Patterns [referred to as Slonimsky’s Thesaurus, for short; Slonimsky (1947)] by the Russian-American composer Nicolas Slonimsky who was “obsessed with twelve-tone rows” (Feisst 2011; Slonimsky 1988). The Thesaurus was a seminal book in music theory. As suggested by its title, the book is an exhaustive collection of melodies which Slonimsky laboriously generated manually and systematically. The purpose of such a volume was to provide a repository of melodies which breaks free of traditional tonal harmonic principles. In other words, it was an attempt to systematically open up possibilities in the Western music system which had not yet been explored (for later similar approaches see e.g., Johnson (2014)). Slonimsky’s undertaking can be viewed as a way of codifying and unlocking the potential of melodic possibilities through systematisation. It highlights that by using algorithimic thinking, new possibilities can be unlocked. One basic principle behind Slonimsky’s systematisation was to divide one or more octaves of the twelve-tone system into symmetrical intervals and from there systematically interpolate different numbers of notes between the intervals created by the divisions. Additionally, he used other formulae to generate melodic patterns which had not yet been yielded by his basic scheme. Of The Thesaurus, the Second Viennese School composer Arnold Schoenberg wrote to Slonimsky, “you might [have] in all probability organized every possible succession of tones” and referred to his work as “an admirable feast of mental gymnastics” (from the blurb of the book). Whilst the former is almost certainly not true (unless constrained to a length, the number of possible melodies are infinite), the latter is undoubtedly. However, such a task would now be solved at ease by a script on a modern computer, partly motivating this research project. The Thesaurus thus represents a significant intersection between theoretical systems and jazz improvisation. Although originally conceived as a classical resource, the text became influential among jazz musicians beyong Coltrane (including Frank Zappa, for instance; Slonimsky (1988)) seeking to expand their improvisational vocabulary by enabling a systematic approaches to exploring melodic possibilities within the twelve-tone chromatic scale. The application of these patterns to jazz contexts demonstrates how theoretical systems can be adapted to serve improvisational purposes, expanding the vocabulary available to jazz improvisers while maintaining connections to the oral tradition. Coltrane exemplifies this duality of pushing the boundaries; whilst exploring new, avant-grade melodic language, he continued to play jazz standards live at concerts even late into his career. Similar “boundary pushing” melodic vocabularies in the jazz literature are Yuseff Lateef’s Repository of Scales and Melodic Patterns (Lateef 2015) and works by Masaya Yamaguchi such as The Complete Thesaurus of Musical Scales (Yamaguchi 2006) and Lexicon of Geometric Patterns for Jazz Improvisation (Yamaguchi 2011). Note that, Coltrane’s use of Slonimsky’s Thesaurus represents a desire to move from simply “learning the oral jazz tradition” to developing novel melodic ideas. This is an important distinction to be made and represent two kinds of approaches to acquiring melodic representations for improvisational jazz music. The first, is about “learning the tradition” - transcribing important historical elements of jazz language; specific jazz licks etc. Such language acquisition is more pedagogical, using “tried-and-tested” patterns which helps play within idiomatic jazz frameworks (e.g., particular chord changes/jazz standards). I term the latter approach about acquiring novel melodic vocabularies, “aspirational melodic vocabularies” representing that the intention behind their acquisition is about pushing creative boundaries and finding new and exciting melodic combinations. This melodic space is virtually infinite, but exciting to explore for improvisers looking to develop a personal melodic style. There are thus two potential use cases when thinking about the development of a training program. The first for developing idiomatic jazz language will be of interest to beginners and experts alike, but the second, aspirational melodic vocabulary acquisition will generally only be of interest to experts. 3.1.6 Historical Background: Conclusion Record players, transcription books, and pattern books have each contributed to jazz ear training across the 20th and 21st centuries. Sociocultural/historical perspectives affirm that while written and technological resources are valuable, they should be embedded in a pedagogy that privileges listening, in accordance with the “jazz tradition”. The most enduring jazz learning strategies would suggest that tools are integrated into a broader aural framework, preserving the tradition of learning music by ear, even in the context of modern jazz education codification. The historical development of approaches to improving playing by ear in jazz pedagogy reveals a gradual evolution from purely aural learning to increasingly systematised methods. As the revered saxophonist Liebman argues in his discussion of transcription practices, this process involves “a three part learning process: body, mind and spirit-in that order” (“The Complete Transcription Process David Liebman” 2006). Whether we disgard the mysticism or not, this represents a holistic view often shared in jazz musician circles which recognises that effective ear training integrates physical, intellectual, and intuitive aspects of musical development, reflecting the multifaceted nature of jazz improvisation itself. The continued evolution of ear training approaches in jazz education reflects broader trends in the field, balancing respect for traditional aural learning with recognition of the benefits offered by more systematised methods and technological tools. This integration of approaches provides contemporary students with multiple pathways for developing the aural skills essential to effective jazz performance. As jazz education continues to evolve, maintaining this balance between aural tradition and systematised instruction remains a central challenge and opportunity for jazz pedagogy. The adoption of melodic thesaruses, such as the exhaustive algorithmic work of Slonimsky’s illustrates how jazz education has drawn from diverse sources beyond the jazz tradition itself. This connection between Coltrane’s innovative improvisational approach and Slonimsky’s theoretical work illustrates the emerging synthesis between systematised approaches and creative expression in jazz education. 3.2 Modern Technological Approaches to Ear Training Somewhat narratively separate from the specific jazz pedagogical issues discussed in the previous section, but which is nonetheless relevant to any learner in the 21st centry - including those enlisting in jazz studies - is a discussion of the development of computerised ear training technology, which be discussed below. 3.2.1 Historical Development of Ear-Training Technology In formal music education, “dictation” (listening to music and writing it down) has been a staple of ear training (Munive Benites, Lalitte, and Eyharabide 2023; Baker 2019). The first computer-assisted ear training tools appeared in the late 1960s and by the 1970s many university music departments had begun using software for interval, melody, chord and rhythm training (Munive Benites, Lalitte, and Eyharabide 2023; Peters 1992; Stevens 1991). These early systems predated mainstream personal computers and were often limited in interface and scope, but they signaled that technology could supplement one-on-one tutoring. During the 1980s–90s, as home computers became common, commercial ear-training packages emerged. For instance, desktop programs offered exercises in interval recognition or chord dictation with synthetic examples. Software like EarMaster (first released in the 1990s; “EarMaster - The App for Ear Training, Sight-Singing, and Rhythm Training” (2025)) began to compile thousands of drills in scales, chords and rhythms. Meanwhile, audio-editing tools introduced the ability to manipulate real recordings: by the 1990s and early 2000’s, software like Transcribe(“Transcribe! - Software to Help Transcribe Recorded Music” 2021) and Amazing Slow Downer (Roni Music 2025)allowed users to slow down a recording or loop segments without altering pitch, greatly aiding transcription practice. In the 2000s, internet and mobile devices further expanded possibilities with web-based quizzes (e.g. at musictheory.net), smartphone apps (many interval- and chord-drill apps), and online communities for sharing transcriptions all appeared. A recent survey notes that ear-training apps and digital media are now recognized as important in music education, especially after the COVID-19 pandemic accelerated remote learning (Munive Benites, Lalitte, and Eyharabide 2023; Biasutti, Antonini Philippe, and Schiavio 2022). More recently platforms, particularly YouTube, have become critical resources for jazz ear training. Thousands of free and paid video lessons offer guided transcription walkthroughs, call-and-response singing drills, harmonic analysis, and interval recognition in context. Creators like Adam Neely, Rick Beato, Aimee Nolte, and Jens Larsen provide high-quality instructional content often supplemented by downloadable PDFs containing annotated solos, theoretical explanations, and practice routines. These materials function as hybrid tools: students watch demonstrations, listen to examples repeatedly, and then practice using guided notation. While not interactive like apps, the structured nature of these lessons makes them accessible and effective for visual and auditory learners. Additionally, PDF packages often include backing tracks or isolated stems, making them compatible with tools like Transcribe! or iReal Pro for further ear training integration. A notable recent example is JazzLessonVideos.com, co-founded by saxophonist Chad Lefkowitz-Brown (Chad LB) and educator Nathan Graybeal. Their platform offers a comprehensive suite of ear training resources tailored for jazz musicians. Their offerings include the Jazz Ear Training Video Course by Dr. Joe Gilman, comprising 23 videos with over 90 minutes of instruction. This course provides step-by-step guidance on identifying chord extensions, comparing chord types, and recognizing individual tones within chords, supplemented by interactive listening challenges. Additionally, their Ear Training Method Book focuses on essential skills such as interval and pitch identification, melodic dictation, chord identification, and chord progression recognition, complete with audio exercises and fill-in worksheets. These resources are designed to enhance aural skills through structured, self-paced learning, making them valuable tools for jazz students seeking to improve their improvisational abilities. The platform’s approach emphasises the integration of video instruction with downloadable PDF materials, allowing students to engage with content visually, aurally, and kinesthetically. For instance, their “Melodic Cells” PDF package features 64 exercises focusing on non-diatonic, diatonic, and compound melodic cells, providing a structured method for developing improvisational vocabulary. Such resources are designed to be used in conjunction with backing tracks and other practice tools, facilitating a comprehensive ear training experience that bridges theoretical knowledge and practical application. . ### Commercial Ear-Training and Transcription Tools There are several main areas of computer assisted tools for helping improve playing by ear and related skills: transcription software, interval and melody training apps, chord/harmony recognition tools and rhythm training tools, which will be summarised below. 3.2.1.1 Transcription software Jazz students often begin by transcribing solos and rhythms by ear. In contemporary times, specialised programs can help facilitate this process. Transcribe! (“Transcribe! - Software to Help Transcribe Recorded Music” 2021) and Amazing Slow Downer (Roni Music 2025) are two leading commercial tools that let a user loop passages, change tempo, and even shift pitch in real time. This lets learners gradually discern fast or complex lines at a slower pace without distortion. These tools also typically display a spectrogram or note analysis, which can guide accurate pitch identification. Other options include Audacity (2025a) (free audio editor with tempo/pitch controls) and Sonic Visualiser (Centre for Digital Music 2025) (research-oriented spectrogram display). Recently, AI-driven transcription has emerged: for example, plugins like Melodyne (2025b) can extract melody lines into MIDI, and open-source tools like Spotify’s Basic Pitch (Spotify 2025) (using machine learning) can convert a recording to notes. Such tools can automatically detect pitches and even chords in monophonic lines, though they struggle with dense jazz ensembles. Nonetheless, they offer an assistive first draft for transcribing complex solos or harmonic progressions. Interval and Melody Training Apps. Many apps and websites are designed as drill-and-practice tools for intervals and melodies. Well-known examples include EarMaster (“EarMaster - The App for Ear Training, Sight-Singing, and Rhythm Training” 2025), Tenuto (by musictheory.net), Perfect Ear (Crazy Ootka Software AB 2025), and TonedEar (TonedEar 2025). These let a student train on identifying intervals, scales, and short melodies by ear. For instance, EarMaster (“EarMaster - The App for Ear Training, Sight-Singing, and Rhythm Training” 2025)offers thousands of exercises in intervals, chord types, cadences, and melodic dictation, while Tenuto’s exercises let one tap out or sing back intervals or scales on a keyboard interface. Functional Ear Trainer (iOS/Android) uses the concept of solfège by establishing a tonal center and having the student name scale-degrees or harmonic functions of incoming notes. Such apps typically provide immediate feedback and can adjust difficulty, which is a strength over traditional one-on-one drills (whose pace is fixed by the teacher). A Chinese study found that structured use of EarMaster significantly improved university students’ pitch, rhythm and timbre recognition compared to traditional instruction (Lingjie 2025). 3.2.1.2 Chord/Harmonic Recognition Tools. Identifying chords and progressions by ear is crucial in jazz. Some ear-training apps include chord identification modules: for example, EarMaster (“EarMaster - The App for Ear Training, Sight-Singing, and Rhythm Training” 2025) can play triads or seventh-chords of different qualities (major, minor, dominant, etc.) for recognition practice. Tenuto (musictheory.net 2025) and Perfect Ear (Crazy Ootka Software AB 2025) also have chord quizzes. Meanwhile, digital accompaniments like iReal Pro (2025e) indirectly train harmonic ear skills. iReal Pro is a jazz “fake book” app that plays back chord charts in realistic backing-band styles. Students use it to loop ii–V–I progressions, turnarounds, or full standards at varying tempos, and practice improvising over them. Although iReal Pro doesn’t explicitly quiz the ear, it immerses students in hearing chord changes in context. (Notably, iReal Pro is also used pedagogically by many teachers – e.g. forums note that educators share custom chord exercises on its forum irealpro.com.) Other software like Band-in-a-Box (2025d) or Chordbot (AB 2025) generates full-band or looped chord sequences. Chord-detection services (e.g. Chordify (B. V. 2025)) even attempt to automatically transcribe chords from recordings, though their accuracy can falter on jazz’s complex voicings. In summary, these tools help jazz students internalise harmonic progressions, but most pedagogical testing still relies on human evaluation or structured quizzes. 3.2.1.3 Rhythm Training Tools. Jazz rhythm (swing feel, syncopation) can be elusive. Apps such as Rhythm Trainer, Rhythm Trainer Complete, or online rhythm dictation games allow practice identifying and tapping back rhythmic patterns. EarMaster (“EarMaster - The App for Ear Training, Sight-Singing, and Rhythm Training” 2025) includes clap-back and error-detection rhythm exercises. Some drum-focused VR games (e.g. the Drumhead (VR 2025) VR app) present rhythms visually. These tools support the move toward an “aural skills” approach even for rhythm. However, authentic jazz swing feel and poly-rhythms are hard to simulate outside an ensemble. Educators often still use live or recorded jazz drum examples (e.g. using GrooveScribe (TheDrumNinja and Firth 2025) to slow or isolate drum grooves) and encourage students to mimic them by ear. Nonetheless, any metronome with accent settings or rhythm game adds value for fundamental training. 3.2.2 Non-Commercial and Research Initiatives Beyond commercial products, there are academic and open initiatives for ear training. Music education researchers have explored computer-based ear training since the 1970s, and some projects still surface: e.g. Sibelius Ear Training Exercises (built-in to notation software) or ToneGym (ToneGym 2025) (a freemium web app with interactive ear games) have some educational followings. A particularly novel project is Virtual Reality ear training. For example, Berklee College of Music has developed a free VR Ear Trainer game: players enter a virtual room and tap notes or rhythms to match aural prompts, using immersive spatial audio (Smith-Muller 2024). In research, Bournemouth University created a VR interval-recognition system and found users enjoyed it (Fletcher, Hulusic, and Amelidis 2019). These VR approaches align with modern pedagogical emphases on engagement and multisensory learning. Additionally, open-source AI tools like Spotify’s Basic Pitch (audio-to-MIDI conversion) (Spotify 2025) represent an emerging trend: while not classroom tools per se, they hint at future “smart” apps that can listen to a student’s improvisation and provide instant feedback on pitch and harmony. In terms of research on effectiveness, studies confirm that technology can boost ear-training. The Chinese Earmaster study (Lingjie 2025) is one example (not jazz-specific but broadly applicable), showing structured software practice outperformed traditional drills in improving aural skills. On the other hand, some surveys report that teachers are cautious: French music educators noted a wealth of apps exists but found few that fit their curricula and quality standards (Munive Benites, Lalitte, and Eyharabide 2023). In practice, most instructors integrate tech pragmatically (e.g. assigning app exercises for homework, using iReal Pro in lab hours) rather than abandoning traditional exercises outright. 3.2.3 Pedagogical Strategies and Practical Use Modern tools offer flexible practice but must be aligned with pedagogical goals. For transcription work, a typical strategy is: (1) listen to a short phrase repeatedly; (2) use software to slow it (e.g. to 60–80% speed) without changing pitch; (3) loop and isolate segments; (4) gradually raise speed as confidence grows. Teachers might recommend Transcribe! (“Transcribe! - Software to Help Transcribe Recorded Music” 2021) or Amazing Slow Downer (Roni Music 2025) for this, noting they allow marking sections and adjusting playback easily. Students also often use generic audio editors or DAWs (Logic, Ableton, Audacity) to visually align waveforms or annotate phrases. For interval recognition, instructors may mix app drills with in-class ear tests: e.g. EarMaster sessions can reinforce what students practiced privately on Tenuto (musictheory.net 2025). Some educators advocate functional ear training (using scale degrees) especially for jazz, since jazz harmony is often thought of functionally; apps like Functional Ear Trainer (Benbassat 2025) implement this method. Others prefer interval drills, training students to name or match intervals regardless of key, arguing it transfers to any style. In practice, many adopt a hybrid: using apps for technical skill-building, then embedding those skills in jazz context via transcription and improvisation. Chord and harmonic dictation can be taught by first drilling triad types (through apps or teacher-led quizzing) and gradually moving to seventh chords and modes. Here technology’s pros/cons are clear: an EarMaster chord drill gives clear examples of an isolated chord tone, but hearing chords in real music often involves piano voicings or comping instruments. Some software (e.g. Band-in-a-Box) can generate controlled chord voicings for drill, but a teacher must ensure the student generalizes to actual recordings. Tools like iReal Pro support this by letting students choose different “rhythmic feels” (swing, bossa, etc.) for a given progression, so they hear the chords in musical context. In classes, teachers might assign each student to program a progression into iReal Pro (e.g. a standard tune’s changes) and then transpose and improvise over it. This contrasts with traditional jazz pedagogy, where playing “head a tunes and backings” was often live or from fake books; iReal Pro effectively democratizes a live band rehearsal. Rhythm and groove training via apps must be complemented by ensemble playing. An app can tap or clap back polyrhythms or swing patterns to a click, but internalizing groove usually involves playing with others. Many jazz educators therefore use tech to drill fundamentals (time feel, comping rhythms) but still emphasize jam sessions or drummer-led practice for true syncopation skills. Some programs (e.g. Melodic Perception interactive software) teach reading swing notations, but often jazz theory recommends learning rhythms aurally (listen-and-repeat) above all. Overall, modern ear-training tools align with traditional jazz pedagogy in that they emphasize listening and imitation, but they diverge in method. Traditional methods rely on a teacher or bandleader, whereas apps provide automated feedback and endless repetition. For example, while Lennie Tristano insisted on singing solos note-for-note (which students must check by ear or rely on the teacher’s judgment), an app like EarMaster could instantly verify if a student sang the correct interval. Similarly, instead of a teacher clapping a rhythm for a student to echo, a rhythm app can play random patterns for unsupervised practice. This autonomy can increase practice time and motivation, but some educators warn it may reduce human nuance: a student might become skilled at clicking “correct” on an app but still lack expressive feel. 3.2.4 Benefits and Limitations Modern tools allow personalised, flexible practice. Students can drill weak areas at their own pace, track progress quantitatively, and engage in gamified learning. Slowed/audio-transcription tools let learners tackle pieces beyond their current technical level. Mobile apps and cloud platforms enable anywhere-anytime practice, which is especially helpful for busy or distant learners. In higher education, platforms like EarMaster Cloud (2025c) or MusicFirst integrate with online curricula, giving instructors assignments and reports. Some researchers note that these tools can bridge gaps in large classes where one-on-one ear training is impractical (Munive Benites, Lalitte, and Eyharabide 2023). However, technology cannot fully replicate a skilled teacher or ensemble experience. Commercial apps often focus on Western tonal exercises (major/minor systems), which may not cover jazz’s extended/improvised tonalities (e.g. altered dominants or modal jazz). Additionally, software feedback is often binary (right/wrong) and ignores expression. Some students may “beat the drill” by test-taking tactics (memorizing patterns) without deeper musical understanding. Technical issues also arise: automated chord detectors misinterpret complex voicings; time-stretch algorithms can artefact; VR systems may cause fatigue and currently only train simple tasks (intervals, not swing feel) (Fletcher, Hulusic, and Amelidis 2019). Teachers report that finding apps that mesh with a course’s philosophy is challenging (Munive Benites, Lalitte, and Eyharabide 2023); for example, an app that only tests fixed notes may conflict with an improvisation-first curriculum. Cost is another factor: many apps require purchase (though often at modest prices), and institutional adoption sometimes faces budget constraints. Importantly, some purists worry that reliance on “cheat-like” tools can impede ear development e.g., that slow-down software may constitute “cheating” when transcribing, and hearing at full tempo is the real test. However, educators counter that these are training wheels: speed can be reintroduced gradually to build authentic skill. Pedagogically, best practice seems to be using technology as a supplement: begin with slow, guided work, but also include exercises at normal tempo and many “blind” listening tasks. 3.2.5 Modern Technological Approaches to Ear Training: Conclusion Modern technology has substantially broadened the toolkit for jazz ear training. From historically modest computer-assisted programs to today’s interactive apps, VR experiences, and AI transcribers, musicians and educators have more resources than ever for developing listening skills. These tools generally align with traditional jazz pedagogy by reinforcing learning-by-ear, but they allow more self-guided, data-driven practice than the old master-apprentice model. Academic research (from empirical studies to qualitative surveys) suggests that, when used thoughtfully, technology can improve outcomes in pitch, rhythm, and harmonic recognition (Munive Benites, Lalitte, and Eyharabide 2023; Lingjie 2025). At the same time, they remind us of the irreplaceable value of human nuance – the swinging rhythm of a live drummer, the phrasing of a soloist, and the inspirational guidance of a mentor. The most effective ear-training approach in jazz likely combines the best of both worlds: students drill fundamentals and tricky passages with software, then validate and refine their skills in real musical contexts (singer circles, jam sessions, or ensemble rehearsals). As technology continues to evolve – with VR, AI, and improved music analysis – jazz education will doubtless adapt, aiming always to train both the intuitive “good ear” and the knowledgeable, expressive improviser. 3.3 Playing by Ear Research: A Literature Review In this section, I provide an overview of the literature on playing by ear and related skills. Research specifically on the topic of “playing by ear” mainly comes from the music education and music performance literatures. I will review this literature first. However, this literature does not have a strong cognitive psychological depth. To build on this literature, therefore, I will turn more to the cognitive psychology literature. Here, “playing by ear” has not been established as a clear topic, though there is much related research. I will therefore synthesise these literatures and contextualise playing by ear specifically with a deeper cognitive psychological basis: that is, I will review the internal mechanisms that might underlie performance. This will prepare the next chapter, where I will propose a theorectical cognitive (and computational) model of playing by ear skills. 3.3.1 Historical Overview 3.3.1.1 Foundation or Frill? (Woody 2012) In music education research, in at least as early as the 1950’s did researchers such as Mainwaring (1951) begin advancing the importance of playing by ear skills in formal music education. By the late eighties, the music education literature began further lamenting that playing by ear skills were largely overlooked in music education, with the tyranny of visual notation being a main focus of teaching (G. McPherson and Gabrielsson 2002; Priest 1989). A key emphasis and conclusion of this emerging literature in music education was that playing by ear skills were grossly overlooked, and actually turned out to be important contributors to musicianship as a whole: even sight reading skills (G. E. McPherson 2005). 3.3.1.2 From Sound to Sight rather than from Sight to Sound As such, G. McPherson and Gabrielsson (2002) convey how musical meaning arises through a dynamic interplay between performer intention, listener interpretation, and cultural context, rather than being fixed in notation. They argued that music should be taught from sound to sign—that is, beginning with auditory and expressive engagement before introducing formal notation. This approach, they contend, better reflects how meaning is constructed in music and supports deeper, more intuitive learning. Their contribution is significant within music education and cognition, challenging notation-first traditions and advocating for a more experiential, communication-oriented model of instruction. Such an approach was later refined by Haston and McPherson (2022), who gave specific directives as to when learning visual notation should be (e.g., 6-7 years old, after many years of ear-only training). 3.3.1.3 The Contemporary Situation Whilst visual notation may ostensibly reign supreme, perhaps in part due to the somewhat polemical literature reviewed that emerged arguing for the importance of playing by ear (G. McPherson and Gabrielsson 2002; Musco 2010; Priest 1989), did attitudes and practices change. At the present time of writing, playing by ear seems to now be more widely recognised as a valuable pedagogical tool, with aural skill tests, including singing/playing by ear, being a feature of exam boards, at least in the UK. The development of playing-by-ear skills varies significantly across music examination boards. The ABRSM includes aural tests in which candidates sing back short melodies (Grades 1–3), but it does not assess instrumental playback by ear (ABRSM 2024). In contrast, the Trinity College London syllabus features a “musical recall” component that directly tests the ability to play by ear on an instrument (Trinity College London 2024). Similarly, Rockschool places strong emphasis on contemporary aural skills, including instrumental playback and improvisation as part of its exams from early grades (RSL Awards 2024). These differences reflect broader pedagogical focuses, with ABRSM favoring traditional classical training and Trinity and Rockschool supporting a more holistic or contemporary approach. Perhaps due to these contemporary picture, has there been a relative dearth of new research on playing by ear. In the next section, I will describe findings from the music education and performance literature that help us understand the development of playing by ear skills and its relationship to other factors. 3.3.2 Developmental and Pedagogical Aspects of Playing By Ear Skill Acquisition How do musicians acquire the ability to play by ear? Musco (2010)’s review reports that playing by ear skills may develop naturally over time (Gerber 1993; G. E. McPherson 2005), through unsupervised practice (Gary E. McPherson 1995) and practice with instruction [Bernhard (2004); Brown (1990); Dickey (1991). Evidence suggests that the practice of recalling melodies on an instrument can improve playing by ear skills (Bernhard 2005; Dickey 1991; Wilder 1988). 3.3.2.1 Early Development and Informal Learning Developmental studies indicate that playing by ear can emerge at surprisingly early stages given the right exposure. Many children first experience music informally – singing songs, copying melodies by ear on piano or guitar, improvising little tunes – often well before they read notation. Longitudinal research (e.g., G. E. McPherson 2005; G. E. McPherson, Bailey, and Sinclair 1997) followed young instrumental students over time and found that those who engaged in activities like singing or picking out tunes by ear showed enhanced musical understanding in the long run (Varvarigou 2014). McPherson noted that having beginners “sing before playing” an instrument helps them internalise pitch and rhythm, which later aids in connecting sound to symbols (notation). In essence, early ear-playing experiences allow children to embody music – linking what they hear to what their fingers do – which lays a cognitive foundation for all subsequent musical learning. Another study by Gerber (1993) suggested that skill in playing by ear often develops naturally through unguided exploration: children who spend time noodling on their instrument, figuring out favorite tunes by ear, tend to improve in that ability through self-discovery. Such findings align with anecdotal observations that many talented ear-players are self-taught or began learning informally (e.g. picking up guitar chords from recordings or learning folk tunes by ear). In popular music contexts, it is common for beginners to start with imitation of recordings, gradually building a repertoire of songs by ear (Green 2002). Overall, developmental evidence supports the idea that given exposure and opportunity, learners will cultivate ear-playing competencies even without formal instruction – though formal training can further refine and expand these skills. 3.3.2.2 Formal Pedagogy and Ear-Training Despite its evident benefits, playing by ear has not always been systematically incorporated into formal music education. Traditional Western pedagogy, especially in classical music, has heavily focused on notation literacy and technical exercises, with ear-playing sometimes regarded as a lesser skill or even a distraction. Nonetheless, a growing body of pedagogical research advocates for integrating playing by ear into teaching curricula for its positive impact on musicianship. Baker and Green (2013) conducted a landmark “Ear Playing Project” experiment with 10–14 year-old students in the UK, providing one group of students with regular opportunities to learn melodies by ear from recordings (instead of from notation) while a control group received only traditional instruction. After a period of instruction, all students were given standardised aural tests that required them to play back melodies and rhythms by ear. The results were striking: the ear-playing group surpassed the control group in every criterion (pitch accuracy, rhythmic accuracy, melodic contour, tempo, etc.) on the post-tests. In other words, those who practiced learning music by ear showed significantly greater improvement in aural skills than those who only learned through notation. This evidence strongly suggests that incorporating ear-playing in lessons can enhance students’ aural development. Notably, these gains did not come at the expense of other skills; instead, ear-playing experience appeared to reinforce musical memory and understanding that also benefit reading and performing. Parallel research by Musco (2010) and Woody (2012) addressed a common concern among music teachers that spending time on by-ear learning might impede music reading progress (Varvarigou 2014). Their findings counter this worry: students can learn to play by ear and to read music in tandem, with each skill informing the other. In fact, Woody (2012) argued that ear skills make students more sensitive and “musical” when reading notation, since they are not just decoding notes on the page but internally hearing and understanding them (“Woody’s Research to Appear in Psychology of Music, Upcoming New Book Announce University of Nebraska-Lincoln” 2018). Recognising these advantages, educational approaches like Suzuki method (for violin and other instruments) have long emphasised early ear training—students learn tunes by ear first, delaying notation, which purportedly develops strong listening and memory skills (Hermann 1999). Similarly, contemporary methods in popular music education (e.g., Lucy Green’s informal learning approach, Green (2008)) deliberately use playing by ear in group teaching: students might be tasked with figuring out a pop song by listening to a recording together, fostering collaborative ear-learning. Teachers who have tried these approaches often report increased student engagement. For example, Varvarigou (2014) examined instrumental teachers’ experiences when they introduced “copying by ear” tasks in one-to-one lessons as part of the Ear Playing Project. Teachers adopted various strategies, such as singing or humming phrases to prompt students, asking guiding questions, and giving positive feedback while students learned songs by ear. By the end of the project, the teachers noted that incorporating ear-playing provided “a new and enjoyable way” to work on aural skills and even boosted their own confidence in teaching by ear. They observed that students seemed to enjoy lessons more, showed improved improvisation abilities, and grew more confident on their instruments through these activities. These qualitative outcomes echo formal test results: playing by ear can increase musical enjoyment and creativity, likely because it is a more naturally immersive way to learn music than reading alone. 3.3.2.3 Jazz Education and Professional Development To summarise what was discussed in Chapter 3, in jazz music, learning by ear has always been fundamental, and research on jazz pedagogy provides insights into advanced playing by ear skill development. For instance, jazz educators encourage students to transcribe solos by ear (i.e. learn them by listening and reproducing, sometimes writing them down afterward) and to learn tunes from recordings rather than fakebooks. This practice not only ingrains repertoire, but also develops the aural vocabulary needed for improvisation. Ethnographic work, such as Paul Berliner’s Thinking in Jazz (1994), documents how jazz legends acquired their craft: virtually all spent countless hours listening to records, imitating phrasing and solos, and jamming informally – all ear-driven learning. Systematic studies reinforce this: for example, a study of collegiate jazz improvisers by Norgaard (2011) found that expert improvisers rely heavily on musical memory during performance. In interviews, professional jazz musicians described their improvisational thinking as a mix of pre-learned phrases (licks) retrieved from memory, and spontaneous creation guided by ear. They reported that while improvising, they often recall well-learned ideas from memory and insert them, or choose notes based on an internal auditory priority (what they hear as the next phrase) rather than a purely theoretical formula. This again highlights the “primacy of the ear” (also the name given to a popular resource of the same name; Blake (2010)). 3.3.2.4 Improvisation and Creativity Improvisation is closely intertwined with playing by ear, as both involve real-time generation of music without reading. Many studies of improvisation emphasise the role of the ear: improvisers are essentially composing with their instrument in real time, and to do so, they must listen – to the accompanying music, to their own ideas as they unfold, and to an internal sense of where the music could go next. Psychological models of improvisation (e.g., Johnson-Laird 2002; Pressing 1988) propose that improvisers use a repertoire of memorised licks plus generative rules to create new solos. The selection and execution of those licks is guided by auditory feedback: a player might hear a motif in their head and immediately play it, then listen to what they just played and let that spark the next idea. Norgaard (2011)’s study of expert jazz improvisers illustrated this iterative process of planning, executing, and monitoring during solos. The musicians described constantly evaluating their output (“was that phrase good? should I repeat it or change it?”) and making adjustments on-the-fly. This improvisational thinking requires a well-trained ear to judge one’s musical statements and to hear possibilities before playing them. Because of this, exercises that strengthen playing by ear are often recommended to aspiring improvisers. A vivid demonstration of advanced audiomotor integration is seen in jazz improvisers. Harris, van Kranenburg, and de Jong (2016) compared improvising pianists with classical score-dependent pianists in their ability to reproduce unfamiliar melodies by ear. They found that the improvising (ear-trained) musicians had a “superior ability […] to replicate both the pitch and rhythm of aurally perceived music at the keyboard, not only in the original key, but also in other tonalities”. In other words, those accustomed to playing by ear could accurately pick up a tune and immediately play it and even transpose it, far outperforming the note-bound players. The same study linked this behavioral skill to neural differences: an fMRI comparison revealed that improvising musicians showed enhanced activation in a right dorsal fronto-parietal network during audiation tasks, suggesting more efficient neural pathways for audiomotor transformations. These brain regions (including posterior parietal and premotor cortices) are thought to support mapping auditory inputs to motor responses. Likewise, Limb and Braun (2008)’s fMRI study of jazz improvisation found that spontaneous musical creation involves distinct neural dynamics (including heightened activity in motor-planning areas and the suppression of self-monitoring regions), illustrating the brain’s adaptation to real-time ear-to-hand performance. Collectively, research underscores that playing by ear is a whole-brain activity: it recruits auditory perception circuits, memory and planning areas, and motor execution networks in a tightly coordinated loop. With training, this loop becomes highly optimised – as seen in professionals who can hear a melody once and immediately play it. Sensorimotor integration is thus at the core of playing by ear, enabling the immediate translation of heard musical ideas into played music. 3.3.2.5 Learning Strategies for Playing by Ear There is limited research on the practice of learning by ear, especially among popular musicians (Lilliestam 1996), with previous studies of learning by ear usually focusing on students or novices, often limited to short melodies (Lahav2005:1?; Varvarigou and Green 2015). Despite this, there have been a few studies on experienced musicians, who use music theory and advanced strategies (Johansson2004:1?; WoodyLehmann2010:1?). Musicians often learn by ear from recordings, using informal methods (Bennett 1980; Groce 1989) and recordings have been central to learning, even before digital tools (Bennett 1983; Campbell 1995). It has been suggested that recordings serve as “notation” for popular musicians Learning is typically a private, self-taught process (Bennett 1983; Campbell 1995). Contemporarily, tools like Transcribe! and Amazing Slow Downer help slow and loop recordings, but their usage remains under-researched (Driedger and Müller 2016; Johansson 2004). It has also been suggested that modelling i.e., call and response is an effective method to teach playing by ear skills (dickey1988comparison?; Haston and McPherson 2022; Haston 2010). Additionally, providing “opportunities for self-initiated learning and non-evaluated practice” – where students explore without immediate external correction – has been suggested as crucial for developing confidence in ear-playing (Varvarigou 2014). Such unguided exploration allows individuals to discover what strategies work best for them, whether it’s trial-and-error, visualising the music, or relying on muscle memory. An important theme in playing by ear research is the contrast between formally trained musicians and those from informal backgrounds (such as self-taught popular musicians). Studies consistently show differences in how these groups learn and approach ear-playing. Woody and Lehmann (2010a) found that college music students with vernacular (informal) experiences needed significantly fewer trials to learn melodies by ear than their strictly classically trained peers. The formally trained musicians often struggled initially because their habits were tied to written cues and deliberate practice of technique; they tended to focus on “physically producing the melodies” (e.g. thinking about finger positions) instead of listening to the melody as a whole. In contrast, the informal-trained students had automated much of the motor execution, allowing them to focus attention on the sound itself. This finding echoes many anecdotal observations in music teaching: students coming from church bands, garage bands, or other informal environments often have a natural ease with picking up songs by ear and playing with others, whereas conservatory-trained students might excel at reading and technique but initially feel “lost” without sheet music. Of course, these differences are not immutable – formal training can incorporate ear skills, and informal musicians can learn theory and reading. The point is that early experiences shape one’s musical orientation. Individual differences in auditory memory, motivation, and even personality (e.g. willingness to take risks) can also affect playing by ear ability. Some individuals seem to have a proclivity for learning by ear; researchers like Mishra (2016) have speculated about auditory working memory capacity or auditory imagery vividness as factors that might make one person a stronger ear-learner than another. However, the overarching message from the literature is that playing by ear is trainable. Even those who do not consider themselves naturally good at it can improve with practice and the right strategies. For instance, a case study by Creech et al. (2008) showed that adult classical musicians, when encouraged to learn simple tunes by ear as part of a workshop, made substantial progress and reported it positively impacted their listening skills. As one educator summarised, “ear playing is learned by playing by ear” (Johansson 2004) – meaning the best way to get better is simply to do it, genre by genre, song by song, gradually building the mental library and reflexes that enable ever more fluent playing by ear. Recent research has noted how YouTube has been used in various fields for observing real-world behaviors, including learning by ear (Anthony, Kim, and Findlater 2013; Mauriello, Avrahami, and Mankoff 2018). YouTube offers access to real-world examples of musicians learning in their natural environments. Liscio and Brown (2024) conducted a hypothesis-generating study to explore how experienced popular musicians learn music by ear, using 18 YouTube videos as data. Their goal was to identify patterns of human-recording interaction that could inform the design of future music learning technologies. From their observations, they derived six key hypotheses grounded in empirical evidence. First, they found that the scope of learning influences musicians’ interaction with recordings. While some musicians attempted to learn entire songs, most focused on fragments such as riffs or solos, which may reflect both genre conventions (e.g., repetition in popular music) and the limited affordances of their tools. Second, notation and transcription played a surprisingly limited role. While a few musicians created sheet music or tablature during their learning process, these artifacts seemed to function more as memory aids than as tools directly supporting ear-based learning. Notably, most musicians did not consult notation while playing, suggesting that transcription and ear learning may operate as parallel, rather than integrated, activities. Third, purpose-built technologies—such as software designed to slow or loop audio—were seldom used. Most musicians relied on general-purpose media players like YouTube or Spotify and demonstrated little use of features like tempo or pitch adjustment. This underutilisation raises questions about the accessibility, awareness, or necessity of such tools among skilled learners. Fourth, the authors highlighted the importance of working memory in by-ear learning. Musicians frequently sang or hummed notes after pausing playback, indicating that vocal rehearsal may serve as a bridge between perception and performance. The ability to mentally retain short musical phrases and reproduce them on an instrument was a common strategy among participants. Fifth, familiarity with recordings prior to learning appeared to enhance the effectiveness of ear-based learning. Some musicians listened to tracks repeatedly before attempting to play them, or demonstrated genre-specific knowledge that allowed them to anticipate musical patterns. This suggests that pre-exposure and stylistic fluency may scaffold the by-ear learning process. Finally, the study found that theoretical knowledge, particularly regarding harmony and key, aided musicians in identifying chords and notes more efficiently. Those who could name chords or apply scale structures progressed more rapidly than those relying purely on trial-and-error methods. However, the authors caution that this efficiency may stem as much from instrumental fluency as from abstract theoretical understanding. Overall, this study contributes important insights into informal, in-the-wild musical learning practices and calls for future research to refine technological tools that align with the actual strategies musicians use. It also underscores the need for more inclusive datasets and broader participant demographics in future work. In Table xx, I reproduce the hypotheses generated by their study. # Create the data frame table_data &lt;- tibble::tibble( Hypothesis = c( &quot;Desiderata recording interactions change depending on learning scope&quot;, &quot;Transcription serves no role in the by-ear learning task&quot;, &quot;Experienced musicians rarely engage with purpose-built tools&quot;, &quot;A musician’s working memory will dictate their ability to copy notes&quot;, &quot;Intentional recording familiarization improves by-ear learning&quot;, &quot;Knowledge of theory helps make note and chord identification more efficient&quot; ) ) # Display the table knitr::kable(table_data, caption = &quot;Hypotheses about playing by ear strategies generated by Liscio and Brown (2014)&quot;) Table 3.1: Hypotheses about playing by ear strategies generated by Liscio and Brown (2014) Hypothesis Desiderata recording interactions change depending on learning scope Transcription serves no role in the by-ear learning task Experienced musicians rarely engage with purpose-built tools A musician’s working memory will dictate their ability to copy notes Intentional recording familiarization improves by-ear learning Knowledge of theory helps make note and chord identification more efficient 3.3.2.6 Playing By Ear Abilities and its Relationship to Other Factors Musco (2010) how playing by ear skills are related to sight-reading (Luce 1965; Gary E. McPherson 1995; G. E. McPherson 2005). Haston and McPherson (2022) argue that singing should be a frequent and integral component of early music instruction. To advance beyond singing alone, a connection between vocalisation and the corresponding instrumental fingerings should be established - in lieu of visual notation. Musicians who both sing and play during the modeling or echo-teaching process strengthen the connection between their ears and fingers, promoting multimodal learning (Haston and McPherson 2022). The process of mentally and physically rehearsing a piece “by ear”—through simultaneous singing and fingering - would cultivate a child’s capacity to perceive sound through kinesthetic engagement, thereby promoting the integration of bodily movement with cognitive processes (Galvão and Kemp 1999; Haston and McPherson 2022). They further argue that application of solfège could further facilitate this transition by reinforcing the progression from rote memorisation (e.g., singing on a neutral syllable such as “la”) to conceptual understanding (e.g., employing sol-fa syllables and correlating them with instrumental fingerings). 3.3.2.7 Interim Summary In summary, research in music education converges on the idea that integrating ear-playing experiences – whether through informal learning projects, ear-training exercises, or improvisational activities – significantly enriches musical development. It produces musicians who are more well-rounded, confident, creative, and able to adapt to new musical situations (a crucial trait for professionals). Ear-playing is no longer seen as mere talent or trick; it is increasingly recognised as a pedagogical tool and an objective of instruction in its own right. 3.3.3 McPherson’s Integrated Model of Musical Skill Development To situate playing by ear and its development among other skills in an integrative fashion, Gary E. McPherson (1993) proposed and tested a theoretical model that examines the development of key musical performance skills among student instrumentalists. Drawing on both literature and empirical data—student performance outcomes and interviews, McPherson’s model offers a nuanced view of how various learning experiences and individual practices contribute to musicianship. The model, later supported by further analysis (G. McPherson and Gabrielsson 2002), reveals both direct and indirect pathways between factors such as early musical experiences, quality and length of study, and the development of skills like sight-reading, improvisation, and playing by ear. One of the central findings of McPherson’s model is the pivotal role of playing by ear. This skill was shown to be the strongest predictor of a student’s ability to improvise, suggesting that aural engagement is foundational to more creative aspects of musical performance. Playing by ear was also shown to have a meaningful, though more complex, relationship with sight-reading ability—supporting earlier findings that ear-training may play a more critical role in developing musical fluency than traditionally emphasised (Luce 1965; Priest 1989). If also found that early musical experiences positively influenced students’ capacity to play by ear, though they had little impact on sight-reading. Similarly, engaging in enriching activities—such as improvising or playing by ear during practice—had a strong impact on the development of ear-playing skills, but again, less so on sight-reading. Interestingly, the quality of formal study had only a weak influence on both ear-playing and sight-reading, while the length of study had a moderate and consistent effect across multiple performance domains, including rehearsed performance, sight-reading, and playing by ear. Through qualitative analysis of student interviews, McPherson also identified clear differences in cognitive strategy between less and more skilled performers. Less proficient players often relied on abstract, notation-based thinking, such as trying to identify note names while listening. In contrast, more capable musicians demonstrated a more integrated aural-kinesthetic approach, mentally translating what they heard directly into instrumental actions. Overall, McPherson’s model offers an argument for the centrality of ear-playing in music education, as further iterated in (Haston and McPherson 2022). It suggests that developing the ability to play by ear not only supports improvisational skill but also facilitates the integration of aural, visual, and motor domains essential to fluent musicianship. These findings carry significant pedagogical implications, potentially challenging traditional hierarchies in music training and highlighting the potential value of emphasising aural-based learning strategies in instrumental instruction. Figure 3.1: Path analysis of the McPherson (1993) theoretical model. Thicker lines represent stronger relationships and thinner lines weaker relationships from one variable to the next, as shown by the directed arrows. Reproduced with the permission of Gary McPherson. 3.3.4 Cognitive Mechanisms Underlying Playing by Ear In the next section, I now move towards describing playing by ear at a more detailed cognitive level. As far as I am aware, no comprehensive cognitive model of playing by ear yet exists. However, several models come very close to describing this phenomenon, and I thus review and later synthesise them here. In general, music performance relies on three key cognitive skills: goal imaging (imagining the sound), motor production (executing movements), and self-monitoring (hearing and assessing one’s performance) (Lehmann and Davidson 2002; Woody 2003). Similarly, Haston and McPherson (2022) describe playing by ear as requiring “aural discrimination skills and the alignment of concomitant motor skills: goal imaging, motor production, and self-monitoring. Linking that goal image to motor production is key.”. Fluency in playing an instrument and by-ear playing involves creating strong connections between imagined sounds and physical actions—developing ear–hand coordination. Ideally, even reading music should trigger mental sounds that are already linked to specific instrumental actions. In terms of cognition, I will review the various levels of processes required to play by ear, starting with “low-level” processes and moving up to high-level cognition. 3.3.5 Auditory Perception Auditory perception represents a complex cognitive process through which the human brain transforms acoustic vibrations into meaningful sound experiences, involving multiple interconnected neural pathways and processing stages. The process begins with sound wave transduction in the cochlea, where mechanical vibrations are converted into electrical signals that travel through the vestibulocochlear nerve to various brainstem nuclei before reaching the medial geniculate body of the thalamus, which acts as a critical information processing hub that actively shapes auditory representations rather than serving as a simple relay (Bartlett 2013). From the thalamus, processed auditory information reaches the primary auditory cortex in the temporal lobe, where sophisticated computational processes enable the brain to perform what Bregman (1990) termed “auditory scene analysis” - the fundamental ability to segregate, integrate, and organise complex mixtures of simultaneous sounds into distinct perceptual objects (Sussman 2017). This cognitive process involves both automatic, stimulus-driven mechanisms that operate without conscious attention, as well as top-down attentional processes that allow selective focus on specific sound sources in noisy environments, such as following a conversation at a cocktail party (Sussman 2017). The auditory system’s remarkable capacity for sound localization, pitch perception, and timbre recognition emerges from the integration of multiple acoustic cues processed across parallel neural pathways, with attention serving as a limited but flexible resource that can enhance processing of attended sounds while maintaining access to unattended auditory information in neural memory (Carlini, Bordeau, and Ambard 2024; Sussman 2017). 3.3.6 Working Memory With attention applied to the auditory stimulus, tonal information is parsed and generally enters an acoustic store. This acoustic store is encapsulated in a memory system known as working memory (A. D. Baddeley and Hitch 1974; A. Baddeley 2000), a form short term memory system. Cognitive psychology distinguishes between short-term memory and long-term memory in terms of both capacity and duration. Short-term memory (often equated with working memory) is a limited storage that holds information for mere seconds if not actively rehearsed (Cascella and Al Khalili 2025). In contrast, long-term memory can retain information for days, years, or even a lifetime. These two memory systems differ not only in duration but also in function and mechanism. Working memory acts as a mental “scratchpad” for immediate information processing (such as remembering a phone number long enough to dial it), whereas long-term memory stores more durable knowledge such as facts, personal experiences, and skills. The classic (atkinsonHumanMemoryProposed1968?) conceptualised memory as a flow from transient sensory memory, to short-term memory, and eventually to long-term storage. While modern research has refined this modal model, the basic distinction remains useful: short-term memory is fleeting unless information is deliberately maintained or encoded into long-term memory. The construct of working memory is now well-developed in psychology, with the most popular model being Baddeley and Hitch (1974)’s multi-component model, subsequently updated in Baddeley (2000). Working memory refers to the ability to transform and manipulate information in short-term memory. In general, it is thought to comprise components for manipulating phonic and visual stimuli separately. Music scholars have long recognised the important role of working memory in musical behaviours, particularly those involving aural skills (Chenette, 2021; Cornelius &amp; Brown, 2020; Gates, 2021; Karpinski, 2000). Indeed, those with formal music training have widely been documented to have better geneal working memory capacities (Talamini, Altoè, Carretti, &amp; Grassi, 2017; Talamini, Carretti, &amp; Grassi, 2016), but note, it is not clear that musical training causally influences general working memory (Silas et al., 2022). It has been argued that (baddeleyWorkingMemory1983model?)’s standard general working memory model does not sufficiently explain the manipulation of musical stimuli (e.g., see Berz (1995)). Instead, it is proposed that there are other working memory components that represent a different class of working memory called long-term working memory (Ericsson and Kintsch 1995), which particularly relies on experience-driven and domain- specific training in specialised domains, such as formal music training and chess, cultivates domain-specific forms of working memory, whereby (musical) abilities are subserved by relatively specialised systems, quite distinct from general working memory. In our own research, we have found that a music-specific working memory system can be statistically dissociated from a general working system Silas et al. (2022). Musical training appears related to musical working memory, which in turn is related to general working memory, but general working memory and musical training and not directly related. One could conceive that general working memory encapsulates musical working memory, which relies more on training and music-specific knowledge. In this way, general working memory probably constrains the development of musical working memory. We documented the possible scenarios which might explain the links between domain-general and domain-specific (music) working memory faculties: they may be relatively (statistically) disparate, but nonetheless, rely on each other, potentially bidirectionally. The implications of this are that, perhaps by definition, musical abilities are subserved by both domain-general (potentially to do more with inherited characteristics) and domain-specific (potentially more to do with training) faculties. In other words, someone with a very good general working memory might be able to demonstrate a similar level of musical (e.g., sung recall) performance to someone who has had more musical training. The former’s general faculties may help them monitor their performance as well as someone who has carved out music-specific templates to aid the same task. The underlying processes may be different, but the observable phenotype similar. Framed in terms of our study: if music conforms to a style that people in the general population are familiar with, do musical features (often better remembered by expert musicians) tend to matter? With relatively simplistic, familiar musical styles, is performance really mainly mediated by music-specific processes, or could it be more domain-general processes which turn out to be important? If melodies are long enough to require multiple attempts to sing in full, are musical features beyond length clearly important, compared to the length of a melody alone? In Silas and Müllensiefen (2023), we further explored this potential difference musical vs. non musical nature of sung recall further. We found that, the handling of relatively stylically simplistic (i.e, pop) melodies, can almost be thought more of a general working memory, rather than musical working memory, task. This is because, if melodic material is simplistic enough enough to handle, such that the general population will have had much exposure to their contents, then the importance of musical training in memory way not be as pronounced, and sung recall may then become more about general constraints on human memory (Christiansen &amp; Chater, 2016; Cowan, 2010; Miller, 1956; Oberauer &amp; Cowan, 2007). In this way, we found that not all all variance in melodic recall behaviour may be explained in musicological terms, suggesting that domain-general memory mechanisms should not be overlooked in explaining the representation of musical stimuli, at least when they are stylistically simplistic. This suggested the potential application of models and ideas from already well-established theories of produced mental representations in non-musical domains (non-musical serial recall; Anderson (1972)). We concluded that concepts related to general working memory constraints (e.g., non-musical serial recall, item length) are important in explaining melodic recall, potentially more so than any other musicological considerations (e.g., interval representations, tonality) at least when the length of the melodies certainly requires multiple attempts to sing back all the notes (e.g., length 15-48), or with pop melodies which are relatively simple to sing. In other words: the “melodic recall” of relatively simple pop melodies appears to be closely related to “recall” in other memory domains. In summary, the handling of musical stimuli in working memory depends ultimately on general working memory, but the more stylistically advanced the features become, the more musical training is required in developing domain-specific expertise (in Western music listeners, familiarity with simplsitic pop music does not constinute expertise, but a norm). In the non-musical literature on verbal recall exists some relevant non-musical analogues to the observations that Sloboda and Parker (1985) made, that the attempt length increases across attempts (e.g., from the Adaptive Control of Thought-Rational, ACT-R literature Anderson, 1972, 1972; Chikhaoui et al., 2009). The so-called list length effect is the finding that recognition performance is superior for items that are part of a short list than for items that were part of a long list (Kinnell &amp; Dennis, 2012). Usually, the literature on verbal memory has used lists of unrelated words as stimuli and asks the participant to recall as many items as they can from memory. Over multiple attempts, Murdock Jr. (1960) specifically found that the shape of the learning curve across attempts can be described as an exponential curve with an asymptote equal to the number of items in a target list. However, word lists have different properties to melodies, which presuppose serial recall (i.e., a note order) and embody important structural features within interval and rhythmic patterns. It is not clear if the unique properties of musical stimuli mean that melodic recall processes are underpinned by fundamentally different processes to their non-musical analogues. Next, we discuss previous approaches to studying melodic memory. 3.3.6.1 Auditory Imagery and Mental Representation A central cognitive component of playing by ear is auditory imagery: the ability to internally hear or imagine music without external sound (geldingEfficientAdaptiveTest2021?). Skilled by-ear players exhibit strong auditory imagery capacities, which enable them to anticipate and mentally “hear” pitches and phrases before playing them. A fundamental skill for playing by ear is precise pitch perception – the ability to identify and reproduce notes and intervals. Many ear players develop acute relative pitch skills, allowing them to recognise scale degrees or chord tones by ear and map them to their instrument. Some research has examined the role of absolute pitch in ear-playing as well. While absolute pitch (the ability to identify pitches without a reference) can aid certain tasks, it appears to be somewhat rare, even in musician populations (&lt; 30% with absolute pitch; Gregersen et al. (1999); Leite, Mota-Rolim, and Queiroz (2016)). Instead, most musicians seem to rely on well-honed relative pitch and tonal schema to play by ear in real musical contexts. For instance, learning a song by ear often involves detecting its tonal center and scale, recognizing common chord progressions (e.g. the 12-bar blues or II–V–I progression in jazz), and using those frameworks to guide note choices. Johansson (2004), in a study of rock musicians, observed that ear players often identify familiar “clichés, harmonic formulas and other stylistic traits” of the genre to decode songs on the fly. In other words, they listen for characteristic chord patterns or melodic motifs, which helps them predict upcoming pitches. This genre-dependent pattern recognition underscores that ear-playing proficiency is partly a product of genre-specific listening experience. Musicians essentially build a library of musical phrases and progressions in memory by extensive listening; when a new piece is heard, they match it to known patterns. Over time, this process can become quite automatic. In summary, cognitive mechanisms of pitch processing in playing by ear involve both fine-grained perception (discriminating pitches and intervals) and higher-order schema (tonal and stylistic expectations), all coordinated through the musician’s auditory imagery and memory systems. Playing by ear, sight-reading, and interacting musically help build an internal “aural database,” expanding musicians’ capacity to memorize more music (Fine et al. 2006; Haston and McPherson 2022; Odam 1995) Aural imagery allows musicians to develop and store mental representations of melodic and harmonic patterns, which supports more efficient future learning (Freymuth 1999) Memorising music relies on integrating aural, visual, and kinesthetic codes, making learning more robust and multidimensional (Hallam 1998) Playing by ear strengthens aural and kinesthetic memory - and possibly visual memory - as musicians often mentally visualise notation or musical structure (Woody and Lehmann 2010b; Woody 2019) 3.3.6.2 Pitch Processing and Audiation playing by ear promotes “inner hearing” (Haston and McPherson 2022; Woody 2012) - allows musicians to form a mental model of the sound they are going to play. developing inner hearing allows performers to “listen with expectation” (Haston and McPherson 2022) 3.3.6.3 Sensorimotor Integration and Neural Basis Behavioural evidence suggests that the auditory imagery generated (in working memory) can guide performance by activating sensorimotor representations (novembreConceptualReviewActionperception2014a?; Keller, Dalla Bella, and Koch 2010; Keller and Koch 2006; Pfordresher 2005; Pfordresher and Palmer 2006). Imagining sounds that correspond to specific instrumental actions can trigger the motor programs for those actions. In other words, when an experienced musician internally hears a melody, that imagery is closely coupled with the fingerings or movements needed to produce it. This aligns with the concept of audiation in music education (E. E. Gordon 2001; E. Gordon 2007), whereby musicians “hear” music in their mind as a precursor to performing it. Neuroimaging evidence further supports this auditory–motor linkage. For example, studies show that simply imagining musical sequences engages regions of the brain’s motor planning network in trained musicians (Bangert et al. 2006; D’Ausilio et al. 2006; Haueisen and Knösche 2001; novembreConceptualReviewActionperception2014a?). Keller (2012) noted that mental imagery in music performance serves important functions such as planning upcoming notes and monitoring intonation. Neuroimaging studies have shown that similar brain activity occurs when music is heard and when it is imagined, suggesting that musicians can internally “hear” accurate pitches and “feel” the appropriate muscular movements during mental practice (Alluri et al. 2017; C. L. Gordon, Cobb, and Balasubramaniam 2018; Haston and McPherson 2022). This integration of auditory imagery with the sensation of performance highlights the constant cross-modal processing involved in instrumental playing (Ross 1985; Woody 2012). Additional research indicates that mental practice away from the instrument can be as effective as physical practice (Miksza 2023). Perhaps the most striking aspect of playing by ear is the seamless integration of heard sound with motor action. This audiomotor integration means that when an experienced musician hears a melody (externally or internally), their brain quickly maps those sounds to the required finger movements or vocal actions. Neuroscientific research has shed light on the brain networks that enable this feat. In skilled musicians, auditory and motor regions of the brain form strong bidirectional connections due to training. (novembreConceptualReviewActionperception2014a?) review evidence that musical training “couples” perception and action systems, such that perceiving a musical phrase can automatically activate the motor program to play it. This reflects the brain’s general action–perception coupling mechanism, where anticipated effects (like a heard note) can trigger the motor command associated with producing that effect. In the context of ear-playing, this means a violinist might hear a phrase in their mind and feel the impulse to execute the fingerings and bowings, or a pianist hearing a chord might internally rehearse the hand shape required. 3.4 Interim Summary To summarise the cognitive psychological literature reviewed and relate it back to the music education literature: playing music by ear also engages working memory and long-term memory in unique ways. Musicians must temporarily hold incoming auditory information (i.e., the sounds just heard) while simultaneously planning and executing motor responses on their instrument. This places demands on the phonological loop (for retaining pitch sequences) as well as domain-specific long term working memory for melodies, as well as the integration of auditory and motor memory. Studies have explored how working memory for musical material might differ between those who rely on ear-playing and those who rely on notation. Nichols, Wöllner, and Halpern (2018) compared jazz and classically trained musicians on auditory working memory tasks and found notable differences. Jazz players – who typically emphasise learning by ear – outperformed classical players in remembering and reproducing heard melodies, especially when they had to play back the melodies on an instrument. In this study, musicians were asked to recall the final notes of sequences after a distraction, either by writing/singing or by playing them on a piano. Jazz musicians showed superior recall for aurally presented melodies when responding instrumentally (playing by ear), and their scores positively correlated with years of jazz experience. The authors concluded that jazz training (i.e., involving extensive ear-playing and improvisation experience) might enhance a domain-specific working memory capacity which allows the better handling of musical information in real time, especially relevant to improvised music contexts. Furthermore, ear-trained musicians often develop efficient cognitive chunking strategies. Instead of processing every note in isolation, they rely on familiar patterns and harmonic sequences stored in long-term memory. When reproducing a melody by ear, skilled musicians may anticipate likely chord progressions or melodic resolutions based on their internalised “musical grammar”. This predictive listening reduces memory load by allowing the musician to generate expectations. In this way, Woody and Lehmann (2010b) found that musicians with extensive vernacular (informal) music experience could learn melodies by ear with fewer repetitions than classically trained musicians, thanks in part to better anticipation of melodic/harmonic patterns. Woody and Lehmann (2010a) studied how well college music majors could learn melodies by ear. They compared students with “vernacular” music backgrounds (e.g., jazz, rock) to those trained mainly through formal instruction. The vernacular musicians outperformed the formal ones, needing fewer attempts to accurately sing or play back melodies. On average, vernacular students required 3 attempts to sing and 3.8 to play melodies, compared to 6.4 and 10.6 for formal students, suggesting that vernacular training places emphasis on both melodic memory (goal imaging) and the ability to reproduce melodies on instruments (motor production). The ear-trained players applied a “more sophisticated knowledge base to generate accurate expectations” about how a melody would unfold. In contrast, formally trained players without strong ear-playing backgrounds used less efficient, more surface-level strategies for encoding melodies. This implies that cognitive schemas and pattern knowledge (acquired largely through listening and imitating) free up working memory resources during playing by ear by providing ready-made chunks and expectations. Expert playing by ear musicians form rich mental representations of pieces, often encoding not just the melodic contour but also harmonic and structural features. Musicians highly skilled in ear-playing use “harmonically oriented integrated cognitive strategies,” whereas less experienced ear-players rely on more fragmentary, note-by-note approaches (Woody 2020). In Woody’s study, university music students who were guided to focus on a melody’s underlying chord progression developed a more substantive “goal image” of the tune, which in turn improved their ability to learn it by ear. This indicates that adding harmonic context to one’s auditory imagery (for instance, knowing the chord changes) leads to more robust mental representations and facilitates ear-playing. Thus, proficient playing by ear involves vivid auditory imagery and schema-like memory for musical patterns, especially harmonic frameworks. 3.5 Pfordresher’s Cognitive Model of Singing Accuracy and H-PAC Models Pfordresher et al. (2015) provided a foundational overview of theoretical approaches to singing accuracy, emphasising the complexity of the cognitive and sensorimotor processes involved in vocal production. They highlight that accurate singing depends on more than just pitch perception or motor ability—it arises from the interaction between perceptual and motor systems, including how auditory targets are represented and transformed into vocal output. knitr::include_graphics(&#39;../img/pfordresher_singing_model.jpg&#39;) Figure 3.2: Pfordresher et al.’s (2015) cognitive model of singing accuracy. Reproduced with the permission of Peter Pfordresher. This perspective sets the stage for the H-PAC (Hierarchical Perception-Action Coupling) model later proposed by (pfordresherSoundActionMusic2019?), which formalises these ideas into a mechanistic account of how singing behaviour emerges from hierarchical layers of processing. According to the H-PAC model, musical performance involves the dynamic coupling of auditory perception with motor planning and execution, structured across multiple levels—from abstract representations such as melodic contour, down to specific articulatory commands that activate the vocal apparatus. These commands refer to the fine-grained motor instructions sent to the muscles involved in vocal production (e.g., the larynx, tongue, lips), translating high-level musical goals into physical sound. knitr::include_graphics(&#39;../img/pfordresher_hpac_model.png&#39;) Figure 3.3: Pfordresher et al.’s (2019) H-PAC model. Reproduced with the permission of Peter Pfordresher. A central feature of both the (2015) and (pfordresherSoundActionMusic2019?) accounts is the notion that poor-pitch singing often reflects a disruption in the mapping between auditory targets and motor output, rather than a deficit in either domain alone. This helps explain why some individuals with intact pitch perception still struggle to produce accurate vocal pitches. Within the H-PAC framework, such difficulties are attributed to failures in either feedforward mechanisms - which plan motor actions based on internally generated auditory representations - or feedback mechanisms, which compare ongoing output with auditory goals and make real-time corrections. At the low level, this cognitive system includes an auditory feedback loop, wherein external sound is encoded as perceptual representations of pitch, timbre, duration, and loudness. These low-level representations are used to guide sensorimotor planning through a translation mechanism, which maps perceptual information onto physical actions. In singing, this mapping guides processes like respiration, phonation, and articulation. In instrumental performance, however, these actions are instrument-specific: finger coordination for piano, violin, or clarinet; embouchure control for brass and woodwinds; or arm and wrist movements for percussion. Beyond low-level mapping, higher-level cognitive processes support the categorisation and structuring of auditory input. Drawing on long-term memory (baddeleyMemory2009?; Ericsson and Kintsch 1995), listeners use mental templates to extract musical features such as tonality, melodic contour, and phrasing. These abstract representations provide top-down input to inform sensorimotor planning, while also being shaped by real-time auditory feedback. The result is a bidirectional architecture, in which perception and action are constantly influencing one another. Crucially, this same system can be extended to playing by ear—the reproduction of music on an instrument without notation. In such contexts, an auditory stimulus is perceived, mentally represented, and mapped onto the motor system of a particular instrument. The perceptual and cognitive processes involved in encoding, segmenting, and categorising the input remain largely the same as in singing; it is primarily the motor modality that differs. Whether reproducing a melody vocally or on a violin, successful performance depends on the strength of the mapping between internal sound representations and the corresponding motor output. One difference between singing and instrumental playing is the different kinds of kinaesthetic perceptual traces and memories that may reinforce learning (Galvão and Kemp 1999). Thus, both Pfordresher et al. (2015) and the (pfordresherSoundActionMusic2019?) H-PAC model offer a unified theoretical framework for understanding musical reproduction across domains. By situating performance within a hierarchically organised, interactive system that links perception, cognition, and motor control, this approach provides a compelling account of how individuals produce music—either through singing or playing by ear. It also offers explanatory power for understanding individual differences in performance ability, the nature of imitation deficits, and potential strategies for remediation or training. 3.6 Baker’s Computational Model of Melodic Dictation Baker (2019)’s computational model of melodic dictation presents a stage-based framework that simulates how listeners perceive, process, retain, and ultimately transcribe melodies into symbolic notation. Whilst melodic dictation - the reproductive act of transcribing a melody into visual notation on paper - is different to melodic recall/playing by ear, this model has some useful features to add depth to the other frameworks discussed above. The model begins with a perception stage, during which the dictation object — typically a short monophonic melody — is encoded into a set of low-level melodic features. For simplicity, the author follows (pearceConstructionEvaluationStatistical2005?)’s representation of both pitch (note and scale degree) and timing (rhythm and interonset interval). Whilst Baker only uses these features for convenience, it is likely that other higher order melodic features are relevant to this task as well as playing by ear. An important part of this model is the notion that as a melody unfolds serially (note-by-note), each new note added to the working memory buffer for the target melody adds a new degree of information content. The information content is computed based on a users stylistic knowledge of Western tonal music that has in turn been created through enculturation (i.e., many years of listening; (pearceConstructionEvaluationStatistical2005?)). The more training a musician has undertaken, the larger their library of “learned” musical chunks that they can rely on. Each transcriber has their own working memory capacity for such information, which when reached should lead to a deterioration in memory. After hearing and representing the melody, a search for representations of that melody in ones “prior knowledge”. If a match is found for a representation that has been explicitly learned, then the melody or segment can enter the transcription module and be transcribed. If there is a gap between prior knowledge and the target melody, on patches will be transcribed, based on any matches that could be found for melodic subsets (i.e., N-grams). Baker (2019)’s conception of cumulatative information content is useful and highly relevant to playing by ear. In playing by ear, this cumulative information content would be implicated in both the heard melody and the recalled melody. The idea of segmenting and only being able to recall N-gram subsets of melodies avaialble in prior knowledge is also a useful notion. 3.7 Models of Sung Recall Lastly, I synthesise models from my own research regarding sung melodic recall. My Silas, Müllensiefen, and Kopiez (2023) study investigated sung recall by examining its relationship with other related skills and participant attributes. Demographically, better sung recall ability ability was associated with more musical training, a younger age, and being female, though the latter two effects were small. The findings suggest that while musical training can enhance singing ability, inherent skills might also predispose individuals to seek musical training. Moreover, there was a relatively large difference between the marginal and conditional R^2 values, which suggests that there is a sizeable proportion of individual differences in the sample of participants tested which explains SAA performance which shows that individual differences should explain performance on a task in which one can develop high levels of domain-specific expertise. We found there that melodic discrimination, pitch imagery abilities, and mistuning perception appear to be fundamental skills contributing to melodic sung recall. The sung recall score we devised moderately correlated with self-reported singing ability and musical training. However, it shows no significant correlation with visuospatial working memory or pitch discrimination, aligning with previous research and suggesting that singing ability is not closely linked to low-level perceptual processes. This suggests that, whilst The study also shows that melodic features which indicate melodic complexity (including melody length) are relevant predictors of sung recall performance. These predictors represented melody length, as well as features related to contour, tonality, statistical occurrence of N-grams in the melody, and durational complexity. Rhythmic trials were generally found to be more difficult and we found that it was appropriate to create separate models for arhythmic and rhythmic. In another of my papers, (Silas and Müllensiefen 2023), we investigated how individuals learn and recall melodies using a computational approach that emphasizes similarity metrics rather than traditional accuracy-based methods. The research involved 31 participants who sang back 28 melodies presented either as piano sounds or vocal audio excerpts from pop songs, with each melody repeated up to six times. The similarity between the target melody and the participants’ sung recalls was measured using advanced algorithmic techniques. The primary goal was to understand how melodic recall improves over successive attempts and what factors most significantly influence recall accuracy. One of the key findings of the study is that the length of sung recall attempts consistently increases across multiple attempts, and this increase significantly correlates with the overall similarity to the target melody. This observation challenges the traditional view that musical features such as interval patterns or tonality are the primary factors influencing melodic recall. Instead, the study suggests that the sheer length of the melody plays a more critical role, indicating that general memory constraints may be more influential than music-specific features. This insight aligns with theories of working memory, suggesting that melodic recall shares characteristics with other types of serial recall rather than being purely domain-specific to music. A significant finding (presented there in Figure 8), examines how similarity changes as a function of attempt and the specific section of the melody (beginning, middle, end). The results show that similarity performance increases more rapidly for the beginning of the melody compared to the middle and end across successive attempts. This indicates that participants tend to stabilise their recall of the initial melodic segments earlier than the middle or end sections. The end sections, in particular, show a slower rate of improvement, suggesting that the latter parts of a melody are more challenging to accurately recall perhaps due to the higher working memory load as a result of having to remember more notes, the later you are in a position, or that generally people engaging in sung recall prioritise getting the earlier parts melody correct first. This finding supports the theory that memory encoding is stronger for the beginning of sequences, a phenomenon consistent with the primacy effect observed in other types of serial recall tasks. It also highlights that recall accuracy is not uniformly distributed across a melody, with the initial segments being more robustly learned compared to later ones. The study also found that musical training did not significantly enhance recall performance when similarity metrics were used as the primary assessment method, as shown in another key result. This challenges the conventional belief that formal musical training substantially improves melodic recall ability. Instead, the data suggests that domain-general memory skills may be just as important, pointing to a more integrative understanding of how musical and non-musical cognitive processes interact. Moreover, the results highlight that similarity scores increase with each successive attempt, but the rate of improvement diminishes after the third or fourth attempt. This pattern suggests that the most substantial gains in melodic recall occur early in the learning process, followed by incremental refinements in pitch and rhythm accuracy. Additionally, melodies presented as vocal audio excerpts resulted in higher recall accuracy compared to piano sound presentations, indicating that the human voice might provide additional mnemonic cues that aid in memory encoding. Overall, the study’s use of similarity-based metrics, particularly the “opti3” metric that integrates pitch intervals, harmonic progression, and rhythmic similarity, offers a more nuanced and accurate assessment of how melodies are learned and recalled. By focusing on the gradual increase in recall length and the differential improvement across melodic segments, the study sheds light on the underlying cognitive processes involved in melodic memory, emphasizing the role of general memory mechanisms and the relatively limited impact of formal musical training. 3.7.1 Playing by Ear Research: Conclusion The phenomenon of playing by ear sits at the intersection of cognitive skill, pedagogical practice, and musical creativity. From a cognitive standpoint, playing by ear engages complex mental processes: vivid auditory imagery, specialised working memory, pattern recognition, and a tight coupling of perception and action. Neuroscience suggests that extensive ear-playing experience can even shape the brain, leading to more efficient audiomotor networks (as seen in improvising musicians). Developmentally, learning to play by ear can begin early and flourish in environments that encourage informal exploration. Incorporating ear-playing into formal instruction – as research in music education has shown – yields benefits for aural skills, musical understanding, and student motivation. In jazz and other professional domains, playing by ear is not only a skill to acquire but a mode of musical existence: it underpins the ability to improvise and to connect with music on a deeply intuitive level. Individual differences certainly influence how people approach playing by ear, yet studies underline that with the right strategies and experiences, virtually any musician can improve their ear-playing ability. This rich body of literature refutes the old notion that playing by ear is a mysterious gift; instead, it is a multifaceted skill set that can be analysed, taught, and learned. As “Woody’s Research to Appear in Psychology of Music, Upcoming New Book Announce University of Nebraska-Lincoln” (2018) observed, developing ear-based musicianship imbues music making with a certain “humanness” – it reconnects performers with the fundamentally aural nature of music. In practical terms, musicians who play by ear often learn new material faster, memorise more effectively, and perform with greater expressivity and flexibility. For educators, the implication is clear: fostering playing by ear alongside other skills produces more well-rounded and versatile musicians. For researchers, playing by ear remains a fertile topic, raising questions about auditory cognition, expert memory, and even cross-cultural musical learning. The literature so far, grounded in cognitive psychology, music psychology, and systematic musicology, provides a compelling case that playing by ear is both an essential subject of study and an invaluable aspect of musical praxis – from the novice learner picking out a tune, to the jazz virtuoso crafting a solo, guided all the while by the inner ear. "],["a-comprehensive-theorectical-model-of-playing-by-ear.html", "4 A Comprehensive Theorectical Model of Playing By Ear 4.1 Computational Musicological Perspectives 4.2 Melodic Similarity 4.3 Cognitive Psychology 4.4 Statistical Modelling Frameworks", " 4 A Comprehensive Theorectical Model of Playing By Ear Consequently, synthesising the results above, we can characterise playing by ear as involving the following processes. Where I mark the text grey, I highlight an additional suggestion beyond the reviewed literature, as a contribution from my own theory development. I will refer to the person playing by ear as the “player” in the following description. Playing by ear first requires a target melody to be perceived. Due to the nature of serial presentation, a presented auditory melody unfolds note-by-note. From the acoustic signal, the player decodes the fundamental frequencies into a melodic object based on their structural knowledge, based on a lifetime of listening (enculturation). The ability to do this also depends on the acoustic features themselves, especially its timbre. The closer the timbre to the player’s on instrument, the easier it is to decode (and later recall) because the player can draw not only on melodic memory, memory for absolute pitch, including multimodal representations that might activate kinaesthetic/tactile traces (CITE). This overall ability to image the correct melodic object is what we call “audiation”. More experienced listeners and better musical training are better able to audiate this image, segmenting/chunking the melody into relevant chunks. Each time a new note is heard from the melody, its strain on working memory increases based on the information content in each new note, but also, which ultimate represents the emergent tonal, rhythmic, metric and other structural features. In long term working memory for melodies, N-Gram chunks are distinct entities, each with their own properties - in particular, the amount that the player has experienced them in their listening experiences (including practising an instrument). This statistical sensibility should approximate the real frequency of occurrence in music. If there are visual cues (e.g., visual notation or a model - like a teacher to immitate) accompanying the audiated melody, these will be decoded in their respective working memory slave systems and potentially integrated in the episodic buffer, to form a multimodal representaiton that could inform overall recall. Once the target melody has completed, it will be stored in a short term memory buffer and flagged as a point of reference. The longer this representation remains in this buffer without mental or physical rehearsal, the more it will decay in memory in a non-linear fashion. The extent of the target melody’s burden on working memory load dictates if how the player will attempt to recall the melody, if at all. If working memory capacity is greatly surpased, with the player able to retrieve no reliable representations, they may not attempt to recall the melody at all. If they have some vague representations, they might attempt to improve a sketch of what they heard. In a performance/high pressure situation (e.g., an aural exam), maybe they will have some fallback approach to attempt to respond musically or “style it out”. Depending on the context, maybe the player (e.g., in the practising/learning) will take a strategy to help reinforce the melodic image because attempting to reproduce it, for example, by singing it. Eventually, if at least some - if not all - of the melody of the is represented reliably in melodic long term working memory, the player will attempt to produce it in the recall and an action plan will be setup to reproduce the melody. Potentially, clear motor action plans will have already been activated in respond to the stimulus in the form of imagined actions to reproduce the sequence. The player will direct their attention to the instrument and the tactile sensations and motoric actions required to reproduce the sound. As they produce each note, these will be perceived by the ear and parsed again, note-by-note into first basic auditory representations in the phonological loop, but then higher order melodic structural ones in melodic working memory. Because the recalled melody is being physically produced by the player, it will have richer cues: e.g., visual ones, for a pianist directing their fingers to different keys, tactile ones for all instruments, motoric cues (i.e., the pattern tactile sensations). Again, they will be parsed eventually into an unfolding multimodal melodic representation. Every time a new note is recalled, a melodic similarity algorithm will compute on the latest version of the recalled melody in relation to the target melody. Generally, as each new note is recalled, the similarity should go up. If the similarity goes down, this will be flagged as a prediction error, feeding back to motor plans and potentially causing the player to make judgements. The player will presumably attempt to make the rest of the recall maximise the similarity, even if certain notes or rhythms have clearly violated the possibility of perfect similarity. Typically, target melodies that were not held strongly enough in working memory or where there were errors in the recall will lead to shorter recalls, whereby the player may stop, perhaps to re-imagine the melodic object, or, if possible, to take another attempt at hearing it and trying again. If there are multiple attempts, which is usually the case in learning a melody, each attempt will increase in length and similarity, until the length of target and recall match and there is perfect similarity, or, if this is not possible in a given (learning) session, working on that melody will eventually be terminated once fatigue or motivation is sufficiently low. Returning to replay that melody at a later time, especially after a night’s sleep, due to consolidation effect, should typically make the process easier next time, so long as the melody in question was too difficult - and therefore, the feasibility of this is a function of a melody’s difficulty, which is also a function of a player’s ability (which is in turn a function of their musical training and general working memory capacity). knitr::include_graphics(&#39;../img/PBE Computational Model.png&#39;) 4.1 Computational Musicological Perspectives Computational musicological approaches help motivate or solve three general problems in the present thesis: 1) some basic mathematics illustrate and motivate the problem of melodic combinatorics and I discuss how corpus analysis can help deal with combinatoric problems, 2) computational melodic feature analysis helps solve the problem of item selection and 3) melodic similarity research helps solve the problem of score computation regarding the accuracy of participants recalling melodies by ear in relation to a target melody. In this section, I will explore these areas and their relevance to the present thesis. 4.1.1 Music and Combinatorics As discussed in the section on historical context, Nicolas Slonimsky derived his Thesaurus of Scales and Melodic Patterns. This was essentially an attempt to document and systematise the possibilities within the twelve-tone system to unlock new melodic vocabulary, which, was subseqently picked up by jazz improvisers such as John Coltrane to inspire melodic invention. Slonimsky undertook this task manually via a system of dividing the octave into parts and interpolating intervening notes, among other various algorithmic procedures. This process of enumeration leads to a more general issue around combinatorics and musical, melodic, material. This problem of enumeration is much easier to solve computationally, up to a point, where computational tractability diminishes. In general, the number of melodic possibilities, given a Let: \\(n\\): number of intervals (length of the n-gram) \\(I = \\{1, 2, \\ldots, h\\}\\): the set of possible intervals, where \\(h\\) is the highest interval (e.g., 12, for an octave) Then the set of all melodic n-grams is: \\[ \\text{Combos} = I^n = \\{ (i_1, i_2, \\ldots, i_n) \\mid i_j \\in I \\text{ for } j = 1, \\ldots, n \\} \\] and the number of combinations is: \\[ \\text{Number of combinations} = |I^n| = h^n \\] Where: \\(|I^n|\\) is the number of elements in the Cartesian product \\(I^n\\) \\(h^n\\) means you are choosing one of \\(h\\) intervals at each of the \\(n\\) positions Already for even the relatively short melody length of nine notes (eight intervals), with a maximum interval semitone size of 12 (i.e., an octave), 429,981,696 melodies are possible and the computational feasibility of computing all such melodies is relatively limited on an ordinary modern day laptop. This suggests a few points: 1) Slonimsky’s thesaurus of fewer than 1,500 was by no means exhaustive; 2) up until a point, such a task is much easier with a computer, with a few lines of code; 3) this “melodyverse” ensures that composers and improvisers will never cease to find new melodic sounds, which is great from a creative point of view. From a cognitive point of view, this may lead the musician learning to play by ear to be nervous: how can one select from the melodyverse, or indeed, a small, but still overwhelming version of it, like Slonimsky’s Thesaurus. Other than the general approach explicated in this thesis, there are several reasons to reserve dauntedness. First, in practice, the combinations of twelve tones are usually constrained in various ways: the first level of constraint in Western music are usually to bound note choices within the seven-note major or minor scales, or subsets of these “tonal” scales, like the pentatonic scale. However, the use of chromaticism within Western harmony re-opens the melodic space again. Additionally, it may not be necessary to have encountered and practised a specific melodic combination in one’s life study history to successfully play it by ear. It is more likely that some level of abstraction develops such that previous learning (the training set) can still generalise to a wider test set; notions important to (artifical) intelligence. Therefore, it is conceivable, that if one chose training data intelligently, one’s playing by ear abilities could project to a much wider space, eventually. This is the goal of this thesis: to constantly find the parts of the melodic space that can open a learner’s ear to possibilities they could not previously hear, be that at a very beginner, or the level of Coltrane. There must be a general solution to this problem. 4.1.2 Music Corpora Analysis (Müllensiefen and Frieler n.d.) 4.1.3 Melodic Represention and Computational Melodic Features A melody can be represented symbolically, in a way that makes computational analysis, in a number of ways. A simple convention, is to use MIDI notation (rothsteinMIDIComprehensiveIntroduction1996?), whereby pitches are represented as integers, with e.g., C4 being represented by 60, Db4 by 61 etc. For durations, millsecond or second values can be used. This is a basic low-level representation, which may be suitable for simple playback. However, this precludes many other possible higher-level musical representations, such as those to do with phrases, meter, not to mention articulation etc. Conklin and Witten (1995) proposed the Multiple Viewpoint System (MVS) as a cognitively motivated framework for modelling musical expectation and structure. Acknowledging that human listeners attend to a range of simultaneous musical cues—such as pitch, rhythm, contour, and phrasing—the MVS represents each musical event through a set of distinct viewpoints, each capturing a different musically salient abstraction (e.g., melodic interval, position within the bar, or phrase boundaries). The system incorporates both long-term statistical knowledge (learned from a corpus of musical works) and short-term contextual information (specific to the piece in progress). This reflects the dual influence of stylistic familiarity and local pattern recognition in human musical cognition. Each viewpoint is modelled as an independent probabilistic context model, and their predictions are combined to estimate the probability of upcoming events. Predictive performance is evaluated using entropy, with lower entropy interpreted as greater predictability. The authors argue that low-entropy predictions align with cognitive plausibility, positioning entropy as a proxy for musical expectation. In empirical tests on Bach chorale melodies, the model achieved prediction accuracy close to that of human listeners. The multiple viewpoint system thus offers a compelling computational account of how listeners may internally represent and anticipate musical structure by integrating multiple concurrent sources of musical information. In general, See Frieler (2018) for a thorough description of monophonic melodic representation in music. 4.1.3.1 Melodic Feature Analysis Conversely, for melodic items with multiple notes, musical features emerge (e.g., tonality, contour, rhythm). Such emergent features clearly rely on high-level mental representations and templates (i.e., musical knowledge). Consequently, there can be significant variance in complexity when a melody is the item of testing, and these kind of item difficulties are important to model. Important melodic representations can be quantified for each melodic item across important dimensions (mullensiefenFANTASTICFeatureANalysis2009?). As suggested by previous literature (Baker 2019; dreyfusRecognitionLeitmotivesRichard2016?; harrisonModellingMelodicDiscrimination2016?), there are several melodic features which could indicate an item’s complexity and predict singing performance (e.g., tonality, interval contour, a melody’s frequency in occurrence). 4.1.4 Computed Melodic Features as a Musicological Basis for Selecting Items Highlighting the intrinsic link between melodic features and human cognition, Herborn (2022) suggested the notion of “composed melodies as applied music psychology”. 4.1.5 Pattern Books as Item Banks The book has at least 1,400 discrete items and some of the patterns are terrifically long and complicated. Additionally, nearly all of the melodies are presented from a single note, C, and could be transposed to start on the other 11 chromatic tones. This means there are at least 16,800 (12 * 1400) items to be learned. The author’s saxophone tutor, who was a virtuoso, once said, “Oh, don’t bother trying to learn [Slonimsky’s Thesaurus of Scales and Melodic Patterns] systematically”. This was a wise comment since, as there is so much information, it is almost impossible for a human to comprehend and track their learning of it systematically. However, with advances in computation, this is a relatively easy challenge to solve once the data is in digital form. The author entered the Slonimsky’s Thesaurus corpus into his computer to provide a digital representation. This corpus will remain the test case for description in this document later on. 4.2 Melodic Similarity In the scientific area that has been termed Music Information Retrieval (Downie 2003), and that has seen a large boost in recent years, several approaches to similarity measurement for melodies and other musical objects have been developed (e.g., Müllensiefen and Frieler 2004a; Pearce and Müllensiefen 2017; Typke, Wiering, and Veltkamp 2007; Yuan et al. 2020). The similarity measures employed in the present dissertation are favoured because they proved their effectiveness and ecological validity (or rather comparability) with the notion of melodic similarity of musically experienced participants in separate studies (Müllensiefen and Pendzich 2009; Müllensiefen and Frieler 2004b, 2007). Therefore, while there still might not be an undisputed theory of melodic identity, as Sloboda and Parker claimed in 1985, this study will use some algorithms that at least came quite close in emulating musically experienced participants’ similarity judgements. However, whilst measures of similarity have been validated in the context of human perceptual judgements (e.g., to predict court case outcomes), we are not aware that they have been profiled in the context of sung recall, as we intend to do here. 4.2.1 Methodological Background Having obtained numerical representations of both sung recalls and the target melodies, the similarities between a target melody and sung recall of that melody, for each attempt of each participant, can be calculated using the algorithmic similarity measures described in Müllensiefen and Frieler (2004b). The similarity measures employed here comply with the two main points already raised by Sloboda and Parker (1985) in their discussion of their methodology of melodic comparison: in most cases - that is especially true for the earlier trials - participants only recall a smaller part of the original melody, which may not even start with the beginning of the original. Thus, a similarity measure (or algorithm) must be chosen that automatically looks for the best alignment of the (short) melodic sequence of the sung recall with the original melodic sequence. Sloboda and Parker (1985) manually attempted a form of alignment, making it a cumbersome task, but additionally, their method is not precisely described. To advance on this point, we detail a precise computational method. An algorithm for the optimal alignment of two symbol sequences that has been widely used in domains such as text retrieval or bio-computing, as well as music information retrieval, is the so-called Edit Distance or Levenshtein distance (e.g., mongeauComparisonMusicalSequences1990?). The Edit Distance is the minimum number of operations it takes to transform one symbol string into another: the possible operations being insertion, deletion, and substitution. The actual calculation of the Edit Distance is carried out using dynamic programming and is not explained here. For a general reference regarding the algorithm see e.g., (gusfieldAlgorithmsStringsTrees1997?). In this case, the maximal Edit Distance of two strings is equal to the length of the longer string. To convert the Edit Distance into a similarity measure with a range of values \\([0,1]\\) we use the following: \\[\\begin{equation} \\sigma(s,t)=1-\\frac{d_e (s,t)}{\\max(|s|,|t|)} \\tag{4.1} \\end{equation}\\] where \\(|s|\\) and \\(|t|\\) denote the element counts of strings \\(s\\) and \\(t\\) respectively, and \\(d_e(s,t)\\) stands for the Edit Distance between strings \\(s\\) and \\(t\\). Just like the manual scoring techniques employed by Sloboda and Parker (1985), the edit distance calculates the similarity between two symbolic sequences by taking the number of edits (i.e. additions, deletions or substitutions) that are necessary to transform one of the sequences into the other, and dividing this figure by the number of symbols in the longer sequence. It thus could be argued that Sloboda and Parker intuitively used a version of the edit distance, evaluating the similarity between the recalls of their participants on the original melody, keeping the order of notes in mind. However, importantly, instead of applying the edit distance to raw note values, here the edit distance is computed on various symbolic representations of musical dimensions (i.e., relative pitch sequences - intervals - as opposed to single pitches; rhythm sequences; and sequences of implied harmonies; Müllensiefen and Frieler (2004b)). Specifically, we employ the opti3 measure of melodic similarity (Müllensiefen and Frieler 2004b) as our main dependent variable. opti3 is a hybrid measure derived from the weighted sum of three individual measures which represent different aspects of melodic similarity. The similarity in interval content is captured by the ngrukkon measure is based on the Ukkonen Measure that measures the difference of the occurrence frequencies of interval trigrams (\\(\\tau\\)) contained within the target melody (\\(f_s (\\tau)\\)) and the comparison melody (\\(f_s (\\tau)\\)) (see (uitdenbogerdMusicInformationRetrieval2002?) Formally: \\[\\begin{equation} u(s,t)=\\sum_{\\tau \\in s_n \\cup t_n} |f_s (\\tau)-f_t(\\tau)| \\tag{4.2} \\end{equation}\\] As the Ukkonen Measure is a distance measure in its original definition, we normalise by the maximum possible number of \\(n\\)-grams and subtract the result from 1: \\[\\begin{equation} \\sigma (s,t) = 1 - \\frac{u(s,t)}{|s|+|t|-2(n-1)} \\tag{4.3} \\end{equation}\\] Note that the Ukkonen measure is not based on the edit distance but still takes order of notes into account at a local level by comparing trigrams of pitch intervals. Harmonic similarity is measured by the harmcore measure. This measure is based on the chords implied by a melodic sequence, taking pitches and durations (i.e., segmentation) into account. Implied harmonies are computed using the Krumhansl-Schmuckler algorithm (Krumhansl 1990) and the harmonic progression of the two melodies are compared by computing the number of operations necessary to transform one harmonic progression into the other sequence via the edit distance. Finally, likewise, rhythmic similarity is computed by first categorizing the durations of the notes of both melodies (known as “fuzzification”) and then applying the edit distance to measure the distance between the two sequences of categorised durations. The resulting measure of rhythmic similarity is called rhythfuzz (Müllensiefen and Frieler 2004b). Note that rhythfuzz does not take metric information into account and works solely based on (relative) note durations. Similarly, ngrukkon works with interval information and is hence invariant to transposition. Based on the perceptual data collected by Müllensiefen and Frieler (2004b), the three individual measures are weighted and combined to form a single aggregate measure of melodic similarity, opti3. Hence, opti3 is sensitive to similarities and differences in three important aspects of melodic perception (pitch intervals, harmony, rhythm). We note that all three individual measures (ngrukkon, harmcore, rhythfuzz) can take values between 0 (= no similarity) and 1 (= identity) and are length-normalized by considering the number of elements of the longer melody. opti3 then comprises (Müllensiefen and Frieler 2004b): \\[\\begin{equation} {opti3}= 0.505 \\cdot \\mathtt{ngrukkon} + 0.417 \\cdot \\mathtt{rhythfuzz} + 0.24 \\cdot \\mathtt{harmcore} -0.146 \\tag{4.4} \\end{equation}\\] where we here present the normalised weights, which constrain the values to be [0,1]. Beyond the target or comparison melody lengths being used to normalise the opti3 score, we note that opti3 is dependent on the length of the two comparison melodies further in only a soft sense, which is particularly relevant to Experiment 2 of this paper, where we use the sung recall attempt length as an auxiliary dependent variable. If one melody is shorter than the other, at least some of the melodic identity is destroyed: necessarily, the rhythmic (rhythfuzz) and intervallic (ngrukkon) components, but not necessarily the harmonic (harmcore) component. It should be clear that opti3 captures far more (musical) information than melody length(s) alone and/or accuracy-style measures. The ecological validity of the aggregate similarity measure has been established in several perceptual experiments (Müllensiefen and Pendzich 2009; Yuan et al. 2020). See Appendix B for concise descriptions comparing the similarity measures. Moreover, to build an intuition on how similarity measures change in relation to attempt, see Appendix C for notated examples of development in sung recall performance and a qualitative description of their change in similarity. In summary, similarity measures pay attention to musical features that arise from the relationships between pitch and rhythmic values and could be considered more “global” in nature. Conversely, accuracy measures which count notes or even intervals (bigrams), do not respect the higher order emergent properties of musical features. Consequently, aggregate similarity measures have a greater ability to represent perceptual properties relevant in human cognition and represent a robust step towards computationally representing a notion of melodic identity. In this way, similarity algorithms have been used to predict subjective similarity judgements, for example, in musical plagiarism court cases, with excellent success (Yuan et al. 2020; Müllensiefen and Pendzich 2009). As an aid in developing an intuitive understanding of the different properties arising from scoring melodic recall data on accuracy-style vs. similarity measures, see Appendix A2 for simple example comparisons. The usage of melodic similarity metrics to score sung recall data has been previously documented by the present author (Silas and Müllensiefen 2023; Silas, Müllensiefen, and Kopiez 2023). 4.2.1.1 Selecting Melodic Material as an Optimisation Problem Having motivated this work in terms of historical context up to contemporary technological approaches to improving playing by ear skills, I will now turn to motivating it in terms of the more specific epistemological approaches taken to assess this question in this dissertation. These approaches are: systematic musicological, cognitive psychological, and computational. In general, these approaches come from the scientific approach, whereas the content reviewed in the last chapter was largely not based on such an approach. 4.3 Cognitive Psychology To support the second main motivation of this project: to build an item selection mechanism for playing by ear items, it is necessary to review literature regarding learning and memory more generally, and where more extensive research has been undertaken into finding items for people to learn as well as review. 4.3.0.1 Theoretical Background Learning and memory are fundamental cognitive processes that allow us to acquire new knowledge and retain it over time. Psychologists have long sought to understand how we learn and why we forget. Early experimental work established basic patterns of memory retention and loss that remain influential today. For example, pioneering studies by Hermann Ebbinghaus in the 1880s laid the groundwork for formal memory research by charting how quickly information is forgotten (Cepeda et al. 2008; Ebbinghaus 1888). Since then, cognitive psychology has built a rich literature on the principles underlying learning and forgetting. This review provides an overview of general principles of learning and memory from a cognitive perspective, with an emphasis on how these principles manifest in both short-term and long-term memory. We will discuss historical findings (such as Ebbinghaus’s forgetting curve and Jenkins &amp; Dallenbach’s work on memory and sleep), examine the phenomena of learning and forgetting curves in different time scales, and explore the effectiveness of spaced repetition as a learning technique. We also highlight a recent computational model of memory – the DASH model (Difficulty, Ability, and Study History) by Mozer and Lindsey (2016) – and discuss how these cognitive principles are applied in modern language learning approaches and apps. Throughout, we ground the review in peer-reviewed findings to ensure academic rigor while keeping the explanations accessible to a general audience. 4.3.0.1.1 Learning and Forgetting Curves One of the core findings in cognitive psychology is that both learning and forgetting tend to follow systematic, nonlinear patterns over time. A learning curve describes how performance or recall improves with repeated practice. Typically, learning is rapid initially, then the rate of improvement slows as one approaches mastery – a phenomenon often described as a negatively accelerated curve (early gains followed by plateau). Conversely, a forgetting curve describes how retained information is lost over time when no further practice occurs. Hermann Ebbinghaus was the first to quantitatively measure a forgetting curve. Using himself as a subject, Ebbinghaus memorized lists of nonsense syllables and tested his memory at various delays, from 20 minutes to 31 days. He found that forgetting occurs rapidly at first – a large proportion of the information is lost soon after learning – and then the rate of forgetting gradually slows down (Cepeda et al. 2008). In other words, memory retention drops steeply in the first hours or day, then levels off, never quite reaching zero. This characteristic shape has been replicated in modern studies. For instance, a 2015 replication of Ebbinghaus’s experiment confirmed the same pattern of a sharp initial decline followed by a more gradual loss, validating that the “forgetting curve” is a robust phenomenon (Murre and Dros 2015). Theoretical analyses of forgetting curves suggest that memory decay is not linear but follows a curve that can often be described by a mathematical function (historically debated as exponential or power-law in form) (Cepeda et al. 2008). The key practical implication is that most forgetting happens early, and what remains eventually stabilizes (information that survives the initial rapid forgetting may stay in memory for a long time). 4.3.0.1.2 Short-Term vs. Long-Term Forgetting The time scale of retention is crucial. In short-term memory, forgetting can occur on the order of seconds when rehearsal is prevented. Classic experiments by Peterson and Peterson (1959) demonstrated that without active rehearsal, even a simple piece of information (such as a trigram of letters) fades quickly from short-term memory. In their study, participants were asked to remember a three-letter sequence, then were prevented from rehearsing it by performing an intervening task (counting backwards) for a variable duration. The results showed a dramatic drop in recall within half a minute – after just 15–18 seconds of distraction, memory for the letters was almost completely gone (on the order of only ~5–10% of trigrams recalled correctly). This rapid loss highlighted the very limited duration of short-term memory when maintenance via rehearsal is blocked. Long-term memory, on the other hand, operates over much longer intervals and is subject to different influences. Forgetting in long-term memory occurs more slowly and is heavily affected by factors such as meaningfulness of the material, repeated exposure, and interference from other learning. While time itself does contribute to forgetting, the role of interference is particularly evident in long-term retention. A famous study by Jenkins and Dallenbach (1924) provided early evidence that forgetting over several hours is not due solely to the passive decay of memory traces with time, but is exacerbated by waking activity (i.e. interference from new experiences). In this study, people learned nonsense syllables and were tested after a retention interval spent either sleeping or staying awake. The researchers found memory performance was significantly better after a period of sleep than after an equivalent period of wakefulness (Fenn and Hambrick 2013). During sleep, when little new information was encountered, participants forgot much less. The condition with wakefulness produced more forgetting, presumably because ongoing mental activities interfered with the previously learned syllables. Jenkins and Dallenbach concluded that interference plays a major role in forgetting: our memories fade faster when we are bombarded with new information (or other cognitive activity) in the interim. Later research on sleep and memory consolidation has reaffirmed that sleep can stabilise memories by protecting them against retroactive interference from new learning (Fenn and Hambrick 2013). In summary, short-term memory is fleeting unless actively maintained, and long-term memory, while more durable, is vulnerable to disruption by other experiences. These foundational insights set the stage for strategies to combat forgetting, such as managing interference and optimizing review timing. 4.3.0.2 Key Findings from Cognitive Psychology 4.3.0.2.1 Ebbinghaus’s Forgetting Curve Hermann Ebbinghaus’s work in the 1880s was groundbreaking as the first systematic, quantitative study of human memory. Using the method of savings (measuring how many fewer trials were needed to relearn material after a delay), Ebbinghaus plotted the decline in memory retention over time. His forgetting curve showed that memory loss is steep soon after learning – he famously noted that more than half of learned syllables could be forgotten within hours – but then the curve flattens out. For example, if 100% of information is remembered immediately after learning, a day later a much smaller fraction might be retained, and further losses after that point are slower. Ebbinghaus also observed that even when something seems forgotten, not all is necessarily lost; savings in relearning demonstrate that some residual memory can facilitate faster re-learning later. The broader significance of Ebbinghaus’s finding is that forgetting is systematic rather than random. The initial rapid drop suggests a need for review soon after learning, while the leveling off indicates a core of memory that can persist. Ebbinghaus’s methods were primitive by today’s standards (he was his own sole test subject), yet remarkably, his results and general conclusions have stood the test of time. Modern quantitative models still aim to describe the exact shape of the forgetting curve, with many agreeing that a negatively accelerated function (such as a power-law decay) best fits empirical data. For learners, an intuitive takeaway from the forgetting curve is the importance of timely reviews: without reinforcement, memories will decay fastest early on. 4.3.0.2.2 Interference and Consolidation Building on Ebbinghaus’s insights, later researchers explored why memories fail. The concept of interference became central to theories of forgetting. Proactive interference (earlier learning disrupting new learning) and retroactive interference (new learning disrupting old memories) were identified as key processes that cause forgetting beyond the mere passage of time. Jenkins and Dallenbach’s 1924 experiment, discussed above, is an elegant demonstration of retroactive interference: when awake and taking in new input, the mind overwrites or obscures existing memories, whereas during sleep (with minimal new input) memory traces are better preserved (Fenn and Hambrick 2013). This finding hinted at the process of memory consolidation – the idea that recently formed memories gradually strengthen and become more stable (often thought to occur during sleep). Later research in physiological psychology confirmed that sleep has beneficial effects on memory consolidation at the neural level (e.g. through replay of learning during slow-wave sleep), complementing the cognitive account that sleep mainly shields against interference. Overall, a key principle is that not all forgetting is due to time; much forgetting is due to interactions between memories. This has practical implications: spacing out learning (as we will see) can reduce interference between items, and ensuring periods of rest (or sleep) after intense learning can improve retention. 4.3.0.2.3 Spacing Effect and Distributed Practice One of the most robust findings in cognitive psychology of learning is the spacing effect – the benefit of distributing study or practice sessions over time, rather than cramming in a single session. Interestingly, Ebbinghaus himself observed a precursor of the spacing effect. In one anecdotal report, Ebbinghaus noted that after “cramming” repetitions of a list in one day he could recall it, but achieving similar recall with practice spread over three days required only about half as many total repetitions (Settles and Meeder 2016). In other words, spaced practice was more efficient than massed practice. Subsequent research over the past century has thoroughly documented that spaced repetition leads to better long-term retention compared to massed learning (i.e. studying the same total amount in one lump). A comprehensive review by Cepeda et al. (2008) examined hundreds of studies and confirmed that spacing effects are reliable across a wide range of materials and contexts. Spacing improves memory for simple verbal facts, textbook material, motor skills, and more – it appears to be a general principle of learning. The size of the benefit is often substantial. For example, one experiment found that properly spacing study sessions nearly doubled the amount of information retained after a one-year delay, compared to a massed practice condition (same total study time) (Khajah, Lindsey, and Mozer 2014). This dramatic improvement occurs because spaced sessions allow some forgetting to happen between sessions, which actually forces more powerful relearning. Each time you retrieve or restudy the material after a delay, the memory trace is strengthened more than if you had restudied it immediately. In effect, spaced practice exploits the forgetting curve by scheduling reviews at the point when some forgetting has occurred but not too much – a strategy often summarized as reviewing “right before you would have forgotten”. Psychologists also describe a related finding, the lag effect, which indicates that spacing is even more effective when the intervals between study sessions gradually increase (for instance, first review after 1 day, next after 3 days, then 1 week, etc.) (Settles and Meeder 2016). This helps match the schedule to the slowing pace of forgetting as retention improves. Importantly, spacing benefits are not limited to rote memorization. Studies have shown improved learning of concepts and problem-solving skills under spaced practice, and field experiments have demonstrated that spacing can boost learning in real classroom settings (Cepeda et al. 2008). In one classroom study, spaced review led to higher test scores on a semester-end exam than a massed review schedule (Sense et al. 2019). The universality of the spacing effect has led researchers to conclude it is a “desirable difficulty” – a learning strategy that feels more challenging (because some forgetting happens between sessions) but yields superior long-term results (Bjork 1994). 4.3.0.2.4 Retrieval Practice Another principle often intertwined with spacing is the benefit of retrieval practice – actively recalling information (testing oneself) is more effective for memory than simply re-reading the material. While not the central focus of this review, it’s worth noting that much of the power of spaced repetition comes from the combination of spacing and retrieval. When learners practice retrieving information at spaced intervals, they harness two potent effects: the spacing effect and the testing effect. Research in cognitive psychology has articulated that a successful recall from memory solidifies knowledge more than passive exposure does (Chukharev-Hudilainen and Klepikova 2016). Moreover, recalling an item after some delay (when it’s a bit difficult to remember) has a bigger impact on retention than recalling it immediately after study. These findings underpin modern spaced repetition techniques, which typically involve attempting to retrieve the answer (as in flashcard quizzes) rather than simply reviewing the information. In summary, the key findings from over a century of research can be distilled into a few general principles: memory retention improves with appropriately spaced reviews; forgetting is mitigated when learning is spaced and when interference is minimized; and active retrieval of knowledge strongly reinforces memory. These insights have paved the way for practical methods to enhance learning. 4.3.0.2.5 Cognitive Models of Memory and Optimal Scheduling Beyond empirical findings, cognitive psychologists have developed theoretical models to explain learning and forgetting, and to predict the optimal timing of review. For instance, the ACT-R cognitive architecture (Adaptive Control of Thought—Rational) includes a mathematical formula for memory decay where each exposure adds to an item’s “activation” but that activation diminishes as a power function of time. Such models can capture phenomena like the spacing effect by adjusting activation based on practice history. A more recent advancement is the development of data-driven models that personalize spacing schedules for individual learners. Mozer and Lindsey (2016)’s DASH model is one prominent example. “DASH” stands for Difficulty, Ability, and Study History, reflecting the key factors the model takes into account (Sense et al. 2019). In a 2014 study, Lindsey et al. (2014) tested a personalized spacing algorithm based on a memory model in a real classroom (a semester-long Spanish course). Students used a computer-based flashcard system for vocabulary practice throughout the term. The researchers compared different scheduling algorithms and found that the personalized, model-driven spacing schedule (DASH) produced the highest end-of-semester exam scores, especially for material learned early in the course. The DASH algorithm works by estimating how well each student knows each item and how quickly that item will be forgotten, based on past study history and item difficulty. In essence, it uses a cognitive model of the forgetting curve for each item and each learner, then schedules reviews at the optimal time (just as the item is predicted to be on the verge of being forgotten). Mozer and Lindsey (2016) further elaborated this approach, emphasizing that integrating cognitive theory with “big data” from educational technology can greatly improve learning outcomes. They demonstrated that a theory-informed model like DASH could outperform generic one-size-fits-all spacing strategies. Simulations and experiments showed that taking into account individual differences in learning and forgetting led to more effective review schedules. For example, one student might need to see a particular vocabulary word five times with increasing intervals (1 day, 3 days, 1 week, 3 weeks, etc.), whereas another student might need fewer or more repetitions at different intervals, depending on how quickly they tend to forget that word. The model’s job is to tailor the schedule optimally. The success of DASH and similar models illustrates a general principle: optimal learning schedules should be adaptive. By using cognitive principles (like the forgetting curve and spacing effect) in a quantitative way, these models guide the timing of study to maximize retention. This line of research merges cognitive psychology with computational methods and forms the bridge to real-world applications in educational technology, such as intelligent tutoring systems and language learning apps. 4.3.0.3 Applications in Language Learning The principles of learning and memory described above have found practical application in many domains, but perhaps one of the most visible in everyday life is language learning. Mastering a new language requires acquiring a large amount of vocabulary and grammatical knowledge – a clear case where effective memory retention is crucial. It is no surprise that many language learning methods and apps explicitly incorporate cognitive psychology findings like spaced repetition to help users remember foreign words and phrases. Here we review how these principles are applied in language learning and assess their effectiveness from an academic standpoint. Spaced Repetition in Vocabulary Acquisition: Vocabulary learning has benefited greatly from spaced repetition techniques. Traditional flashcards have long been used by learners to drill word meanings; with the advent of computers and smartphones, flashcard programs can schedule reviews optimally using algorithms based on the spacing effect. One fundamental idea is that each word should be reviewed at increasing intervals – shortly after first learning it, then a bit later, then perhaps a day or two after, then a week, and so on. This schedule aligns with the psychological finding that an “expanding” review interval helps solidify memory (the lag effect) (Settles and Meeder 2016). Modern spaced repetition software, such as the popular Anki (CITE) or Memrise (CITE) platforms, implement this by tracking each item’s review history and success, and dynamically adjusting when it will next appear. For example, if you easily recall a word, the system might schedule the next review farther out; if you struggle or forget, the system will show it again sooner. This approach is a direct translation of the difficulty-sensitive spacing principle discussed earlier (and indeed echoes the DASH model’s focus on item difficulty and individual performance). Academic research strongly supports the effectiveness of such approaches for language learning. In a controlled study, Chukharev-Hudilainen and Klepikova (2016) tested a computer-based spaced repetition tool for English vocabulary training among foreign language learners. Over the course of a semester, students who used the tool for just a few minutes each day achieved significantly better long-term retention of the vocabulary than those who did not. In fact, the spaced repetition group’s vocabulary retention after a delay was roughly three times higher than that of the control group, despite the minimal daily time investment. This three-fold improvement is striking evidence that incorporating spacing into study can greatly enhance learning efficiency. Other studies have similarly found that spaced presentation of new words yields better memory than massed presentation. Classic research by Bahrick et al. (1993) showed that spreading foreign language word practice sessions across intervals as long as 56 days produced superior retention over a 5-year period, compared to shorter interval practice or cramming. The benefits of spacing are thus seen not only in short laboratory tests but in longitudinal retention of language knowledge. Language Learning Apps and Cognitive Principles: Popular language-learning applications like Duolingo, Babbel, and Memrise have brought these cognitive principles to millions of users. Duolingo, for instance, uses a gamified approach to teach languages, but behind the scenes it employs algorithms influenced by spaced repetition and learning curve models. The Duolingo research team developed a statistical model called “Half-Life Regression” to predict how long a learner will remember a word or grammar concept and when they will need to review it (Settles and Meeder 2016). The model’s name is inspired by the idea of a memory “half-life” – the time it takes for the probability of recalling an item to fall to 50%. By analysing the huge amount of data from its user base, Duolingo’s system can continuously adjust practice schedules for each learner. Empirical evaluation of this system showed tangible improvements: the half-life regression model was able to predict users’ recall accuracy much better (reducing prediction error by over 45% compared to simpler baselines) and, when deployed, it led to a 12% increase in user engagement with daily practice (Settles and Meeder 2016). The increased engagement likely results from users experiencing a manageable level of difficulty – not too easy (which can be boring) and not too hard (which can be discouraging) – by reviewing items at the right time. From an effectiveness standpoint, more engagement means more practice and thus better learning outcomes, a contention supported by the finding that those on optimized schedules retain more and stick with the program longer. Another app, Memrise, uses a similar spaced repetition strategy and often incorporates mnemonic aids. Babbel’s curriculum explicitly staggers review sessions for previously learned vocabulary at expanding intervals to reinforce retention. Overall, these apps are effectively applied cognitive psychology tools. They implement the spacing effect, interleaved practice, and sometimes retrieval practice (since users must actively recall translations) to maximize learning per unit time. Effectiveness and Academic Assessments: Do these approaches actually work in practice? Research so far indicates yes – when used properly, spaced repetition-based language learning can be highly effective. Learners can achieve substantial vocabulary gains through self-guided app practice. Studies comparing app-based learning to classroom instruction have found that dedicated app users can learn a volume of material in a few months that is comparable to one or two semesters of language class instruction. For example, an early independent study on Duolingo by Vesselinov and Grego (2012) estimated that 34 hours of Duolingo practice were equivalent to a semester of college language education in terms of reading and writing outcomes (though speaking skills were not as directly taught). From a cognitive perspective, this efficiency makes sense: the apps force learners to practice frequently, recall actively, and review content right as they are about to forget it – all conditions known to solidify memory. Another empirical evaluation of Babbel conducted by Loewen, Isbell, and Sporn (2020) found that users who engaged with the app over a 12-week period demonstrated significant gains in receptive linguistic knowledge, including vocabulary and grammar, as well as improvements in oral communicative ability in Spanish. Similarly, a 2019 study by Van Deusen-Scholl, Lubrano, and Sporn (2021) in collaboration with Babbel reported that learners who completed a structured set of lessons showed measurable enhancements in conversational skills, particularly in speaking and comprehension. These findings suggest that well-structured language learning apps, when used consistently, can effectively support the acquisition of core language competencies. While user motivation and engagement levels vary (and dropout rates can be high), these studies indicate that learners who adhere to the recommended schedules benefit from the apps’ built-in instructional design. Importantly, such platforms typically integrate spaced repetition with a variety of pedagogical strategies, including multimedia input, contextual learning, and retrieval-based exercises. This integration can make it difficult to isolate the unique contribution of spacing to learning outcomes. Nevertheless, given the substantial body of laboratory research demonstrating the robust benefits of spaced repetition, it is reasonable to conclude that its implementation plays a major role in the observed learning gains. Furthermore, language learning apps like Duolingo and Babbel now serve not only as educational tools but also as research platforms. Large-scale analyses of user data have yielded novel insights into learning processes—for instance, identifying which word types tend to be forgotten more easily, or how factors such as native language and lexical similarity influence learning trajectories. This iterative feedback loop—where cognitive science informs app design, and app usage informs cognitive science—represents a compelling example of applied psychological research in action. In summary, modern language learning applications leverage foundational cognitive principles such as spaced practice, repeated retrieval, and interference management to support durable knowledge acquisition. Spaced repetition, in particular, emerges as a common and effective mechanism across successful programs. Empirical assessments confirm that these strategies improve long-term retention more reliably than massed learning or unsystematic study. By embedding these methods in engaging, user-friendly formats, educational apps have made research-backed learning strategies accessible to a broad public. Learners today benefit from sophisticated, adaptive scheduling algorithms that embody over a century of cognitive psychology research—turning the science of memory into everyday practice. 4.3.1 Conclusion: Cognitive Psychological Background Understanding the general principles of learning and memory has been a central quest in cognitive psychology, from the early experiments of Ebbinghaus to the sophisticated computational models of today. This literature review has highlighted several key concepts: the predictable shape of forgetting and learning curves, the crucial influence of factors like interference and repetition on memory retention, and the immense benefits of spaced practice for durable learning. Foundational studies provided simple but powerful lessons – memory fades quickly without reinforcement, and the context of learning (such as being asleep or awake, or spacing out practice) can dramatically change how much we retain. Building on those insights, modern researchers have developed strategies and models (e.g., the DASH model of Mozer &amp; Lindsey) to optimize learning by tailoring review schedules to the learner’s needs. These advances are not only theoretical; they have been translated into practice in educational technology. Language learning, a field heavily dependent on memorization, has been a prime beneficiary of this translational research. Techniques like spaced repetition and adaptive recall practice, grounded in cognitive theory, demonstrably improve language learners’ outcomes, making the process more efficient and effective (Chukharev-Hudilainen and Klepikova 2016; Settles and Meeder 2016). In essence, cognitive psychology has revealed that learning is not just about how much time we spend or how motivated we are, but also when and how we study. Timing matters: reviewing information at strategically spaced intervals can cement knowledge in a way that marathon cramming never can. Method matters: actively recalling and engaging with the material strengthens memory more than passive exposure. These general principles apply broadly – whether one is trying to remember a list of words, understand a scientific concept, or practice a musical instrument. By applying these principles, educators and learners can significantly boost memory performance. The success of spaced repetition in both lab and real-world settings underscores a hopeful message: forgetting is natural, but it is not inevitable – with the right techniques, we can greatly extend the longevity of our memories. Looking ahead, the synergy between cognitive research and educational application is likely to grow. As we collect more data from learning apps and classroom interventions, our models of memory will become more refined, and in turn those models will power more effective learning tools. This ongoing dialogue between theory and practice was anticipated by pioneers like Ebbinghaus, who sought general laws of memory, and it continues in contemporary efforts to personalize and optimize learning. For the general population, the takeaway is clear: strategies based on cognitive psychology (like spacing your study, self-testing, and giving yourself time to forget and relearn) can make a tremendous difference in how well you learn and remember. Harnessing these principles can turn the ebbing tides of forgetting into lasting knowledge. In conclusion, the marriage of time-tested cognitive principles with modern technology offers an exciting path forward to help people learn better and remember longer – fulfilling, in a very practical way, the early psychologists’ quest to understand and improve human memory. 4.4 Statistical Modelling Frameworks 4.4.1 Cognitive Modelling Item Feature Modelling via Item Response Theory Performance on an ability test can vary as a function of individual differences (i.e., some participants have a higher ability than others), but also as a function of items themselves (i.e., some items may be more difficult than others). As previously discussed, quantitative features can be computed for melodies across various domains (e.g., tonality, contour, rhythm). Such emergent features clearly rely on high-level mental representations and templates (i.e., musical knowledge) and thus should predict performance. Consequently, there can be significant variance in complexity when a melody is the item of testing, and these kind of item difficulties are important to model. We here compute melodic representations can be quantified for each melodic item across important dimensions (mullensiefenFANTASTICFeatureANalysis2009?). In order to formally relate structural features of melodies to the cognitive difficulty of melody processing, the main methodological approach we utilise here is explanatory item-response theory (IRT; De Boeck, Cho, and Wilson (2016)). In this paper, IRT can be considered our first level of modelling, where melodic features become predictors of the opti3 similarity score, which we take as representing variance in both singing accuracy and melodic memory. IRT is useful for our enquiry since it allows the simultaneous modelling of item difficulties and individual differences together via mixed effects modelling, whilst compartmentalising the variance into fixed item effects (melodic features), random item effects (unexplained effects due to melodic items) and participant effects (effects due to individual participants’ abilities). Additionally, an IRT model can be the basis of creating an adaptive test, which is highly efficient and can be variable in test length, since encoding relationships between item features and performance can be used to generate or select items based on modeled difficulties (for similar approaches see geldingEfficientAdaptiveTest2021?; harrisonApplyingModernPsychometric2017?; harrisonDevelopmentValidationComputerised2018?; harrisonModellingMelodicDiscrimination2016?; tsigemanJackJillAdaptive2022?). Such an adaptive test can hence be employed flexibly, with potential applications in education. In this paper, our strategy to relate singing accuracy to melodic memory is to extract participant and item level scores from our IRT mixed effects models and use these outputs in further modelling. For instance, we use participant-level scores to represent individual differences in overall melodic memory and singing ability, and participant level indicators of singing accuracy alone (comprising e.g., single long note singing, singing accuracy, precision), to predict such outputs. This allows us to evaluate the potential extent that low-level singing abilities are responsible for the overall variance in singing performance, leaving the rest to do with variance in melodic memory, or being unexplained. As suggested by previous literature in (Baker 2019; dreyfusRecognitionLeitmotivesRichard2016?; harrisonModellingMelodicDiscrimination2016?; Silas and Müllensiefen 2023; Silas, Müllensiefen, and Kopiez 2023), there are several melodic features which could indicate an item’s complexity and predict playing by ear performance performance (e.g., tonality, interval contour, a melody’s frequency in occurrence). "],["theoretical-model.html", "5 Playing By Ear: A Theoretical Model 5.1 Bringing It Together 5.2 Moving from Theoretical to Computational 5.3 Mathematical Representation 5.4 Algorithm Flow 5.5 Final Equation 5.6 Strategies 5.7 The Melodic Mind as Item Bank 5.8 Predictions", " 5 Playing By Ear: A Theoretical Model As far as the author is aware, no computational model of playing by ear has been described in the music psychology, or related, literature(s). Thus, a first step in this thesis is to propose a theoretical model, which can then later be formulated more precisely as a computational model, then tested, partly with data collected throughout this thesis. However, related computational models have been described, for instance in the domain of aural skill acquisition (Baker 2019) and sung melodic recall by the author (Silas and Müllensiefen 2023; Silas, Müllensiefen, and Kopiez 2023). Theoretical cognitive models have also been described in the domain of singing accuracy (Pfordresher et al. 2015). Therefore, in this section, I will attempt to synthesise this literature to propose a corresponding model specifically for playing by ear skills. I distinguish between two basic playing by ear processes: a) playing by ear skill acquisition (learning) and b) playing by ear skill execution (recall). 5.1 Bringing It Together In sum, then, 5.2 Moving from Theoretical to Computational “When we develop a psychological theory that is sufficiently precise to be implemented as a computer program, we call it a computational model. The process of developing such theories, and implementing their corresponding computer programs, is then called computational modelling.” (Harrison 2025). At this point, I already suggest a more specific feature to the model: that for playing by ear tasks, there exists a mental melodic “similarity” algorithm, housed in working memory. The input to this module is the target melody and the evolving real-time assessment of the melody being produced in the moment. The performer must hear the target melody, store this in short term memory as a reference, and then continuously update the target melody which each new note that is produced. I speculate that the mind recomputes the similarity between the presently number of recalled notes and the target melody iteratively, each time a new note is added. In other words, a check for the accuracy is made every time a note is recalled, and the presence of an error and its nature, will inform whether or not the performer stops to start again or continues to proceed recalling the melody, at least in the stage of learning. Formally: 5.2.1 Inputs \\(T = [t_1, t_2, \\dots, t_n]\\): TargetMelody of fixed length \\(n\\). \\(R = [r_1, r_2, \\dots, r_k]\\): RecalledMelody of length \\(k\\), unknown a priori. 5.2.2 Sequential Construction \\(T\\) is fixed and built one note at a time. \\(R\\) is sequentially constructed one note at a time. 5.2.3 Function Application At each step \\(i\\) (current length of \\(R\\)), the function MelSim is applied to TargetMelody and the current length of RecalledMelody. 5.3 Mathematical Representation Let: - \\(T_i\\) be the prefix of the TargetMelody up to length \\(i\\), i.e., \\(T_i = [t_1, t_2, \\dots, t_i]\\). - \\(R_i\\) be the current state of the RecalledMelody of length \\(i\\), i.e., \\(R_i = [r_1, r_2, \\dots, r_i]\\). - \\(\\text{MelSim}(T_i, R_i)\\) be the similarity function between the two melodies at length \\(i\\). The MelSim function can be formulated as: \\[ \\text{MelSim}(T_i, R_i) = f(T_i, R_i) \\] Where: - \\(f\\) is a similarity measure (e.g., distance function, alignment score). - \\(i\\) iterates from 1 to \\(n\\), where \\(n\\) is the fixed length of the target melody. 5.4 Algorithm Flow Start with an empty recalled melody \\(R\\). For each new note \\(r_i\\) added to \\(R\\): Compute the similarity: \\(\\text{MelSim}(T_i, R_i)\\) if \\(i = n\\). Repeat until the entire recalled melody is constructed. 5.5 Final Equation The similarity calculation is performed only when the length of the RecalledMelody equals the fixed length of the TargetMelody. Therefore, the final equation is expressed as: \\[ \\text{MelSim}(T, R_n) = f(T, R_n) \\] I note that the MelSim algorithm is itself modular and of discussion in and of itself (cite M&amp;F). For the purpose of this paper, 5.6 Strategies Beyond scope of this dissertation to investigate in detail. Pop strategies hypotheses: (Liscio and Brown 2024) Also, we have our own text data, for later. 5.7 The Melodic Mind as Item Bank similar to Haston and McPherson (2022)’s concept of an “aural database” Big concept 5.8 Predictions Simpler melodies will rely more on general working memory than musical working memory. In the short term (&lt; 30 mins), melodies learned by sight will be less better retained than those "],["computational-ecosystem.html", "6 A Computational Ecosystem 6.1 Transcription of Melodic Production Data: pyin", " 6 A Computational Ecosystem # NB: knitr::include_app !!! To investigate computational questions related to playing by ear, an appropriate computational architecture is required. Whilst many musicological and psychological tools exist, as far as I am aware, there is no open source academic framework that provides an end-to-end solution for recording produced musical data, transcribing audio into useful symbolic representations, and assessing these representations within a scientific statistical modelling framework - all in real-time. However, it would be far beyond the scope of any individual to produce all the required components, each themselves requiring considerable research. Indeed, we stand on the shoulders of giants. In that respect, my work here has mainly been to find ways to combine the already existing tools into a framework - to get them to interact with one another - and create useful data structures to store relevant information and make it readily usable in web applications. To not detract from the theoretical questions in this manuscript, I will only give a brief tour of this architecture here. However, please see the documentation for a more thorough treatment. I will start with the low-level packages and work my way up to the higher level ones. 6.1 Transcription of Melodic Production Data: pyin To be able to assess music production data, it is first necessary to transcribe "],["pyin-r-package.html", "7 pYIN R package 7.1 Installation 7.2 Usage 7.3 Notes 7.4 References 7.5 Assessment of Melodic Similarity: melsim 7.6 Psychologically Meaningful Musical Item Banks: itembankr 7.7 References 7.8 musicassessrdb 7.9 Datasets (created with itembankr) 7.10 References 7.11 References 7.12 Other useful functions", " 7 pYIN R package pyin is an R package which wraps the pYIN algorithm (Mauch &amp; Dixon, 2014) for fundamental frequency estimation via Sonic Annotator (Cannam, Jewell, Rhodes, Sandler &amp; d’Invernoand, 2010) for use in the R environment. 7.1 Installation install.packages(&quot;devtools&quot;) devtools::install_github(&quot;sebsilas/pyin&quot;) 7.2 Usage library(pyin) # First test using our test function: test &lt;- test_pyin() # If the pyin setup was successful on your system, &#39;test&#39; should contain a 10x5 data with the transcribed note events of a demo audio file we have distributed with the package. # Try your own audio file: my_audio_transcription &lt;- pyin(&#39;/my/file/path/audio.wav&#39;) # If you install the itembankr package, you can compute extra melodic features on this: devtools::install_github(&#39;sebsilas/itembankr&#39;) library(dplyr) library(itembankr) my_audio_transcription %&gt;% produce_extra_melodic_features() # where my_audio_transcription contains a pYIN result. 7.3 Notes See https://vamp-plugins.org/sonic-annotator/ for information about allowed file types etc. It is possible to also supply transform files, as described there, via the R package version of pYIN (see the transform_file argument) 7.3.1 Compatability This R package currently only supports Windows and Mac 64-bit. If you require support for other systems, please get in touch. 7.4 References Cannam, C., Jewell, M. O., Rhodes, C., Sandler, M., &amp; d’Inverno, M. (2010). Linked Data And You: Bringing music research software into the Semantic Web. Journal of New Music Research, 39(4), 313–325. Mauch, M., &amp; Dixon, S. (2014). PYIN: a fundamental frequency estimator using probabilistic threshold distributions. Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2014). 7.5 Assessment of Melodic Similarity: melsim # TODO: Also on repo 7.6 Psychologically Meaningful Musical Item Banks: itembankr In my literature review, I noted that pattern books and melodic repositories have emerged to support jazz improvisation and acquiring inner melodic representations. Such books present itemised melodic patterns to stimulate compositional ideas or so that training musicians can practise acquiring cognitive representations of new melodic material (Lateef, 2015; Weiskopf, 2015; Slonimsky, 1947; Ricker, 1999). Such books are essentially corpora or item banks of melodic grammar and possibilities. As well as improving one’s melodic “vocabulary”, such books can be used to develop technical prowess, by challenging the player to acquire new action patterns to perform the items. However, often these volumes are just repositories of melodies and lack a systematic structure, or if they do, the structure is not built upon robust cognitive psychological principles tha can advance the objective of efficient learning. This is also a technological issue: moreover, books are static and not able to break down items into smaller pieces and consider the relationship between items. By not being structured in a way that makes them optimised for memorisation, such books are limited. Typically, contents will be structured into discrete items where each item represents a particular melodic sequence. However, each of these items can be broken into smaller contiguous subsets known as N-grams (Damashek, 1995) or “chunks”. Such componentisation makes items more readily memorable since sequence length is a major predictor of how memorable an item is (Cowan, 2010; Miller, 1956). The implication is that, by first learning smaller chunks and then increasing the size of chunks, one can optimise the learning process (Lehmann &amp; Kopiez, 2011). To undertake such a task manually is more or less impossible as it would require an individual to comprehend a combinatorial explosion of relationships between the items in a given corpus (intra-corpus). Moreover, different corpora will also share identical and similar chunks, so learning one chunk may support learning other chunks inter-corpuses too. It would be redundant to treat these as separate chunks (except to count chunks by frequency, another potential indicator of its usefulness) and would instead be more efficient to have a centralised system which can consider information inter-corpora. itembankr is an R package for creating useful item banks from raw, relatively unstructured data. These item banks are intended mainly to be used in psychological testing but can also be used for corpus analyses. The package is currently focused on musical, and particularly monophonic, melodic stimuli. For convenience, we refer to a “corpus” as some raw input data, not in itself particularly useful in psychological testing, and an “item bank” as the useful output, which is typically a set of dataframes with the stimuli from the corpus organised in a useful way (and some new computed melodic representations). Hence, the main function in itembankr is create_item_bank: # Create an item bank from MIDI and/or musicxml files create_item_bank(name = &#39;Berkowitz&#39;, midi_file_dir = &#39;berkowitz_midi_rhythmic_100bpm&#39;, musicxml_file_dir = &#39;berkowitz_musicxml&#39;) You can also create an item bank simply by inputting MIDI notes and durations directly to the input_df argument via an R tibble/dataframe. Minimally, you need abs_melody and durations columns, e.g,: test_item_bank &lt;- tibble::tribble( ~abs_melody, ~durations, &quot;61,62,63,64,65,66&quot;, &quot;1,2,3,4,5,6&quot;, &quot;72, 73, 75, 78&quot;, &quot;1, 1, 1, 1&quot; ) create_item_bank(name = &quot;Test&quot;, input_df = test_item_bank) The output of create_item_bank is one or more .rda files, depending on what you requested as output. You can read these into R like this: # A fairly limited item bank consisting of just file references: load(&#39;Berkowitz_file.rda&#39;) Berkowitz_file &lt;- item_bank # (we rename the object from &quot;item_bank&quot;) rm(item_bank) # A database of the melodic items split up into ngrams (perhaps most useful for arrhythmic usage) load(&#39;Berkowitz_ngram.rda&#39;) Berkowitz_ngram &lt;- item_bank rm(item_bank) # Rather than splitting up the corpus into ngrams, split it up based on (a rather crude approximation of) phrase boundaries. This is perhaps more useful for rhythmic usage. Berkowitz_phrase &lt;- item_bank rm(item_bank) # Everything combined load(&#39;Berkowitz_combined.rda&#39;) Berkowitz_combined &lt;- item_bank rm(item_bank) These various item banks can be used as inputs to tests created with the musicassessr package. Please note, these functions can take a long time to run. By default, create_item_bank produces all item bank types as output. You can request only some of the database types to be created using the output argument. You can also use the underlying functions directly, e.g., see: create_item_bank_from_files split_item_bank_into_ngrams count_freqs get_melody_features create_phrases_db 7.7 References Beaty, R. E., Frieler, K., Norgaard, M., Merseal, H. M., MacDonald, M. C., &amp; Weiss, D. J. (2021). Expert musical improvisations contain sequencing biases seen in language production. Journal of Experimental Psychology. https://doi.org/10.1037/xge0001107 Berkowitz, S., Fontrier, G., Goldstein, P., &amp; Smaldone, E. (2017). A new approach to sight singing (Sixth edition). W. W. Norton &amp; Company Crayencour, H.-C., Velichkina, O., Frieler, K., Höger, F., Pfleiderer, M., Henry, L., Solis, G., Wolff, D., Weyde, T., Peeters, G., Basaran, D., Smith, J., &amp; Proutskova, P. (2021) The DTL1000 Jazz Solo Dataset (in prep.). Journal on Computing and Cultural Heritage Müllensiefen, D. (2009). FANTASTIC: Feature ANalysis Technology Accessing STatistics (In a Corpus; Technical report). 37. Pfleiderer, M., Frieler, K., Abeßer, J., Zaddach, W.-G., &amp; Burkhart, B. (Hrsg.). (2017). Inside the Jazzomat—New perspectives for jazz research. Schott Campus. Slonimsky, N. (1947). Thesaurus of scales and melodic patterns. Literary Licensing, LLC. library(itembankr) library(WJD) library(Berkowitz) #library(Slonimsky) 7.8 musicassessrdb 7.9 Datasets (created with itembankr) 7.9.1 WJD A pre-made itembankr item bank of the Weimar Jazz Database 7.10 References Pfleiderer, M., Frieler, K., Abeßer, J., Zaddach, W.-G., &amp; Burkhart, B. (Hrsg.). (2017). Inside the Jazzomat—New Perspectives for Jazz Research. Schott Campus. The command used to create the item bank was: WJD &lt;- corpus_to_item_bank(corpus_name = &quot;WJD&quot;, corpus_df = phrases_dbs2, output_type = &quot;ngram&quot;, phrases_db = phrases_dbs2, launch_app = FALSE) The properties of the item bank are: WJD::WJD itembankr::hist_item_bank(WJD::WJD) 7.10.1 Berkowitz A pre-made itembankr item bank of the Baker (2021) / Berkowitz (2017) sight-singing corpus. 7.11 References Baker, D. (2021). MeloSol Corpus. Empirical Musicology Review, 16, 106–113. https://doi.org/10.18061/emr.v16i1.7645 Berkowitz, S., Fontrier, G., Goldstein, P., &amp; Smaldone, E. (2017). A new approach to sight singing (Sixth edition). W. W. Norton &amp; Company. head(Berkowitz::Berkowitz) itembankr::hist_item_bank(Berkowitz::Berkowitz) 7.11.1 Slonimsky head(Slonimsky::Slonimsky) itembankr::hist_item_bank(Slonimsky::Slonimsky) 7.12 Other useful functions itembankr::subset_item_bank() itembankexplorer::item_bank_explorer(Berkowitz::Berkowitz) Much of the architecture is grouped under the musicassessr package and ecosystem (silasMusicassessrEcosystemRecord2023?). musicassessr is, in essence, a giant wrapper, bringing already developed psychological and musicological into the same software environment, backed by a statistical modelling framework in R (R Core Team 2020). 7.12.1 Assessing musical behaviours: musicassessr "],["musicassessr.html", "8 musicassessr 8.1 The musicassessr ecosystem", " 8 musicassessr musicassessr is an R package for facilitating the deployment of (particularly, musical) stimuli in psychological tests as well as recording and scoring data. It provides convenience functions to deploy stimuli via psychTestR, advanced psychTestR page types to collect new types of data, and utilities to process and score this data, among other things. 8.1 The musicassessr ecosystem musicassessr is part of a network of packages. See also: pyin: transcribe monophonic audio in the R environment using the pYIN algorithm itembankr: produce musicassessr-compatible item banks with useful melodic features Berkowitz: a pre-made itembankr item bank of the Baker (2021) / Berkowitz (2017) sight-singing corpus. WJD: a pre-made itembankr item bank of the Weimar Jazz Database. 8.1.1 Musical ability tests musicassessr currently facilitates the following music ability tests: SAA (Singing Ability Assessment; Silas, Müllensiefen, &amp; Kopiez, 2023) PBET (Playing By Ear Test) PDT (Pitch Discrimination Test; our re-implementation of Soranzo &amp; Grassi, 2014) 8.1.2 Cheat Sheet 8.1.3 Analysis pipeline Analysis pipeline 8.1.4 Research and Documentation You can find several articles and tutorials here, which include summarised results of research utilising this software (see: white papers). For in-depth reading, follow the results to the actual publications. "],["setup.html", "9 Setup 9.1 References 9.2 Ability tests", " 9 Setup If you are running musicassessr on Linux, you will need to install SoX manually. You will also need to install NodeJS. 9.1 References Baker, D. (2021). MeloSol Corpus. Empirical Musicology Review, 16, 106–113. https://doi.org/10.18061/emr.v16i1.7645 Beaty, R. E., Frieler, K., Norgaard, M., Merseal, H. M., MacDonald, M. C., &amp; Weiss, D. J. (2021). Expert musical improvisations contain sequencing biases seen in language production. Journal of Experimental Psychology. https://doi.org/10.1037/xge0001107 Berkowitz, S., Fontrier, G., Goldstein, P., &amp; Smaldone, E. (2017). A new approach to sight singing (Sixth edition). W. W. Norton &amp; Company. Cannam, C., Jewell, M. O., Rhodes, C., Sandler, M., &amp; d’Inverno, M. (2010). Linked Data And You: Bringing music research software into the Semantic Web. Journal of New Music Research, 39(4), 313–325. Crayencour, H.-C., Velichkina, O., Frieler, K., Höger, F., Pfleiderer, M., Henry, L., Solis, G., Wolff, D., Weyde, T., Peeters, G., Basaran, D., Smith, J., &amp; Proutskova, P. (2021). The DTL1000 Jazz Solo Dataset (in prep.). Journal on Computing and Cultural Heritage Harrison, P. M. C. (2020). psychTestR: An R package for designing and conducting behavioural psychological experiments. Journal of Open Source Software, 5(49), 2088. https://doi.org/10.21105/joss.02088 Mauch, M., &amp; Dixon, S. (2014). PYIN: a fundamental frequency estimator using probabilistic threshold distributions. Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2014). Müllensiefen, D., &amp; Frieler, K. (2007). Modelling experts’ notions of melodic similarity. Musicae Scientiae, 11(1_suppl), 183–210. https://doi.org/10.1177/102986490701100108 Silas, S., &amp; Frieler, K. (2023). The musicassessr ecosystem: Record, measure, score and present feedback about musical production behaviour in real-time, supported by psychometric models. Deutsche Gesellschaft für Musikpsychologie, Hanover. Silas, S., Müllensiefen, D., &amp; Kopiez, R. (2023). Singing Ability Assessment: Development and validation of a singing test based on item response theory and a general open-source software environment for singing data. Behaviour Research Methods. Silas, S., Müllensiefen, D., &amp; Kopiez, R. (2023). Utilising a new generation of musical production tests to understand musical learning: Singing ability assessment, melodic recall and playing by ear. Deutsche Gesellschaft für Musikpsychologie, Hanover. Silas, S., Kopiez, R., &amp; Müllensiefen, D. (2021). What makes playing by ear difficult? SEMPRE conference. Soranzo, A., &amp; Grassi, M. (2014). Psychoacoustics: A comprehensive Matlab toolbox for auditory testing. Frontiers in Psychology, 5. https://doi.org/10.3389/fpsyg.2014.00712 library(musicassessr) library(htmltools) htmltools::tagList( lapply(musicassessr::musicassessr_js(visual_notation = TRUE), function(x) { if(base::startsWith(x, &quot;http&quot;)) { htmltools::tags$script(src = x) } else { htmltools::includeScript(x) } }) ) musicassessr::present_stimuli( stimuli = c(60, 62, 64, 65), stimuli_type = &quot;midi_notes&quot;, display_modality = &quot;visual&quot; ) 9.2 Ability tests 9.2.1 Play By Ear Test (PBET) "],["play-by-ear-test-pbet-1.html", "10 Play By Ear Test (PBET) 10.1 Author 10.2 Demos 10.3 Installation 10.4 Usage 10.5 Usage notes 10.6 Citation 10.7 References 10.8 Author 10.9 Demos 10.10 Installation 10.11 Usage 10.12 Usage notes 10.13 Citation 10.14 References 10.15 Conclusion", " 10 Play By Ear Test (PBET) The PBET is a test of playing by ear ability which can be launched in R/Shiny via the psychTestR package. 10.1 Author Seb Silas, sebsilas@gmail.com 10.2 Demos Short PBET with feedback: https://musicassessr.com/PBET-demo 10.3 Installation Install nodeJS: https://nodejs.org/en/download/ Install R: https://cloud.r-project.org/ Install RStudio: https://posit.co/downloads/ Launch RStudio Install the devtools R package by pasting the following command into the RStudio terminal: install.packages('devtools') Install the PBET package: devtools::install_github('sebsilas/PBET') 10.4 Usage Once you have completed the above steps, you can run the PBET test by doing the following: Create a folder which will contain your app. The name of this folder will become the app_name argument when you run the PBET or PBET_standalone functions later. In the folder, create an R file called app.R. In this file, paste your code to run the PBET there, like below: # Load the PBET package library(PBET) # Run a short test with feedback: PBET_standalone(app_name = &#39;my_PBET_app&#39;, skip_setup = TRUE, absolute_url = &quot;https://adaptiveeartraining.com&quot;, num_items = list(&quot;interval_perception&quot; = 0L, &quot;find_this_note&quot; = 0L, &quot;arrhythmic&quot; = list(&quot;key_easy&quot; = 1L, &quot;key_hard&quot; = 1L), &quot;rhythmic&quot; = list(&quot;key_easy&quot; = 1L, &quot;key_hard&quot; = 1L), &quot;wjd_audio&quot; = list(&quot;key_easy&quot; = 0L, &quot;key_hard&quot; = 0L)), examples = list( &quot;find_this_note&quot; = 0L, &quot;interval_perception&quot; = 0L, &quot;arrhythmic&quot; = list(&quot;easy&quot; = 0L, &quot;hard&quot; = 0L), &quot;rhythmic&quot; = list(&quot;easy&quot; = 0L, &quot;hard&quot; = 0L), # because it&#39;s effectively the same task as arrhythmic &quot;wjd_audio&quot; = list(&quot;easy&quot; = 0L, &quot;hard&quot; = 0L) ), feedback = TRUE) Remember that the app_name argument should match the name of your folder. Make sure that your current directory is the app folder you created. You can check the current directory by using getwd(). You may need to set the app directory by placing setwd('/Users/musicassessr/my_project/my_PBET_app') at the beginning of your app file. Run the app. To do this in RStudio, use the shortcut Command + Shift + Enter on Mac (Ctrl + Shift + Enter on Windows). If the above steps were successful, the PBET test should load in your web browser, and as you progress through the test, you should receive feedback after each trial. Recorded audio should appear in the /app_name/www/audio/ directory. If not, go to our Troubleshooting page or raise an issue on Github. 10.5 Usage notes This has not yet been tested on Windows. We will be doing this very soon. The test requires internet connectivity. The PBET runs in your web browser. It is only recommended to run the test in Google Chrome or Opera. You will need to set one of these to be your default browser for RStudio to launch the test there (restart RStudio after doing this). 10.6 Citation We advise mentioning the software versions you used, in particular the versions of the PBET, musicassessr, psychTestR, and psychTestRCAT packages. You can find these version numbers from R by running the following commands: library(PBET) library(psychTestR) library(psychTestRCAT) if (!require(devtools)) install.packages(&quot;devtools&quot;) x &lt;- devtools::session_info() x$packages[x$packages$package %in% c(&quot;PBET&quot;, &quot;psychTestR&quot;, &quot;psychTestRCAT&quot;), ] 10.7 References Silas, S., Müllensiefen, D., &amp; Kopiez, R. (2023). Play By Ear Test: Development and validation of an open-source playing by ear test. In prep. 10.7.1 Singing Ability Assessment (SAA) The Singing Ability Assessment (SAA; Silas, Müllensiefen, &amp; Kopiez, 2023) is a test of singing ability which can be launched in R/Shiny via the psychTestR package. 10.8 Author Seb Silas, sebsilas@gmail.com 10.9 Demos Short SAA with feedback: https://adaptiveeartraining.com/SAA-demo Prototype adaptive-SAA(aSAA) with feedback: https://adaptiveeartraining.com/aSAA 10.10 Installation Install nodeJS: https://nodejs.org/en/download/ Install R: https://cloud.r-project.org/ Install RStudio: https://posit.co/downloads/ Launch RStudio Install the devtools R package by pasting the following command into the RStudio terminal: install.packages('devtools') Install the SAA package: devtools::install_github('sebsilas/SAA') 10.11 Usage Once you have completed the above steps, you can run the SAA test by doing the following: Create a folder which will contain your app. The name of this folder will become the app_name argument when you run the SAA or SAA_standalone functions later. In the folder, create an R file called app.R. In this file, paste your code to run the SAA there, like below: # Load the SAA package library(SAA) # Run a short test with feedback: SAA_standalone(app_name = &quot;my_SAA_app&quot;, num_items = list(long_tones = 2L, arrhythmic = 2L, rhythmic = 2L), feedback = TRUE, SNR_test = FALSE, get_range = FALSE, examples = 0) Remember that the app_name argument should match the name of your folder. Make sure that your current directory is the app folder you created. You can check the current directory by using getwd(). You may need to set the app directory by placing setwd('/Users/musicassessr/my_project/my_SAA_app') at the beginning of your app file. Run the app. To do this in RStudio, use the shortcut Command + Shift + Enter on Mac (Ctrl + Shift + Enter on Windows). If the above steps were successful, the SAA test should load in your web browser, and as you progress through the test, you should receive feedback after each trial. Recorded audio should appear in the /app_name/www/audio/ directory. If not, go to our Troubleshooting page or raise an issue on Github. 10.12 Usage notes This has not yet been comprehensively tested on Windows. Please get in touch if you have any issues. The test requires internet connectivity. The SAA runs in your web browser. It is only recommended to run the test in Google Chrome or Opera. You will need to set one of these to be your default browser for RStudio to launch the test there (restart RStudio after doing this). 10.13 Citation We advise mentioning the software versions you used, in particular the versions of the SAA, musicassessr, psychTestR, and psychTestRCAT packages. You can find these version numbers from R by running the following commands: library(SAA) library(psychTestR) library(psychTestRCAT) if (!require(devtools)) install.packages(&quot;devtools&quot;) x &lt;- devtools::session_info() x$packages[x$packages$package %in% c(&quot;SAA&quot;, &quot;psychTestR&quot;, &quot;psychTestRCAT&quot;), ] 10.14 References Silas, S., Müllensiefen, D., &amp; Kopiez, R. (2023). Singing Ability Assessment: Development and validation of a singing test based on item response theory and a general open-source software environment for singing data. Behaviour Research Methods. https://doi.org/10.3758/s13428-023-02188-0 10.15 Conclusion Thus, a primary outcome of this dissertation was the development of a computational ecosystem to record and assess music production data in real-time. "],["melsim_development.html", "11 Experiment 1: Development of a Melodic Similarity Algorithm for Short Recalled Melodies", " 11 Experiment 1: Development of a Melodic Similarity Algorithm for Short Recalled Melodies "],["pbet_lab_study.html", "12 Experiment 2: An Explanatory Model Of Playing By Ear 12.1 Methods", " 12 Experiment 2: An Explanatory Model Of Playing By Ear 12.1 Methods 12.1.1 Participants and Preliminary Questionnaire Prior to participating in the in-lab component of the study, all participants completed an online pre-screening questionnaire. This battery included the Goldsmiths Musical Sophistication Index (GMS), which assessed participants’ self-reported musical training, singing ability, absolute pitch, and the age at which they began musical instruction. Participants also completed the DEG questionnaire to gather demographic information and musical background, as well as a custom set of items tailored to the aims of the present study. These custom questions asked about participants’ musician identity (e.g., professional, student, amateur), primary musical genre, experience and comfort with memorisation, highest graded performance level (e.g., UK ABRSM/Trinity levels), and self-assessed absolute pitch ability. Participants also selected their primary instrument (including voice) from a dropdown menu. Upon completion of this battery, participants received a unique participant ID and were invited to attend an in-person testing session. 12.1.2 Apparatus and General Procedure The in-lab portion of the study was conducted using the musicassessr platform, implemented in R using the psychTestR framework. All testing took place on lab computers, with participants using either a MIDI keyboard or a microphone as their input device, depending on the specific requirements of each task. The experiment was administered via a custom-built web application with real-time audio and MIDI processing capabilities. Participants completed seven musical tasks in total, each designed to probe different aspects of musical memory, perception, and performance. To ensure consistency across participants, the Singing Ability Assessment (SAA) was always presented first. The remaining tasks were presented in a pseudo-randomised order unique to each participant, with the exception of the JAJ visuospatial working memory test, which was always administered last. This structure was chosen to control for potential order effects while preserving the integrity of the singing-first manipulations later in the battery. 12.1.2.1 Experimental Tasks 12.1.2.1.1 Singing Ability Assessment (SAA). The first task evaluated participants’ capacity to vocally reproduce short melodic sequences from memory. Each trial consisted of a brief melody, either rhythmic or arrhythmic in nature, played once over headphones. Participants were then required to sing the melody back using a microphone, without the aid of visual notation. The test consisted of 16 melodies in total—eight rhythmic and eight arrhythmic—randomly drawn from a subset of the Berkowitz melodic dictation corpus. If enabled in the experiment configuration, the task began with a brief voice range calibration procedure to adjust playback range to the participant’s vocal register. Each melody was attempted once. 12.1.2.1.2 Playing by Ear Test (Standard PBET). This task assessed participants’ ability to reproduce melodies they had heard by playing them back on a MIDI keyboard or singing them. Each trial presented a novel melody ranging in length from 5 to 15 notes, which participants were required to reproduce from memory. Both rhythmic and arrhythmic items were included, sampled using an adaptive n-tile procedure to ensure an even distribution of difficulty. Participants were allowed up to three attempts per melody. To support pitch anchoring, the first note of each melody was always provided. No visual cues were given, and participants did not receive feedback on their performance. 12.1.2.1.3 Learn-Test Paradigm (Audio-Visual PBET). In this condition, participants first learned a set of melodies through either auditory or visual exposure. Melodies in the auditory condition were played over headphones, whereas melodies in the visual condition were presented as standard musical notation. Following the learning phase, all melodies were tested aurally, requiring participants to reproduce them without any visual aid. This paradigm allowed for the comparison of performance based on the modality through which melodies were encoded. Presentation order of melody type (visual vs. auditory) was randomised within each participant. 12.1.2.1.4 PBET with Singing First. This variant of the standard PBET introduced an intermediate vocal rehearsal phase. For each melody, participants first attempted to sing the sequence aloud, and then reproduced the same melody using their instrument. This task included a total of 10 melodies—five rhythmic and five arrhythmic—randomly sampled from the same item bank as the standard PBET. Singing was always the first response attempt, with up to two additional instrumental attempts allowed if needed. This condition was designed to investigate whether vocal rehearsal enhanced subsequent instrumental reproduction. 12.1.2.1.5 Find This Note + Pitch Imagery Assessment (PIAT). Participants next completed an interleaved task involving auditory pitch localisation and internal auditory imagery. In the “Find This Note” component, participants heard a single target pitch and attempted to reproduce it on their instrument. This was alternated with the Pitch Imagery Auditory Task (PIAT), a cognitive test that required participants to internally manipulate and recall pitch sequences. Input modality (MIDI or audio) was determined based on participant preference and hardware availability and remained consistent throughout the task. No feedback was provided. 12.1.2.1.6 Interval Perception Task. In this purely perceptual task, participants listened to pairs of notes and identified the musical interval between them. Intervals varied in type and direction (ascending or descending), and no reproduction was required. The task was designed to assess baseline interval discrimination skills. 12.1.2.1.7 JAJ Task. The final task in the battery was a visuospatial working memory assessment. Participants viewed sequences of spatial positions on a grid and were later required to reproduce the sequence in the correct order. This task was included to provide a cognitive control measure and was always administered last to prevent interference with the musical tasks. 12.1.2.1.8 Trial Structure and Randomisation All melodic stimuli were drawn from a subset of the Berkowitz melodic dictation exercises and categorised as either rhythmic or arrhythmic. Stimuli were sampled using adaptive n-tile procedures to ensure balanced difficulty across participants and across conditions. Within each test, trials were presented in a randomised order, and melody length was randomly sampled between 5 and 15 notes where applicable. Input modality (MIDI keyboard or microphone) was either fixed per task or determined dynamically based on participant setup. Where applicable, participants were given practice examples before beginning formal test trials. All responses were captured using either audio recording or MIDI data and were scored in real-time using custom asynchronous scoring functions embedded in the testing platform. After each task, participants were asked to provide brief written reflections on their experience and strategies used. "],["pbet_online_study.html", "13 Experiment 3: Modelling Melodic Difficulty in Playing by Ear: An Item Response Analysis of Online Performance Data", " 13 Experiment 3: Modelling Melodic Difficulty in Playing by Ear: An Item Response Analysis of Online Performance Data "],["study_history_study.html", "14 Experiment 4: A Longitudinal Study of Playing By Ear Skills", " 14 Experiment 4: A Longitudinal Study of Playing By Ear Skills "],["similarity_study.html", "15 Experiment 5: A Similarity-Based Approach to Item Selection 15.1 Collaborative Filtering 15.2 Approaches to Similarity for Large-Scale Corpora 15.3 Melodic corpora as networks", " 15 Experiment 5: A Similarity-Based Approach to Item Selection 15.1 Collaborative Filtering # library(recommenderlab) library(magrittr) #&#39; recommend_n_melodies #&#39; #&#39; @param p_id A valid p_id to recommend top n melodies for #&#39; @param n number of melodies to recommend #&#39; #&#39; @return A json with the top n recommended melodies #&#39; @export recommend_n_melodies &lt;- function(p_id, n){ # extract and clean user-item interactions logging::loginfo(&quot;Generating %s survey recommendations for User %s&quot;,n,p_id) logging::loginfo(&quot;Parsing relevant data from database&quot;) logging::loginfo(&quot;%s Survey-User interactions parsed&quot;, nrow(dat)) dat &lt;- dat %&gt;% dplyr::select(abs_melody,p_id, opti3) %&gt;% dplyr::mutate(mel_p_id = paste(abs_melody, p_id, sep=&quot;-&quot;)) %&gt;% # This gets rid of duplicates and ensures that if there # are entries for a user both &#39;opti3&#39; and not &#39;opti3&#39; # a survey, the &#39;opti3&#39; entry is retained dplyr::arrange(desc(opti3)) %&gt;% dplyr::distinct(mel_p_id, .keep_all=T) %&gt;% dplyr::select(-mel_p_id) %&gt;% tidyr::pivot_wider(names_from=abs_melody, values_from=opti3, values_fill = NA) # Transform into binarized matrix user_id &lt;- dat$p_id dat$p_id &lt;- NULL mel_mat &lt;- as.matrix(dat) rownames(mel_mat) &lt;- user_id r &lt;- as(mel_mat,&quot;realRatingMatrix&quot;) r_b &lt;- binarize(r, minRating=1) # Build the hybrid predictor logging::loginfo(&quot;Training recommenders&quot;) popular_recommender &lt;- Recommender(data=r_b, method=&quot;POPULAR&quot;) my_recommender &lt;- popular_recommender # if the user isn&#39;t in dat, we recommend based on popularity, # in which case any index will do (but the recommender stil requires one) user_idx &lt;- 1 if (p_id %in% user_id){ # If the user has attempted at least one survey before, use # a mixture of user-based any hybrid recommendations user_recommender &lt;- Recommender(data = r_b, method=&quot;UBCF&quot;, param=list(nn=50)) hybrid &lt;- HybridRecommender(user_recommender, popular_recommender, weights=c(0.50, 0.50)) # get the index of the p_id user_idx &lt;- match(p_id, user_id ) my_recommender &lt;- hybrid } logging::loginfo(&quot;Generating recommendations&quot;) output &lt;- predict(object = my_recommender, newdata = user_idx, n = n, data = r_b) recs &lt;- unlist(output@items) itemIds &lt;- colnames(mel_mat) recommended_items &lt;- itemIds[recs] logging::loginfo(&quot;Recommendations generated, returning to client&quot;) recommended_items } t &lt;- recommend_n_melodies(p_id = sample(unique(dat$p_id), size = 1), n = 10) t 15.2 Approaches to Similarity for Large-Scale Corpora 15.3 Melodic corpora as networks 15.3.1 Approach #1: Similarity of Features as a Proxy for Melodic Similarity Conclusion: similarity of features =! melodic (opti3/[perceptual?!]) similarity - this is important, because similar features will produce similar difficulty scores (need to test this formally though).. which shows that similarity as a separate item selector to difficulty is warranted/important (potentially) 15.3.2 Approach #2: Generative Similarity bringing in desimilarize et al. app 15.3.3 Approach #3: Parent Melody Representation On breaking up items into N-grams, store representations/similarity. By definition N-grams will be similarity to the overall melody they come from "],["additional-learning-paradigms.html", "16 Additional Learning Paradigms 16.1 Generative Similarity 16.2 Learn Chunks", " 16 Additional Learning Paradigms pedagogical part 16.1 Generative Similarity 16.2 Learn Chunks identify correct vs incorrect ngrams in a recall: add incorrect ngrams to beginning of session buffer append incorrect N-Grams to top of buffer priorotise earlier ngrams prioritise those with temporal order prioritise those which are easier or, just incrementally test until each ngram is obtained from the beginning. e.g, if the target is, 60:65, first the participant must at least get 60:61; once this is got, 60:63 etc.. the program adds a note each time (or if the participant adds an additional extra note, the program could skip an n-gram) "],["experiment-6-model-synthesis-development-of-a-musical-dashsim-model.html", "17 Experiment 6: Model synthesis: Development of a musical DASH+SIM model", " 17 Experiment 6: Model synthesis: Development of a musical DASH+SIM model Solve the item curation problem "],["interventional_Study.html", "18 Experiment 7: An Interventional Study", " 18 Experiment 7: An Interventional Study AB, Contrasonic. 2025. “Chordbot: Play &amp; Create Backing Tracks.” ABRSM. 2024. ABRSM Aural Tests. Manual. Alluri, Vinoo et al. 2017. “Musical Imagery Involves Wernicke’s Area in Bilateral and Anti-Correlated Network Interactions in Musicians.” Scientific Reports 7: 17066. https://doi.org/10.1038/s41598-017-17178-4. Anderson, John Robert. 1972. “Fran: A Simulation Model of Free Recall.” In Psychology of Learning and Motivation, edited by Gordon H. Bower, 5:315–78. Academic Press. https://doi.org/10.1016/S0079-7421(08)60444-2. Anthony, Lisa, YooJin Kim, and Leah Findlater. 2013. “Analyzing User-Generated YouTube Videos to Understand Touchscreen Use by People with Motor Impairments.” In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, 1223–32. Audacity Team. 2025a. “Audacity: Free, Open Source, Cross-Platform Audio Software.” B. V., Chordify. 2025. “Chordify: Chords for Any Song.” Baddeley, Alan. 2000. “The Episodic Buffer: A New Component of Working Memory?” Trends in Cognitive Sciences 4 (11, 11): 417–23. https://doi.org/10.1016/S1364-6613(00)01538-2. Baddeley, Alan D., and Graham Hitch. 1974. “Working Memory.” In Psychology of Learning and Motivation, edited by Gordon H. Bower, 8:47–89. Academic Press. https://doi.org/10.1016/S0079-7421(08)60452-1. Bahrick, Harry P., Lorraine E. Bahrick, Alice S. Bahrick, and Phyllis E. Bahrick. 1993. “Maintenance of Foreign Language Vocabulary and the Spacing Effect.” Psychological Science 4 (5): 316–21. https://doi.org/10.1111/j.1467-9280.1993.tb00571.x. Baker, David. 1969. Jazz Improvisation: A Comprehensive Method for All Players. DB, Music Workshop Publications. ———. 2019. “Modeling Melodic Dictation.” LSU Doctoral Dissertations, June. https://doi.org/10.31390/gradschool_dissertations.4960. Baker, David, and Lucy Green. 2013. “Ear Playing and Aural Development in the Instrumental Lesson: Results from a ’Case-Control’ Experiment.” Research Studies in Music Education 35 (2): 141–59. https://doi.org/10.1177/1321103X13508254. Bangert, Marc, Thomas Peschel, Gottfried Schlaug, Michael Rotte, Dieter Drescher, Hermann Hinrichs, Hans-Jochen Heinze, and Eckart Altenmüller. 2006. “Shared Networks for Auditory and Motor Processing in Professional Pianists: Evidence from fMRI Conjunction.” NeuroImage 30 (3): 917–26. https://doi.org/10.1016/j.neuroimage.2005.10.044. Bartlett, Edward L. 2013. “The Organization and Physiology of the Auditory Thalamus and Its Role in Processing Acoustic Features Important for Speech Perception.” Brain and Language 126 (1): 29–48. https://doi.org/10.1016/j.bandl.2013.03.003. Beihoff, Norbert J. 1934. Course in Modern Arranging and Orchestration. Beihoff Music Company. Benbassat, Alain. 2025. “Functional Ear Trainer.” Bennett, D. 1980. “Informal Learning Among Popular Musicians.” British Journal of Music Education 2 (2): 27–39. ———. 1983. “Learning Popular Music Through Recordings.” Music Educators Journal 69 (9): 44–47. Benward, Kenton, and Frank Wildman. 1984. Jazz Improvisation: An Integrated Approach. Dubuque, IA: Wm. C. Brown. Berliner, Paul F. 1994. Thinking in Jazz: The Infinite Art of Improvisation. Chicago: University of Chicago Press. Bernhard, H. Christian. 2004. “The Effects of Tonal Training on the Melodic Ear Playing and Sight Reading Achievement of Beginning Wind Instrumentalists.” Contributions to Music Education 31 (1): 91–107. https://www.jstor.org/stable/24133280. ———. 2005. “Playing by Ear: Is Expert Opinion Supported by Research?” Bulletin of the Council for Research in Music Education, no. 163: 79–86. https://doi.org/10.2307/27861482. Bertholf, Garrett. 2014. “John Coltrane: Jazz Improvisation, Performance, and Transcription.” PhD thesis, Colby College. Berz, William A. 1995. “Working Memory in Music: A Theoretical Model.” Music Perception 12 (3): 353–64. https://doi.org/10.2307/40286188. Biasutti, Michele, Roberta Antonini Philippe, and Andrea Schiavio. 2022. “Assessing Teachers’ Perspectives on Giving Music Lessons Remotely During the COVID-19 Lockdown Period.” Musicae Scientiae 26 (3): 585–603. https://doi.org/10.1177/1029864921996033. Bjork, Robert A. 1994. “Institutional Impediments to Effective Training.” In Learning, Remembering, Believing: Enhancing Human Performance, edited by Daniel Druckman and Robert A. Bjork, 295–306. Washington, DC: National Academy Press. Blake, Ran. 2010. Primacy of the Ear. Third Stream Associates: Lulu.com. Bregman, Albert S. 1990. Auditory Scene Analysis: The Perceptual Organization of Sound. MIT Press. Brown, Thomas W. 1990. “An Investigation of the Effectiveness of a Piano Course in Playing by Ear and Aural Skills Development for College Students.” Dissertation Abstracts International 51: 4052. https://www.jstor.org/stable/27861482. Butterfield, Matthew W. 2002. “Music Analysis and the Social Life of Jazz Recordings.” Current Musicology, no. 71-73: 324–52. Campbell, Patricia S. 1995. “Recording as Notation in Popular Musicians’ Learning.” Ethnomusicology 39 (1): 41–65. Carlini, Alessandro, Camille Bordeau, and Maxime Ambard. 2024. “Auditory Localization: A Comprehensive Practical Review.” Frontiers in Psychology 15 (July). https://doi.org/10.3389/fpsyg.2024.1408073. Cascella, Marco, and Yasir Al Khalili. 2025. “Short-Term Memory Impairment.” In StatPearls. Treasure Island (FL): StatPearls Publishing. Celemony Software. 2025b. “Melodyne.” Centre for Digital Music, Queen Mary University of London. 2025. “Sonic Visualiser.” Cepeda, Nicholas J., Edward Vul, Doug Rohrer, John T. Wixted, and Harold Pashler. 2008. “Spacing Effects in Learning: A Temporal Ridgeline of Optimal Retention.” Psychological Science 19 (11): 1095–1102. https://doi.org/10.1111/j.1467-9280.2008.02209.x. Chukharev-Hudilainen, Evgeny, and Tatiana A. Klepikova. 2016. “The Effectiveness of Computer-Based Spaced Repetition in Foreign Language Vocabulary Instruction: A Double-Blind Study.” CALICO Journal 33 (3): 334–54. https://doi.org/10.1558/cj.v33i3.26055. Coker, Jerry. 1964. Improvising Jazz. Englewood Cliffs, NJ: Prentice-Hall. Coker, Jerry, James Casale, Gary Campbell, and Jerry Greene. 1999. Patterns for Jazz: A Theory Text for Jazz Composition and Improvisation: For Bass Clef Instruments. Alfred Music. Conklin, Darrell, and Ian H. Witten. 1995. “Multiple Viewpoint Systems for Music Prediction.” Journal of New Music Research 24 (1): 51–73. https://doi.org/10.1080/09298219508570672. Crazy Ootka Software AB. 2025. “Perfect Ear: Music Theory, Ear &amp; Rhythm Training.” Creech, Andrea, Susan Hallam, Maria Varvarigou, and Hilary McQueen. 2008. “From Music Student to Professional: The Process of Transition.” British Journal of Music Education 25 (3): 315–31. https://doi.org/10.1017/S0265051708008125. D’Ausilio, A., E. Altenmüller, M. Olivetti Belardinelli, and M. Lotze. 2006. “Cross-Modal Plasticity of the Motor Cortex While Listening to a Rehearsed Musical Piece.” European Journal of Neuroscience 24 (3): 955–58. https://doi.org/10.1111/j.1460-9568.2006.04960.x. De Boeck, Paul, Sun-Joo Cho, and Mark Wilson. 2016. “Explanatory Item Response Models.” In The Wiley Handbook of Cognition and Assessment, 247–66. John Wiley &amp; Sons, Ltd. https://doi.org/10.1002/9781118956588.ch11. Dickey, Marc R. 1991. “A Comparison of Verbal Instruction and Nonverbal Teacher-Student Modeling in Instrumental Ensembles.” Journal of Research in Music Education 39 (3): 132–42. https://doi.org/10.2307/3344693. Downie, J. Stephen. 2003. “Music Information Retrieval.” Annual Review of Information Science and Technology 37 (1): 295–340. https://doi.org/10.1002/aris.1440370108. Driedger, Jürgen, and Meinard Müller. 2016. “Interactive Analysis and Re-Synthesis of Music Recordings.” In Proceedings of the International Society for Music Information Retrieval Conference (ISMIR), 621–27. “EarMaster - The App for Ear Training, Sight-Singing, and Rhythm Training.” 2025. EarMaster. https://www.earmaster.com/?gad_source=1&amp;gad_campaignid=1353643090&amp;gbraid=0AAAAAD_cU1UTALh5mx7uET13D6z3BQ_Em&amp;gclid=CjwKCAjwl_XBBhAUEiwAWK2hzkpF8Gg1pVNCP0BzQhzdNO-1qzO2HQgH2YoFQQkwFKRe_rktNY3JShoC3mgQAvD_BwE. EarMaster ApS. 2025c. “EarMaster Cloud.” Ebbinghaus, Hermann. 1888. Memory. Scholar’s Choice. Ericsson, K. A., and W. Kintsch. 1995. “Long-Term Working Memory.” Psychological Review 102 (2): 211–45. https://doi.org/10.1037/0033-295x.102.2.211. Feisst, Sabine. 2011. Schoenberg’s New World: The American Years. OUP USA. Fenn, Kimberly M., and David Z. Hambrick. 2013. “What Drives Sleep-Dependent Memory Consolidation: Greater Gain or Less Loss?” Psychonomic Bulletin &amp; Review 20 (3): 501–6. https://doi.org/10.3758/s13423-012-0366-z. Fine, Philip A. et al. 2006. Music in the Brain: The Cognitive Neuroscience of Music. Oxford University Press. Fletcher, Connor, Vedad Hulusic, and Panos Amelidis. 2019. “Virtual Reality Ear Training System: A Study on Spatialised Audio in Interval Recognition.” In 2019 11th International Conference on Virtual Worlds and Games for Serious Applications (VS-Games), 1–4. https://doi.org/10.1109/VS-Games.2019.8864592. Freymuth, Malva Susanne. 1999. Mental Practice and Imagery for Musicians: A Practical Guide for Optimizing Practice Time, Enhancing Performance, and Preventing Injury. Boulder, CO: Integrated Musician’s Press. Frieler, Klaus. 2018. “Computational Melody Analysis.” In Inside the Jazzomat. New Perspectives for Jazz Research, edited by M. Pfleiderer, K. Frieler, W.-G. Abeßer, B. Zaddach, and B. Burkhard, 41–84. Schott Campus. Galvão, Afonso, and Anthony Kemp. 1999. “Kinaesthesia and Instrumental Music Instruction: Some Implications.” Psychology of Music 27 (2): 129–37. https://doi.org/10.1177/0305735699272004. Gerber, M. S. 1993. “An Investigation of the Relationship Between Performance Expertise and Melodic Ear-to-Hand Coordination of Music Patterns Within Various Degrees of Contextual Constraint,” 1. Gordon, Chelsea L., Patrice R. Cobb, and Ramesh Balasubramaniam. 2018. “Recruitment of the Motor System During Music Listening: An ALE Meta-Analysis of fMRI Data.” PLoS ONE 13 (11): e0207213. https://doi.org/10.1371/journal.pone.0207213. Gordon, Edwin. 2007. Learning Sequences in Music: A Contemporary Music Learning Theory. GIA Publications. Gordon, Edwin E. 2001. Roots of Music Learning Theory and Audiation. Chicago: GIA Publications. Green, Lucy. 2002. How Popular Musicians Learn: A Way Ahead for Music Education. Aldershot: Ashgate. ———. 2008. Music, Informal Learning and the School: A New Classroom Pedagogy. Ashgate Publishing, Ltd. Gregersen, Peter K., Elena Kowalsky, Nina Kohn, and Elizabeth West Marvin. 1999. “Absolute Pitch: Prevalence, Ethnic Variation, and Estimation of the Genetic Component.” The American Journal of Human Genetics 65 (3): 911–13. https://doi.org/10.1086/302541. Groce, T. 1989. “Learning from Recordings.” Journal of Popular Music Studies 1: 45–60. Hallam, Susan. 1998. “Predictors of Success in Instrumental Music Lessons: A Longitudinal Study.” Psychology of Music 26 (2): 116–32. https://doi.org/10.1177/0305735698262003. Harris, Robert, Peter van Kranenburg, and Bauke M. de Jong. 2016. “Behavioral Quantification of Audiomotor Transformations in Improvising and Score-Dependent Musicians.” PLoS ONE 11 (11): e0166033. https://doi.org/10.1371/journal.pone.0166033. Harrison, P. M. C. 2025. Music and Science. Self-published online. Haston, Warren. 2010. “Beginning Wind Instrument Instruction: A Comparison of Aural and Visual Approaches.” Contributions to Music Education 37 (2): 9–28. https://www.jstor.org/stable/24127224. Haston, Warren, and Gary E. McPherson. 2022. “Playing by Ear.” In The Oxford Handbook of Music Performance, Volume 1, edited by Gary E. McPherson, 173–90. New York: Oxford University Press. Haueisen, Jens, and Thomas R. Knösche. 2001. “Involuntary Motor Activity in Pianists Evoked by Music Perception.” Journal of Cognitive Neuroscience 13 (6): 786–92. https://doi.org/10.1162/08989290152541449. Herborn, Peter. 2022. Features Of The Perception And Construction Of Melodies. Emanobooks. Hermann, Evelyn. 1999. Shinichi Suzuki: The Man and His Philosophy (Revised). Alfred Music. Herzig, Monika. 2019. “The Abcs of Jazz Education. Rethinking Jazz Pedagogy.” In Jazzforschung Heute. Jenkins, John G., and Karl M. Dallenbach. 1924. “Obliviscence During Sleep and Waking.” American Journal of Psychology 35 (4): 605–12. Johansson, Marie. 2004. “Chord Learning Strategies of Experienced Musicians.” Music Education Research 6 (2): 173–87. Johnson, Tom. 2014. Other Harmony: Beyond Tonal and Atonal. Paris: Editions 75, Two-Eighteen Press. Johnson-Laird, P. N. 2002. “How Jazz Musicians Improvise.” Music Perception 19 (3): 415–42. https://doi.org/10.1525/mp.2002.19.3.415. Keller, Peter E. 2012. “Mental Imagery in Music Performance: Underlying Mechanisms and Potential Benefits.” Annals of the New York Academy of Sciences 1252 (1): 206–13. https://doi.org/10.1111/j.1749-6632.2011.06439.x. Keller, Peter E., Simone Dalla Bella, and Iring Koch. 2010. “Auditory Imagery Shapes Movement Timing and Kinematics: Evidence from a Musical Task.” Journal of Experimental Psychology: Human Perception and Performance 36 (2): 508–13. https://doi.org/10.1037/a0017604. Keller, Peter E., and Iring Koch. 2006. “The Planning and Execution of Short Auditory Sequences.” Psychonomic Bulletin &amp; Review 13 (4): 711–16. https://doi.org/10.3758/bf03193985. Khajah, Mohammad M., Robert V. Lindsey, and Michael C. Mozer. 2014. “Maximizing Students’ Retention via Spaced Review: Practical Guidance from Computational Models of Memory.” Topics in Cognitive Science 6 (1): 157–69. https://doi.org/10.1111/tops.12077. Koger, Terry S., Mark Martin, Gary Richardson, Hassan Sabree, Anthony Wade, and Dominique-René de Lerma. 1985. “Fifty Years of \"Down Beat\" Solo Jazz Transcriptions: A Register.” Black Music Research Journal 5: 43–79. https://doi.org/10.2307/779496. Krumhansl, Carol. 1990. Cognitive Foundations of Musical Pitch. Oxford University Press, USA. Lateef, Yusef A. 2015. Repository of Scales and Melodic Patterns. Jamey Aebersold Jazz. Lehmann, Andreas C., and Jane W. Davidson. 2002. “Taking an Acquired Skills Perspective on Music Performance.” In The New Handbook of Research on Music Teaching and Learning, edited by Richard Colwell and Carol Richardson, 542–60. Oxford, UK: Oxford University Press. Leite, Raphael B. C., Sergio A. Mota-Rolim, and Claudio M. T. Queiroz. 2016. “Music Proficiency and Quantification of Absolute Pitch: A Large-Scale Study Among Brazilian Musicians.” Frontiers in Neuroscience 10 (October). https://doi.org/10.3389/fnins.2016.00447. Lester, James. 1995. Too Marvelous for Words: The Life and Genius of Art Tatum. New York: Oxford University Press. Lilliestam, Lars. 1996. “On Playing by Ear.” Popular Music 15 (2): 195–216. https://doi.org/10.1017/S0261143000008114. Limb, Charles J., and Allen R. Braun. 2008. “Neural Substrates of Spontaneous Musical Performance: An fMRI Study of Jazz Improvisation.” PLOS ONE 3 (2): e1679. https://doi.org/10.1371/journal.pone.0001679. Lindsey, Robert V., Jared D. Shroyer, Harold Pashler, and Michael C. Mozer. 2014. “Improving Students’ Long-Term Knowledge Retention Through Personalized Review.” Psychological Science 25 (3): 639–47. https://doi.org/10.1177/0956797613504302. Lingjie, An. 2025. “Effectiveness Study of Applying Earmaster for Ear Training in Chinese University Students.” International Journal of Sociologies and Anthropologies Science Reviews 5 (2): 29–38. https://doi.org/10.60027/ijsasr.2025.5514. Liscio, Christopher, and Daniel G. Brown. 2024. “Watching Popular Musicians Learn by Ear: A Hypothesis-Generating Study of Human-Recording Interactions in YouTube Videos.” arXiv. https://doi.org/10.48550/arXiv.2406.04058. Loewen, Shawn, Daniel R. Isbell, and Zachary Sporn. 2020. “The Effectiveness of App-Based Language Instruction for Developing Receptive Linguistic Knowledge and Oral Communicative Ability.” Foreign Language Annals 53 (2): 209–33. https://doi.org/10.1111/flan.12454. Luce, John R. 1965. “Sight-Reading and Ear-Playing Abilities as Related to Instrumental Music Students.” Journal of Research in Music Education 13 (2): 101–9. https://doi.org/10.2307/3344447. Mainwaring, James. 1951. Teaching Music in Schools. London: Paxton. Martin, Henry. 2012. “Expanding Jazz Tonality: The Compositions of John Coltrane.” Theory and Practice 37/38: 185–219. https://www.jstor.org/stable/43864910. Mauriello, Laura, Daniel Avrahami, and Jennifer Mankoff. 2018. “Using Social Media to Observe Behavior: A Study of Music Learning on YouTube.” In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems, 1–13. McPherson, G. E. 2005. “From Child to Musician: Skill Development During the Beginning Stages of Learning an Instrument.” Psychology of Music 33 (1): 5–35. https://doi.org/10.1177/0305735605048012. McPherson, G. E., Michael Bailey, and Kenneth E. Sinclair. 1997. “Path Analysis of a Theoretical Model to Describe the Relationship Among Five Types of Musical Performance.” Journal of Research in Music Education 45 (1): 103–29. https://doi.org/10.2307/3345469. McPherson, Gary E. 1993. “Factors and Abilities Influencing the Development of Visual, Aural and Creative Performance Skills in Music and Their Educational Implications.” PhD thesis, Sydney, Australia: University of Sydney. ———. 1995. “The Assessment of Musical Performance: Development and Validation of Five New Measures.” Psychology of Music 23 (2): 142–61. https://doi.org/10.1177/0305735695232003. McPherson, Gary, and Alf Gabrielsson. 2002. “From Sound to Sign.” In The Science &amp; Psychology of Music Performance: Creative Strategies for Teaching and Learning, 98–115. https://doi.org/10.1093/acprof:oso/9780195138108.003.0007. Miksza, Peter. 2023. “Mental Practice in Music Learning.” In The Oxford Handbook of Music Practice-Based Research, edited by Editor Name Author, 100–120. Oxford University Press. Mishra, Jennifer. 2016. “Rhythmic and Melodic Sight Reading Interventions: Two Meta-Analyses.” Psychology of Music 44 (5): 1082–94. https://doi.org/10.1177/0305735615610925. Mozer, Michael C., and Robert V. Lindsey. 2016. “Predicting and Improving Memory Retention: Psychological Theory Matters in the Big Data Era.” In. Müllensiefen, Daniel, and Klaus Frieler. 2004a. “Cognitive Adequacy in the Measurement of Melodic Similarity: Algorithmic Vs. Human Judgments.” In Music Query: Methods, Models, and User Studies. MIT Press. ———. 2004b. “Melodic Similarity: Approaches and Applications.” In Proceedings of the 8th ICMPC, edited by S. D. Lipscombe, R. Ashley, R. O. Gjerdingen, and P. Webster, 283–89. IL. ———. 2007. “Modelling Experts’ Notions of Melodic Similarity.” Musicae Scientiae 11 (1_suppl): 183–210. https://doi.org/10.1177/102986490701100108. ———. n.d. “Statistical Methods in Music Corpus Studies: Application, Use Cases, and Best Practice Examples.” In The Oxford Handbook of Music and Corpus Studies, edited by Daniel Shanahan, John Ashley Burgoyne, and Ian Quinn, 0. Oxford University Press. Accessed June 8, 2025. https://doi.org/10.1093/oxfordhb/9780190945442.013.8. Müllensiefen, Daniel, and Marc Pendzich. 2009. “Court Decisions on Music Plagiarism and the Predictive Value of Similarity Algorithms.” Musicae Scientiae 13 (1_suppl): 257–95. https://doi.org/10.1177/102986490901300111. Munive Benites, David, Philippe Lalitte, and Victoria Eyharabide. 2023. “Ear Training Applications in Music Education: Exploring Utilization, Effectiveness, and Adoption Factors in France.” In Proceedings of the 15th International Conference on Computer Supported Education - Volume 1: EKM, 447–53. SciTePress / INSTICC. https://doi.org/10.5220/0012054200003470. Murre, Jaap M. J., and Joeri Dros. 2015. “Replication and Analysis of Ebbinghaus’ Forgetting Curve.” PLoS ONE 10 (7): e0120644. https://doi.org/10.1371/journal.pone.0120644. Musco, Ann Marie. 2010. “Playing by Ear: Is Expert Opinion Supported by Research?” Bulletin of the Council for Research in Music Education, no. 184: 49–64. https://www.jstor.org/stable/27861482. musictheory.net. 2025. “Tenuto.” Nichols, Bryan E., Clemens Wöllner, and Andrea R. Halpern. 2018. “Score One for Jazz: Working Memory in Jazz and Classical Musicians.” Psychomusicology: Music, Mind, and Brain 28 (2): 101–7. https://doi.org/10.1037/pmu0000211. Norgaard, Martin. 2011. “Descriptions of Improvisational Thinking by Artist-Level Jazz Musicians.” Journal of Research in Music Education 59 (2): 109–27. https://doi.org/10.1177/0022429411405669. O’Gallagher, John. 2020. “Pitch-Class Set Usage and Development in Late-Period Improvisations of John Coltrane.” Jazz Perspectives 12 (1): 93–121. https://doi.org/10.1080/17494060.2020.1734055. Ockelford, Adam. 2007. Music for Children and Young People with Complex Needs. Oxford: Oxford University Press. Odam, George. 1995. The Sounding Symbol: Music Education in Action. Nelson Thornes. Parker, Charlie. 2009. Charlie Parker - Omnibook: For E-flat Instruments. Hal Leonard Corporation. Pearce, Marcus, and Daniel Müllensiefen. 2017. “Compression-Based Modelling of Musical Similarity Perception.” Journal of New Music Research 46 (2): 135–55. https://doi.org/10.1080/09298215.2017.1305419. Pearl, Judea, and Dana Mackenzie. 2018. The Book of Why: The New Science of Cause and Effect. Penguin UK. Peters, G. David. 1992. “Music Software and Emerging Technology.” Music Educators Journal 79 (3): 22–63. https://doi.org/10.2307/3398478. Peterson, Lloyd R., and Margaret J. Peterson. 1959. “Short-Term Retention of Individual Verbal Items.” Journal of Experimental Psychology 58 (3): 193–98. Pfordresher, Peter Q. 2005. “Auditory Feedback in Music Performance: The Role of Melodic Structure and Musical Skill.” Journal of Experimental Psychology: Human Perception and Performance 31 (6): 1331–45. https://doi.org/10.1037/0096-1523.31.6.1331. Pfordresher, Peter Q., Steven M. Demorest, Simone Dalla Bella, Sean Hutchins, Psyche Loui, Joanne Rutkowski, and Graham F. Welch. 2015. “Theoretical Perspectives on Singing Accuracy: An Introduction to the Special Issue on Singing Accuracy (Part 1).” Music Perception 32 (3): 227–31. https://doi.org/10.1525/mp.2015.32.3.227. Pfordresher, Peter Q., and Caroline Palmer. 2006. “Effects of Hearing the Past, Present, or Future During Music Performance.” Perception &amp; Psychophysics 68 (3): 362–76. https://doi.org/10.3758/BF03193683. PG Music Inc. 2025d. “Band-in-a-Box.” Pressing, Jeff. 1988. “Improvisation: Methods and Models.” In Generative Processes in Music: The Psychology of Performance, Improvisation, and Composition, edited by John A. Sloboda, 129–78. Oxford: Oxford University Press. Priest, Philip. 1989. “Playing by Ear: Its Nature and Application to Instrumental Learning.” British Journal of Music Education 6 (2): 173–91. https://doi.org/10.1017/S0265051700007038. Prouty, Kenneth E. 2005. “The History of Jazz Education: A Critical Reassessment.” Journal of Historical Research in Music Education 26 (2): 79–100. https://doi.org/10.1177/153660060502600202. R Core Team. 2020. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. Re, Adrien Marcus. 2004. “The Role of Transcription in Jazz Improvisation: Examining the Aural-Imitative Approach in Jazz Pedagogy.” D.{{A}}. {{Dissertation}}, Ball State University. Roni Music. 2025. “Amazing Slow Downer by Roni Music.” https://www.ronimusic.com/. Ross, John M. 1985. “Mental Practice and Imagery in Music Performance.” Psychology of Music 13 (2): 123–34. RSL Awards. 2024. RSL Awards Music Performance Syllabus 2024. Manual. Sapolsky, Robert M. 2004. Why Zebras Don’t Get Ulcers. New York: St Martin’s Press. Sense, Florian, Tiffany S. Jastrzembski, Michael C. Mozer, Michael Krusmark, and Hedderik van Rijn. 2019. “Perspectives on Computational Models of Learning and Forgetting.” In Proceedings of the 17th International Conference on Cognitive Modeling, edited by Terence C. Stewart, 216–21. Montreal, Canada: Applied Cognitive Science Lab. Settles, Burr, and Brendan Meeder. 2016. “A Trainable Spaced Repetition Model for Language Learning.” In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL), 1848–58. Berlin, Germany: Association for Computational Linguistics. Silas, Sebastian, and Daniel Müllensiefen. 2023. “Learning and Recalling Melodies: A Computational Investigation Using the Melodic Recall Paradigm.” Music Perception. https://doi.org/10.1525/mp.2023.41.2.77. Silas, Sebastian, Daniel Müllensiefen, Rebecca Gelding, Klaus Frieler, and Peter M. C. Harrison. 2022. “The Associations Between Music Training, Musical Working Memory, and Visuospatial Working Memory: An Opportunity for Causal Modeling.” Music Perception 39 (4): 401–20. https://doi.org/10.1525/mp.2022.39.4.401. Silas, Sebastian, Daniel Müllensiefen, and Reinhard Kopiez. 2023. “Singing Ability Assessment: Development and Validation of a Singing Test Based on Item Response Theory and a General Open-Source Software Environment for Singing Data.” Behavior Research Methods. https://doi.org/10.3758/s13428-023-02188-0. Sloboda, J. A, and D. H. H. Parker. 1985. “Immediate Recall of Melodies.” In Musical Structure and Cognition, edited by R. West, P Howell, and I Cross, 143–67. London: Academic Press. Slonimsky, Nicolas. 1947. Thesaurus of Scales and Melodic Patterns. Literary Licensing, LLC. ———. 1988. Perfect Pitch: A Life Story. Oxford University Press. Smith-Muller, Talia. 2024. “Berklee Online Brings Ear Training to Virtual Reality with New Game.” Berklee Online Take Note. Southall, Jean. 2009. Blind Tom: The Black Pianist-Composer. Columbia, SC: University of South Carolina Press. Spice, Graham. 2010. “Literature Review of Jazz Improvisation Pedagogy,” January. Spotify. 2025. “Basic Pitch: Open Source Audio-to-MIDI Transcription Tool.” Stevens, Robin S. 1991. “The Best of Both Worlds: An Eclectic Approach to the Use of Computer Technology in Music Education.” International Journal of Music Education os-17 (1): 24–36. https://doi.org/10.1177/025576149101700104. Surber, Greg A. 2009. “Record Progressions: Technology and Its Role in the Development and Dissemination of Jazz.” PhD thesis, Ohio University. Sussman, Elyse S. 2017. “Auditory Scene Analysis: An Attention Perspective.” Journal of Speech, Language, and Hearing Research : JSLHR 60 (10): 2989–3000. https://doi.org/10.1044/2017_JSLHR-H-17-0041. Technimo LLC. 2025e. “iReal Pro.” “The Complete Transcription Process David Liebman.” 2006. TheDrumNinja, and Vic Firth. 2025. “GrooveScribe.” TonedEar. 2025. “TonedEar Ear Training App.” ToneGym. 2025. “ToneGym: Ear Training and Music Theory.” “Transcribe! - Software to Help Transcribe Recorded Music.” 2021. https://www.seventhstring.com/xscribe/overview.html. Treffert, Darold. 2009. Islands of Genius: The Bountiful Mind of the Autistic, Acquired, and Sudden Savant. London: Jessica Kingsley Publishers. Trinity College London. 2024. Trinity College London Music Syllabus 2024. Manual. Typke, Rainer, Frans Wiering, and Remco C. Veltkamp. 2007. “Transportation Distances and Human Perception of Melodic Similarity.” Musicae Scientiae 11 (1_suppl): 153–81. https://doi.org/10.1177/102986490701100107. Van Deusen-Scholl, Nelleke, Mary Jo Lubrano, and Zachary Sporn. 2021. “Measuring Babbel’s Efficacy in Developing Oral Proficiency.” Varvarigou, Maria. 2014. “’Play It by Ear’–Teachers’ Responses to Ear-Playing Tasks During One-to-One Instrumental Lessons.” Music Education Research 16 (4): 471–84. https://doi.org/10.1080/14613808.2013.859662. Varvarigou, Maria, and Lucy Green. 2015. “By-Ear Learning of Short Melodies: A Case Study.” International Journal of Music Education 33 (2): 115–27. Vesselinov, Roumen, and John Grego. 2012. “Duolingo Effectiveness Study.” City University of New York and University of South Carolina. VR, Drumhead. 2025. “Drumhead VR: Virtual Drumming and Rhythm Training.” Wilder, Michael D. 1988. “An Investigation of the Relationship Between Melodic Ear-to-Hand Coordination and Written and Aural Theory Skills Within an Undergraduate Music Theory Course.” Journal of Research in Music Education 36 (2): 105–18. https://doi.org/10.2307/3345377. Wilf, Eitan Y. 2012. “Rituals of Creativity: Tradition, Modernity, and the ‘Acoustic Unconscious’ in a U.S. Collegiate Jazz Music Program.” American Anthropologist 114 (1): 32–44. https://doi.org/10.1111/j.1548-1433.2011.01396.x. Witmer, Robert, and James Robbins. 1988. “A Historical and Critical Survey of Recent Pedagogical Materials for the Teaching and Learning of Jazz.” Bulletin of the Council for Research in Music Education, no. 96: 7–29. https://www.jstor.org/stable/i40013211. Woody, Robert H. 2003. “Explaining Expressive Performance: Component Cognitive Skills in an Aural Modeling Task.” Journal of Research in Music Education 51 (1): 51–63. https://doi.org/10.2307/3345648. ———. 2012. “Playing by Ear: Foundation or Frill?” Music Educators Journal 99 (2): 82–88. https://doi.org/10.1177/0027432112459080. ———. 2019. Psychology for Musicians: Understanding and Acquiring the Skills. Oxford University Press. ———. 2020. “Musicians’ Use of Harmonic Cognitive Strategies When Playing by Ear.” Psychology of Music 48 (5): 674–92. https://doi.org/10.1177/0305735618816365. Woody, Robert H., and Andreas C. Lehmann. 2010a. “Student Musicians’ Ear-Playing Ability as a Function of Instrument Type and Playing Experience.” Psychology of Music 38 (1): 39–63. ———. 2010b. “Student Musicians’ Ear-Playing Ability as a Function of Vernacular Music Experiences.” Journal of Research in Music Education 58 (2): 101–15. https://doi.org/10.1177/0022429410377119. “Woody’s Research to Appear in Psychology of Music, Upcoming New Book Announce University of Nebraska-Lincoln.” 2018. University of Nebraska News Room. Wooten, Victor. 2008. The Music Lesson: A Spiritual Search for Growth Through Music. New York: Berkley Books. Yamaguchi, Masaya. 2006. The Complete Thesaurus of Musical Scales. New York: Masaya Music. ———. 2011. Lexicon of Geometric Patterns for Jazz Improvisation. New York: Masaya Music. Yuan, Yuchen, Sho Oishi, Charles Cronin, Daniel Müllensiefen, Quentin Atkinson, Shinya Fujii, and Patrick E. Savage. 2020. “Perceptual Vs. Automated Judgments of Music Copyright Infringement.” PsyArXiv. https://doi.org/10.31234/osf.io/tq7v5. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
