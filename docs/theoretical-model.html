<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4 Playing By Ear: A Theoretical Model | Playing By Ear: A Computational Approach</title>
  <meta name="description" content="4 Playing By Ear: A Theoretical Model | Playing By Ear: A Computational Approach" />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="4 Playing By Ear: A Theoretical Model | Playing By Ear: A Computational Approach" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4 Playing By Ear: A Theoretical Model | Playing By Ear: A Computational Approach" />
  
  
  

<meta name="author" content="Seb Silas" />


<meta name="date" content="2025-06-03" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="backgrounds.html"/>
<link rel="next" href="computational-ecosystem.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Front Matter</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#foreword"><i class="fa fa-check"></i><b>1.1</b> Foreword</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i><b>1.2</b> Acknowledgements</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#toc"><i class="fa fa-check"></i><b>1.3</b> Table of Contents</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>2</b> Introduction</a>
<ul>
<li class="chapter" data-level="2.1" data-path="introduction.html"><a href="introduction.html#what-is-playing-by-ear"><i class="fa fa-check"></i><b>2.1</b> What Is Playing By Ear?</a></li>
<li class="chapter" data-level="2.2" data-path="introduction.html"><a href="introduction.html#learning-to-play-by-ear"><i class="fa fa-check"></i><b>2.2</b> Learning to Play By Ear</a></li>
<li class="chapter" data-level="2.3" data-path="introduction.html"><a href="introduction.html#the-problem-domain-and-approach"><i class="fa fa-check"></i><b>2.3</b> The Problem Domain and Approach</a></li>
<li class="chapter" data-level="2.4" data-path="introduction.html"><a href="introduction.html#motivations"><i class="fa fa-check"></i><b>2.4</b> Motivations</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="introduction.html"><a href="introduction.html#generalising-beyond-jazz-and-playing-by-ear"><i class="fa fa-check"></i><b>2.4.1</b> Generalising Beyond Jazz and Playing By Ear</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="introduction.html"><a href="introduction.html#thesis-statement"><i class="fa fa-check"></i><b>2.5</b> Thesis Statement</a></li>
<li class="chapter" data-level="2.6" data-path="introduction.html"><a href="introduction.html#original-contributions"><i class="fa fa-check"></i><b>2.6</b> Original Contributions</a></li>
<li class="chapter" data-level="2.7" data-path="introduction.html"><a href="introduction.html#research-questions"><i class="fa fa-check"></i><b>2.7</b> Research Questions</a></li>
<li class="chapter" data-level="2.8" data-path="introduction.html"><a href="introduction.html#objectives-and-scope"><i class="fa fa-check"></i><b>2.8</b> Objectives and Scope</a>
<ul>
<li class="chapter" data-level="2.8.1" data-path="introduction.html"><a href="introduction.html#scope"><i class="fa fa-check"></i><b>2.8.1</b> Scope</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="introduction.html"><a href="introduction.html#front-facing-tools"><i class="fa fa-check"></i><b>2.9</b> Front-Facing Tools</a>
<ul>
<li class="chapter" data-level="2.9.1" data-path="introduction.html"><a href="introduction.html#slonimsky.app"><i class="fa fa-check"></i><b>2.9.1</b> Slonimsky.app</a></li>
<li class="chapter" data-level="2.9.2" data-path="introduction.html"><a href="introduction.html#songbird"><i class="fa fa-check"></i><b>2.9.2</b> Songbird</a></li>
</ul></li>
<li class="chapter" data-level="2.10" data-path="introduction.html"><a href="introduction.html#dissertation-outline"><i class="fa fa-check"></i><b>2.10</b> Dissertation Outline</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="backgrounds.html"><a href="backgrounds.html"><i class="fa fa-check"></i><b>3</b> Backgrounds</a>
<ul>
<li class="chapter" data-level="3.1" data-path="backgrounds.html"><a href="backgrounds.html#historical_background"><i class="fa fa-check"></i><b>3.1</b> Historical Background: Approaches to Improving Playing by Ear in Jazz Music</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="backgrounds.html"><a href="backgrounds.html#the-aural-origins-of-jazz-education"><i class="fa fa-check"></i><b>3.1.1</b> The Aural Origins of Jazz Education</a></li>
<li class="chapter" data-level="3.1.2" data-path="backgrounds.html"><a href="backgrounds.html#record-players-as-pedagogical-tools"><i class="fa fa-check"></i><b>3.1.2</b> Record Players as Pedagogical Tools</a></li>
<li class="chapter" data-level="3.1.3" data-path="backgrounds.html"><a href="backgrounds.html#codification-of-jazz-education"><i class="fa fa-check"></i><b>3.1.3</b> Codification of Jazz Education</a></li>
<li class="chapter" data-level="3.1.4" data-path="backgrounds.html"><a href="backgrounds.html#the-chord-scale-approach-pattern-books-and-melodic-vocabulary-acquisition"><i class="fa fa-check"></i><b>3.1.4</b> The Chord-Scale Approach, Pattern Books, and Melodic Vocabulary Acquisition</a></li>
<li class="chapter" data-level="3.1.5" data-path="backgrounds.html"><a href="backgrounds.html#john-coltrane-and-slonimskys-thesaurus-aspirational-melodic-vocabulary-acquisition"><i class="fa fa-check"></i><b>3.1.5</b> John Coltrane and Slonimsky’s Thesaurus: Aspirational Melodic Vocabulary Acquisition</a></li>
<li class="chapter" data-level="3.1.6" data-path="backgrounds.html"><a href="backgrounds.html#historical-background-conclusion"><i class="fa fa-check"></i><b>3.1.6</b> Historical Background: Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="backgrounds.html"><a href="backgrounds.html#modern_technological_approaches"><i class="fa fa-check"></i><b>3.2</b> Modern Technological Approaches to Ear Training</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="backgrounds.html"><a href="backgrounds.html#historical-development-of-ear-training-technology"><i class="fa fa-check"></i><b>3.2.1</b> Historical Development of Ear-Training Technology</a></li>
<li class="chapter" data-level="3.2.2" data-path="backgrounds.html"><a href="backgrounds.html#non-commercial-and-research-initiatives"><i class="fa fa-check"></i><b>3.2.2</b> Non-Commercial and Research Initiatives</a></li>
<li class="chapter" data-level="3.2.3" data-path="backgrounds.html"><a href="backgrounds.html#pedagogical-strategies-and-practical-use"><i class="fa fa-check"></i><b>3.2.3</b> Pedagogical Strategies and Practical Use</a></li>
<li class="chapter" data-level="3.2.4" data-path="backgrounds.html"><a href="backgrounds.html#benefits-and-limitations"><i class="fa fa-check"></i><b>3.2.4</b> Benefits and Limitations</a></li>
<li class="chapter" data-level="3.2.5" data-path="backgrounds.html"><a href="backgrounds.html#modern-technological-approaches-to-ear-training-conclusion"><i class="fa fa-check"></i><b>3.2.5</b> Modern Technological Approaches to Ear Training: Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="backgrounds.html"><a href="backgrounds.html#playing_by_ear_literature_review"><i class="fa fa-check"></i><b>3.3</b> Playing by Ear Research: A Literature Review</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="backgrounds.html"><a href="backgrounds.html#cognitive-and-neural-mechanisms-underlying-playing-by-ear"><i class="fa fa-check"></i><b>3.3.1</b> Cognitive and Neural Mechanisms Underlying Playing by Ear</a></li>
<li class="chapter" data-level="3.3.2" data-path="backgrounds.html"><a href="backgrounds.html#developmental-and-pedagogical-aspects-of-playing-by-ear-skill-acquisition"><i class="fa fa-check"></i><b>3.3.2</b> Developmental and Pedagogical Aspects of Playing By Ear Skill Acquisition</a></li>
<li class="chapter" data-level="3.3.3" data-path="backgrounds.html"><a href="backgrounds.html#practical-strategies-and-individual-differences-in-playing-by-ear"><i class="fa fa-check"></i><b>3.3.3</b> Practical Strategies and Individual Differences in Playing By Ear</a></li>
<li class="chapter" data-level="3.3.4" data-path="backgrounds.html"><a href="backgrounds.html#playing-by-ear-research-conclusion"><i class="fa fa-check"></i><b>3.3.4</b> Playing by Ear Research: Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="backgrounds.html"><a href="backgrounds.html#musicological_background"><i class="fa fa-check"></i><b>3.4</b> Computational Musicological Perspectives</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="backgrounds.html"><a href="backgrounds.html#music-and-combinatorics"><i class="fa fa-check"></i><b>3.4.1</b> Music and Combinatorics</a></li>
<li class="chapter" data-level="3.4.2" data-path="backgrounds.html"><a href="backgrounds.html#melodic-represention-and-computational-melodic-features"><i class="fa fa-check"></i><b>3.4.2</b> Melodic Represention and Computational Melodic Features</a></li>
<li class="chapter" data-level="3.4.3" data-path="backgrounds.html"><a href="backgrounds.html#melody-as-cognitive-psychology"><i class="fa fa-check"></i><b>3.4.3</b> Melody as Cognitive Psychology</a></li>
<li class="chapter" data-level="3.4.4" data-path="backgrounds.html"><a href="backgrounds.html#fuzzification"><i class="fa fa-check"></i><b>3.4.4</b> Fuzzification</a></li>
<li class="chapter" data-level="3.4.5" data-path="backgrounds.html"><a href="backgrounds.html#computed-melodic-features-as-a-musicological-basis-for-selecting-items"><i class="fa fa-check"></i><b>3.4.5</b> Computed Melodic Features as a musicological basis for selecting items</a></li>
<li class="chapter" data-level="3.4.6" data-path="backgrounds.html"><a href="backgrounds.html#pattern-books-as-item-banks"><i class="fa fa-check"></i><b>3.4.6</b> Pattern Books as Item Banks</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="backgrounds.html"><a href="backgrounds.html#cognitive_psychological_background"><i class="fa fa-check"></i><b>3.5</b> Cognitive Psychology</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="backgrounds.html"><a href="backgrounds.html#general-principles-of-learning-and-memory"><i class="fa fa-check"></i><b>3.5.1</b> General Principles of Learning and Memory</a></li>
<li class="chapter" data-level="3.5.2" data-path="backgrounds.html"><a href="backgrounds.html#music-cognitive-psychological-models"><i class="fa fa-check"></i><b>3.5.2</b> Music Cognitive Psychological Models</a></li>
<li class="chapter" data-level="3.5.3" data-path="backgrounds.html"><a href="backgrounds.html#non-musical-working-memory"><i class="fa fa-check"></i><b>3.5.3</b> Non-Musical Working Memory</a></li>
<li class="chapter" data-level="3.5.4" data-path="backgrounds.html"><a href="backgrounds.html#conclusion-cognitive-psychological-background"><i class="fa fa-check"></i><b>3.5.4</b> Conclusion: Cognitive Psychological Background</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="backgrounds.html"><a href="backgrounds.html#statistical_modelling_frameworks"><i class="fa fa-check"></i><b>3.6</b> Statistical Modelling Frameworks</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="backgrounds.html"><a href="backgrounds.html#item-response-theory"><i class="fa fa-check"></i><b>3.6.1</b> Item Response Theory</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="theoretical-model.html"><a href="theoretical-model.html"><i class="fa fa-check"></i><b>4</b> Playing By Ear: A Theoretical Model</a>
<ul>
<li class="chapter" data-level="4.1" data-path="theoretical-model.html"><a href="theoretical-model.html#pfordreshers-h-pac-model"><i class="fa fa-check"></i><b>4.1</b> Pfordresher’s H-PAC Model</a></li>
<li class="chapter" data-level="4.2" data-path="theoretical-model.html"><a href="theoretical-model.html#pfordreshers-cognitive-model-of-singing-accuracy"><i class="fa fa-check"></i><b>4.2</b> Pfordresher’s Cognitive Model of Singing Accuracy</a></li>
<li class="chapter" data-level="4.3" data-path="theoretical-model.html"><a href="theoretical-model.html#bakers-computational-model-of-melodic-dictation"><i class="fa fa-check"></i><b>4.3</b> Baker’s Computational Model of Melodic Dictation</a></li>
<li class="chapter" data-level="4.4" data-path="theoretical-model.html"><a href="theoretical-model.html#models-of-sung-recall"><i class="fa fa-check"></i><b>4.4</b> Models of Sung Recall</a></li>
<li class="chapter" data-level="4.5" data-path="theoretical-model.html"><a href="theoretical-model.html#bringing-it-together"><i class="fa fa-check"></i><b>4.5</b> Bringing It Together</a></li>
<li class="chapter" data-level="4.6" data-path="theoretical-model.html"><a href="theoretical-model.html#moving-from-theoretical-to-computational"><i class="fa fa-check"></i><b>4.6</b> Moving from Theoretical to Computational</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="theoretical-model.html"><a href="theoretical-model.html#inputs"><i class="fa fa-check"></i><b>4.6.1</b> Inputs</a></li>
<li class="chapter" data-level="4.6.2" data-path="theoretical-model.html"><a href="theoretical-model.html#sequential-construction"><i class="fa fa-check"></i><b>4.6.2</b> Sequential Construction</a></li>
<li class="chapter" data-level="4.6.3" data-path="theoretical-model.html"><a href="theoretical-model.html#function-application"><i class="fa fa-check"></i><b>4.6.3</b> Function Application</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="theoretical-model.html"><a href="theoretical-model.html#mathematical-representation"><i class="fa fa-check"></i><b>4.7</b> Mathematical Representation</a></li>
<li class="chapter" data-level="4.8" data-path="theoretical-model.html"><a href="theoretical-model.html#algorithm-flow"><i class="fa fa-check"></i><b>4.8</b> Algorithm Flow</a></li>
<li class="chapter" data-level="4.9" data-path="theoretical-model.html"><a href="theoretical-model.html#final-equation"><i class="fa fa-check"></i><b>4.9</b> Final Equation</a></li>
<li class="chapter" data-level="4.10" data-path="theoretical-model.html"><a href="theoretical-model.html#strategies"><i class="fa fa-check"></i><b>4.10</b> Strategies</a></li>
<li class="chapter" data-level="4.11" data-path="theoretical-model.html"><a href="theoretical-model.html#the-melodic-mind-as-item-bank"><i class="fa fa-check"></i><b>4.11</b> The Melodic Mind as Item Bank</a></li>
<li class="chapter" data-level="4.12" data-path="theoretical-model.html"><a href="theoretical-model.html#predictions"><i class="fa fa-check"></i><b>4.12</b> Predictions</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="computational-ecosystem.html"><a href="computational-ecosystem.html"><i class="fa fa-check"></i><b>5</b> A Computational Ecosystem</a>
<ul>
<li class="chapter" data-level="5.1" data-path="computational-ecosystem.html"><a href="computational-ecosystem.html#transcription-of-melodic-production-data-pyin"><i class="fa fa-check"></i><b>5.1</b> Transcription of melodic production data: pyin</a></li>
<li class="chapter" data-level="5.2" data-path="computational-ecosystem.html"><a href="computational-ecosystem.html#assessment-of-melodic-similarity-melsim"><i class="fa fa-check"></i><b>5.2</b> Assessment of melodic similarity: melsim</a></li>
<li class="chapter" data-level="5.3" data-path="computational-ecosystem.html"><a href="computational-ecosystem.html#psychologically-meaningful-musical-item-banks-itembankr"><i class="fa fa-check"></i><b>5.3</b> Psychologically meaningful musical item banks: itembankr</a></li>
<li class="chapter" data-level="5.4" data-path="computational-ecosystem.html"><a href="computational-ecosystem.html#datasets"><i class="fa fa-check"></i><b>5.4</b> Datasets</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="computational-ecosystem.html"><a href="computational-ecosystem.html#wjd"><i class="fa fa-check"></i><b>5.4.1</b> WJD</a></li>
<li class="chapter" data-level="5.4.2" data-path="computational-ecosystem.html"><a href="computational-ecosystem.html#berkowitz"><i class="fa fa-check"></i><b>5.4.2</b> Berkowitz</a></li>
<li class="chapter" data-level="5.4.3" data-path="computational-ecosystem.html"><a href="computational-ecosystem.html#slonimsky"><i class="fa fa-check"></i><b>5.4.3</b> Slonimsky</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="computational-ecosystem.html"><a href="computational-ecosystem.html#other-useful-functions"><i class="fa fa-check"></i><b>5.5</b> Other useful functions</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="computational-ecosystem.html"><a href="computational-ecosystem.html#assessing-musical-behaviours-musicassessr"><i class="fa fa-check"></i><b>5.5.1</b> Assessing musical behaviours: musicassessr</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="computational-ecosystem.html"><a href="computational-ecosystem.html#ability-tests"><i class="fa fa-check"></i><b>5.6</b> Ability tests</a></li>
<li class="chapter" data-level="5.7" data-path="computational-ecosystem.html"><a href="computational-ecosystem.html#conclusion"><i class="fa fa-check"></i><b>5.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="melsim_development.html"><a href="melsim_development.html"><i class="fa fa-check"></i><b>6</b> Experiment 1: Development of a melodic similarity algorithm for short recalled melodies</a></li>
<li class="chapter" data-level="7" data-path="pbet_lab_study.html"><a href="pbet_lab_study.html"><i class="fa fa-check"></i><b>7</b> Experiment 2: An Explanatory Model Of Playing By Ear</a>
<ul>
<li class="chapter" data-level="7.1" data-path="pbet_lab_study.html"><a href="pbet_lab_study.html#methods"><i class="fa fa-check"></i><b>7.1</b> Methods</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="pbet_lab_study.html"><a href="pbet_lab_study.html#participants-and-preliminary-questionnaire"><i class="fa fa-check"></i><b>7.1.1</b> Participants and Preliminary Questionnaire</a></li>
<li class="chapter" data-level="7.1.2" data-path="pbet_lab_study.html"><a href="pbet_lab_study.html#apparatus-and-general-procedure"><i class="fa fa-check"></i><b>7.1.2</b> Apparatus and General Procedure</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="pbet_online_study.html"><a href="pbet_online_study.html"><i class="fa fa-check"></i><b>8</b> Experiment 3: Modelling Melodic Difficulty in Playing by Ear: An Item Response Analysis of Online Performance Data</a></li>
<li class="chapter" data-level="9" data-path="study_history_study.html"><a href="study_history_study.html"><i class="fa fa-check"></i><b>9</b> Experiment 4: A Longitudinal Study of Playing By Ear Skills</a></li>
<li class="chapter" data-level="10" data-path="similarity_study.html"><a href="similarity_study.html"><i class="fa fa-check"></i><b>10</b> Experiment 5: A Similarity-Based Approach to Item Selection</a>
<ul>
<li class="chapter" data-level="10.1" data-path="similarity_study.html"><a href="similarity_study.html#collaborative-filtering"><i class="fa fa-check"></i><b>10.1</b> Collaborative Filtering</a></li>
<li class="chapter" data-level="10.2" data-path="similarity_study.html"><a href="similarity_study.html#approaches-to-similarity-for-large-scale-corpora"><i class="fa fa-check"></i><b>10.2</b> Approaches to Similarity for Large-Scale Corpora</a></li>
<li class="chapter" data-level="10.3" data-path="similarity_study.html"><a href="similarity_study.html#melodic-corpora-as-networks"><i class="fa fa-check"></i><b>10.3</b> Melodic corpora as networks</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="similarity_study.html"><a href="similarity_study.html#approach-1-similarity-of-features-as-a-proxy-for-melodic-similarity"><i class="fa fa-check"></i><b>10.3.1</b> Approach #1: Similarity of Features as a Proxy for Melodic Similarity</a></li>
<li class="chapter" data-level="10.3.2" data-path="similarity_study.html"><a href="similarity_study.html#approach-2-generative-similarity"><i class="fa fa-check"></i><b>10.3.2</b> Approach #2: Generative Similarity</a></li>
<li class="chapter" data-level="10.3.3" data-path="similarity_study.html"><a href="similarity_study.html#approach-3-parent-melody-representation"><i class="fa fa-check"></i><b>10.3.3</b> Approach #3: Parent Melody Representation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="additional-learning-paradigms.html"><a href="additional-learning-paradigms.html"><i class="fa fa-check"></i><b>11</b> Additional Learning Paradigms</a>
<ul>
<li class="chapter" data-level="11.1" data-path="additional-learning-paradigms.html"><a href="additional-learning-paradigms.html#generative-similarity"><i class="fa fa-check"></i><b>11.1</b> Generative Similarity</a></li>
<li class="chapter" data-level="11.2" data-path="additional-learning-paradigms.html"><a href="additional-learning-paradigms.html#learn-chunks"><i class="fa fa-check"></i><b>11.2</b> Learn Chunks</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="experiment-6-model-synthesis-development-of-a-musical-dashsim-model.html"><a href="experiment-6-model-synthesis-development-of-a-musical-dashsim-model.html"><i class="fa fa-check"></i><b>12</b> Experiment 6: Model synthesis: Development of a musical DASH+SIM model</a></li>
<li class="chapter" data-level="13" data-path="interventional_Study.html"><a href="interventional_Study.html"><i class="fa fa-check"></i><b>13</b> Experiment 7: An Interventional Study</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Playing By Ear: A Computational Approach</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="theoretical-model" class="section level1 hasAnchor" number="4">
<h1><span class="header-section-number">4</span> Playing By Ear: A Theoretical Model<a href="theoretical-model.html#theoretical-model" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>As far as the author is aware, no computational model of playing by ear has been described in the music psychology, or related, literature(s). Thus, a first step in this thesis is to propose a theoretical model, which can then later be formulated more precisely as a computational model, then tested, partly with data collected throughout this thesis.</p>
<p>However, related computational models have been described, for instance in the domain of aural skill acquisition <span class="citation">(Baker 2019)</span> and sung melodic recall by the author <span class="citation">(Silas and Müllensiefen 2023; Silas, Müllensiefen, and Kopiez 2023)</span>. Theoretical cognitive models have also been described in the domain of singing accuracy <span class="citation">(Pfordresher et al. 2015)</span>. Therefore, in this section, I will attempt to synthesise this literature to propose a corresponding model specifically for playing by ear skills. I distinguish between two basic playing by ear processes: a) playing by ear skill acquisition (learning) and b) playing by ear skill execution (recall).</p>
<div id="pfordreshers-h-pac-model" class="section level2 hasAnchor" number="4.1">
<h2><span class="header-section-number">4.1</span> Pfordresher’s H-PAC Model<a href="theoretical-model.html#pfordreshers-h-pac-model" class="anchor-section" aria-label="Anchor link to header"></a></h2>
</div>
<div id="pfordreshers-cognitive-model-of-singing-accuracy" class="section level2 hasAnchor" number="4.2">
<h2><span class="header-section-number">4.2</span> Pfordresher’s Cognitive Model of Singing Accuracy<a href="theoretical-model.html#pfordreshers-cognitive-model-of-singing-accuracy" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>I start first by using <span class="citation">Pfordresher et al. (2015)</span>’s cognitive (but non-computational) model of singing accuracy, which is the most broad of the models I will review. At the low level, this model comprises an auditory feedback loop. In this loop, first, external auditory input is processed as low-level perceptual representations of sound (pitch, duration, timbre, loudness). Such low-level representations are used as input to a translation model, which relates auditory input to sensorimotor action that is relevant to singing. Hence, this enables the guidance of a singer’s sensorimotor plans to adjust their singing (e.g., to be in tune), in response to auditory feedback. Such changes in sensorimotor actions comprise physical processes like respiration, phonation, and articulation. Abstracting the sensorimotor part to the domain of playing by ear, such physical processes would be somewhat instrument-specific. For many instruments, the physical part will involve finger coordination (e.g., clarinet, violin), but for some, coordination will be more at the level of the arm (e.g., percussion, drums). Many processes will be instrument-specific processes. For example, for brass and woodwind instruments, requiring to coordinate an embouchure to produce a sound. We do not exhaust these instrument-specific processes, but only state that such physical processes are part of the model, with instrument-specific attributes.</p>
<p>The lower-level auditory representations are also used as input to higher levels of cognition, which hold mental templates about music (e.g., its features, such as its tonality), stored in long- term memory (Baddeley et al., 2009). These templates allow auditory content to be categorized, forming more sophisticated representations of it, taking on musical domains such as representations of (melodic) features like tonality and contour, as well as segmenting melodies into coherent perceptual chunks. These formed higher-level representations can in turn be used as input back to the lower-level auditory feedback loop and further inform sensorimotor planning. Hence, the overall architecture of Pfordresher et al. (2015)’s cognitive model is bidirectional: both “top down” and “bottom up”. Altogether, this system enables a singer - to fulfill objectives related to sung recall (i.e., hearing stimuli, representing its musical features mentally, responding through singing, and adjusting behavior to fulfill the goal sufficiently). This can readily be abstracted to playing by ear too: the only difference being in the motor plan aspects, which, as mentioned, are instrument-specific. Our focus in the current manuscript is on the higher-level aspects of this model: memory for melodic representations.</p>
<div class="float">
<img src="img/pfordresher_singing_model.png" alt="Caption for the image" />
<div class="figcaption">Caption for the image</div>
</div>
</div>
<div id="bakers-computational-model-of-melodic-dictation" class="section level2 hasAnchor" number="4.3">
<h2><span class="header-section-number">4.3</span> Baker’s Computational Model of Melodic Dictation<a href="theoretical-model.html#bakers-computational-model-of-melodic-dictation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
</div>
<div id="models-of-sung-recall" class="section level2 hasAnchor" number="4.4">
<h2><span class="header-section-number">4.4</span> Models of Sung Recall<a href="theoretical-model.html#models-of-sung-recall" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Lastly, I synthesise models from my own research regarding sung melodic recall. My <span class="citation">Silas, Müllensiefen, and Kopiez (2023)</span> study investigated sung recall by examining its relationship with other related skills and participant attributes. Demographically, better sung recall ability ability was associated with more musical training (𝛽 = 0.07), a younger age (𝛽 = -0.05), and being female (𝛽 = -0.03), though the latter two effects were small. The findings suggest that while musical training can enhance singing ability, inherent skills might also predispose individuals to seek musical training. Moreover, there was a relatively large difference between the marginal and conditional R^2 values, which suggests that there is a sizeable proportion of individual differences in the sample of participants tested which explains SAA performance which shows that individual differences should explain performance on a task in which one can develop high levels of domain-specific expertise.</p>
<p>We found there that melodic discrimination, pitch imagery abilities, and mistuning perception appear to be fundamental skills contributing to melodic sung recall. The sung recall score we devised moderately correlated with self-reported singing ability (r = .46) and musical training (r = ?). However, it shows no significant correlation with visuospatial working memory or pitch discrimination, aligning with previous research and suggesting that singing ability is not closely linked to low-level perceptual processes. This suggests that, whilst</p>
<p>The study also shows that melodic features which indicate melodic complexity (including melody length) are relevant predictors of sung recall performance. These predictors represented melody length, as well as features related to contour, tonality, statistical occurrence of N-grams in the melody, and durational complexity. Rhythmic trials were generally found to be more difficult (B = -0.15, p &lt; .001) and we found that it was appropriate to create separate models for arhythmic and rhythmic.</p>
<p>In another of my papers, <span class="citation">(Silas and Müllensiefen 2023)</span>, we investigated how individuals learn and recall melodies using a computational approach that emphasizes similarity metrics rather than traditional accuracy-based methods. The research involved 31 participants who sang back 28 melodies presented either as piano sounds or vocal audio excerpts from pop songs, with each melody repeated up to six times. The similarity between the target melody and the participants’ sung recalls was measured using advanced algorithmic techniques. The primary goal was to understand how melodic recall improves over successive attempts and what factors most significantly influence recall accuracy.</p>
<p>One of the key findings of the study is that the length of sung recall attempts consistently increases across multiple attempts, and this increase significantly correlates with the overall similarity to the target melody. This observation challenges the traditional view that musical features such as interval patterns or tonality are the primary factors influencing melodic recall. Instead, the study suggests that the sheer length of the melody plays a more critical role, indicating that general memory constraints may be more influential than music-specific features. This insight aligns with theories of working memory, suggesting that melodic recall shares characteristics with other types of serial recall rather than being purely domain-specific to music.</p>
<p>A significant finding (presented there in Figure 8), examines how similarity changes as a function of attempt and the specific section of the melody (beginning, middle, end). The results show that similarity performance increases more rapidly for the beginning of the melody compared to the middle and end across successive attempts. This indicates that participants tend to stabilise their recall of the initial melodic segments earlier than the middle or end sections. The end sections, in particular, show a slower rate of improvement, suggesting that the latter parts of a melody are more challenging to accurately recall perhaps due to the higher working memory load as a result of having to remember more notes, the later you are in a position, or that generally people engaging in sung recall prioritise getting the earlier parts melody correct first. This finding supports the theory that memory encoding is stronger for the beginning of sequences, a phenomenon consistent with the primacy effect observed in other types of serial recall tasks. It also highlights that recall accuracy is not uniformly distributed across a melody, with the initial segments being more robustly learned compared to later ones.</p>
<p>The study also found that musical training did not significantly enhance recall performance when similarity metrics were used as the primary assessment method, as shown in another key result. This challenges the conventional belief that formal musical training substantially improves melodic recall ability. Instead, the data suggests that domain-general memory skills may be just as important, pointing to a more integrative understanding of how musical and non-musical cognitive processes interact.</p>
<p>Moreover, the results highlight that similarity scores increase with each successive attempt, but the rate of improvement diminishes after the third or fourth attempt. This pattern suggests that the most substantial gains in melodic recall occur early in the learning process, followed by incremental refinements in pitch and rhythm accuracy. Additionally, melodies presented as vocal audio excerpts resulted in higher recall accuracy compared to piano sound presentations, indicating that the human voice might provide additional mnemonic cues that aid in memory encoding.</p>
<p>Overall, the study’s use of similarity-based metrics, particularly the “opti3” metric that integrates pitch intervals, harmonic progression, and rhythmic similarity, offers a more nuanced and accurate assessment of how melodies are learned and recalled. By focusing on the gradual increase in recall length and the differential improvement across melodic segments, the study sheds light on the underlying cognitive processes involved in melodic memory, emphasizing the role of general memory mechanisms and the relatively limited impact of formal musical training.</p>
</div>
<div id="bringing-it-together" class="section level2 hasAnchor" number="4.5">
<h2><span class="header-section-number">4.5</span> Bringing It Together<a href="theoretical-model.html#bringing-it-together" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In sum, then,</p>
</div>
<div id="moving-from-theoretical-to-computational" class="section level2 hasAnchor" number="4.6">
<h2><span class="header-section-number">4.6</span> Moving from Theoretical to Computational<a href="theoretical-model.html#moving-from-theoretical-to-computational" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>“When we develop a psychological theory that is sufficiently precise to be implemented as a computer program, we call it a computational model. The process of developing such theories, and implementing their corresponding computer programs, is then called computational modelling.” <span class="citation">(Harrison 2025)</span>.</p>
<p>At this point, I already suggest a more specific feature to the model: that for playing by ear tasks, there exists a mental melodic “similarity” algorithm, housed in working memory. The input to this module is the target melody and the evolving real-time assessment of the melody being produced in the moment. The performer must hear the target melody, store this in short term memory as a reference, and then continuously update the target melody which each new note that is produced. I speculate that the mind recomputes the similarity between the presently number of recalled notes and the target melody iteratively, each time a new note is added. In other words, a check for the accuracy is made every time a note is recalled, and the presence of an error and its nature, will inform whether or not the performer stops to start again or continues to proceed recalling the melody, at least in the stage of learning. Formally:</p>
<div id="inputs" class="section level3 hasAnchor" number="4.6.1">
<h3><span class="header-section-number">4.6.1</span> Inputs<a href="theoretical-model.html#inputs" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><span class="math inline">\(T = [t_1, t_2, \dots, t_n]\)</span>: TargetMelody of fixed length <span class="math inline">\(n\)</span>.</li>
<li><span class="math inline">\(R = [r_1, r_2, \dots, r_k]\)</span>: RecalledMelody of length <span class="math inline">\(k\)</span>, unknown a priori.</li>
</ul>
</div>
<div id="sequential-construction" class="section level3 hasAnchor" number="4.6.2">
<h3><span class="header-section-number">4.6.2</span> Sequential Construction<a href="theoretical-model.html#sequential-construction" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><span class="math inline">\(T\)</span> is fixed and built one note at a time.</li>
<li><span class="math inline">\(R\)</span> is sequentially constructed one note at a time.</li>
</ul>
</div>
<div id="function-application" class="section level3 hasAnchor" number="4.6.3">
<h3><span class="header-section-number">4.6.3</span> Function Application<a href="theoretical-model.html#function-application" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>At each step <span class="math inline">\(i\)</span> (current length of <span class="math inline">\(R\)</span>), the function <strong>MelSim</strong> is applied to <strong>TargetMelody</strong> and the current length of <strong>RecalledMelody</strong>.</li>
</ul>
</div>
</div>
<div id="mathematical-representation" class="section level2 hasAnchor" number="4.7">
<h2><span class="header-section-number">4.7</span> Mathematical Representation<a href="theoretical-model.html#mathematical-representation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let:
- <span class="math inline">\(T_i\)</span> be the prefix of the TargetMelody up to length <span class="math inline">\(i\)</span>, i.e., <span class="math inline">\(T_i = [t_1, t_2, \dots, t_i]\)</span>.
- <span class="math inline">\(R_i\)</span> be the current state of the RecalledMelody of length <span class="math inline">\(i\)</span>, i.e., <span class="math inline">\(R_i = [r_1, r_2, \dots, r_i]\)</span>.
- <span class="math inline">\(\text{MelSim}(T_i, R_i)\)</span> be the similarity function between the two melodies at length <span class="math inline">\(i\)</span>.</p>
<p>The MelSim function can be formulated as:</p>
<p><span class="math display">\[
\text{MelSim}(T_i, R_i) = f(T_i, R_i)
\]</span></p>
<p>Where:
- <span class="math inline">\(f\)</span> is a similarity measure (e.g., distance function, alignment score).
- <span class="math inline">\(i\)</span> iterates from 1 to <span class="math inline">\(n\)</span>, where <span class="math inline">\(n\)</span> is the fixed length of the target melody.</p>
</div>
<div id="algorithm-flow" class="section level2 hasAnchor" number="4.8">
<h2><span class="header-section-number">4.8</span> Algorithm Flow<a href="theoretical-model.html#algorithm-flow" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li>Start with an empty recalled melody <span class="math inline">\(R\)</span>.</li>
<li>For each new note <span class="math inline">\(r_i\)</span> added to <span class="math inline">\(R\)</span>:
<ul>
<li>Compute the similarity: <span class="math inline">\(\text{MelSim}(T_i, R_i)\)</span> if <span class="math inline">\(i = n\)</span>.</li>
</ul></li>
<li>Repeat until the entire recalled melody is constructed.</li>
</ol>
</div>
<div id="final-equation" class="section level2 hasAnchor" number="4.9">
<h2><span class="header-section-number">4.9</span> Final Equation<a href="theoretical-model.html#final-equation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The similarity calculation is performed only when the length of the <strong>RecalledMelody</strong> equals the fixed length of the <strong>TargetMelody</strong>. Therefore, the final equation is expressed as:</p>
<p><span class="math display">\[
\text{MelSim}(T, R_n) = f(T, R_n)
\]</span></p>
<p>I note that the <code>MelSim</code> algorithm is itself modular and of discussion in and of itself (cite M&amp;F). For the purpose of this paper,</p>
</div>
<div id="strategies" class="section level2 hasAnchor" number="4.10">
<h2><span class="header-section-number">4.10</span> Strategies<a href="theoretical-model.html#strategies" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Beyond scope of this dissertation to investigate in detail.</li>
<li>Pop strategies hypotheses: <span class="citation">(Liscio and Brown 2024)</span></li>
<li>Also, we have our own text data, for later.</li>
</ul>
</div>
<div id="the-melodic-mind-as-item-bank" class="section level2 hasAnchor" number="4.11">
<h2><span class="header-section-number">4.11</span> The Melodic Mind as Item Bank<a href="theoretical-model.html#the-melodic-mind-as-item-bank" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Big concept</li>
</ul>
</div>
<div id="predictions" class="section level2 hasAnchor" number="4.12">
<h2><span class="header-section-number">4.12</span> Predictions<a href="theoretical-model.html#predictions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Simpler melodies will rely more on general working memory than musical working memory.</li>
<li>In the short term (&lt; 30 mins), melodies learned by sight will be less better retained than those</li>
</ul>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="backgrounds.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="computational-ecosystem.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": null,
    "text": null
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
